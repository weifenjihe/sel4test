# 1 "/home/b/test/sel4test/cbuild/kernel/kernel_all_pp_prune_wrapper_temp.c"
# 1 "/home/b/test/sel4test/cbuild//"
# 1 "<built-in>"
# 1 "<command-line>"
# 1 "/home/b/test/sel4test/cbuild/kernel/kernel_all_pp_prune_wrapper_temp.c"
# 1 "/home/b/test/sel4test/kernel/src/api/faults.c"
/*
 * Copyright 2020, Data61, CSIRO (ABN 41 687 119 230)
 *
 * SPDX-License-Identifier: GPL-2.0-only
 */

# 1 "/home/b/test/sel4test/kernel/include/config.h" 1
/*
 * Copyright 2020, Data61, CSIRO (ABN 41 687 119 230)
 *
 * SPDX-License-Identifier: GPL-2.0-only
 */

       

# 1 "/home/b/test/sel4test/kernel/libsel4/include/sel4/config.h" 1
/*
 * Copyright 2014, General Dynamics C4 Systems
 *
 * SPDX-License-Identifier: BSD-2-Clause
 */

       

/* Compile-time configuration parameters. Might be set by the build system. */

# 1 "/home/b/test/sel4test/cbuild/kernel/autoconf/autoconf.h" 1

       

# 1 "/home/b/test/sel4test/cbuild/kernel/gen_config/kernel/gen_config.h" 1
       




/* disabled: CONFIG_ARM_HIKEY_PREFETCHER_STBPFDIS */
/* disabled: CONFIG_ARM_HIKEY_PREFETCHER_STBPFRS */
/* disabled: CONFIG_PLAT_IMX7 */
/* disabled: CONFIG_ARCH_AARCH32 */

/* disabled: CONFIG_ARCH_ARM_HYP */
/* disabled: CONFIG_ARCH_RISCV32 */
/* disabled: CONFIG_ARCH_RISCV64 */
/* disabled: CONFIG_ARCH_X86_64 */
/* disabled: CONFIG_ARCH_IA32 */





/* disabled: CONFIG_PLAT_BCM2711 */
/* disabled: CONFIG_PLAT_BCM2837 */
/* disabled: CONFIG_PLAT_FVP */
/* disabled: CONFIG_PLAT_HIKEY */
/* disabled: CONFIG_PLAT_IMX8MQ_EVK */
/* disabled: CONFIG_PLAT_IMX8MM_EVK */
/* disabled: CONFIG_PLAT_IMX8MP_EVK */
/* disabled: CONFIG_PLAT_IMX93 */
/* disabled: CONFIG_PLAT_MAAXBOARD */
/* disabled: CONFIG_PLAT_ODROIDC2 */
/* disabled: CONFIG_PLAT_ODROIDC4 */

/* disabled: CONFIG_PLAT_QEMU_ARM_VIRT */
/* disabled: CONFIG_PLAT_QUARTZ64 */
/* disabled: CONFIG_PLAT_ROCKPRO64 */
/* disabled: CONFIG_PLAT_TQMA8XQP1GB */
/* disabled: CONFIG_PLAT_TX1 */
/* disabled: CONFIG_PLAT_TX2 */
/* disabled: CONFIG_PLAT_ZYNQMP */

/* disabled: CONFIG_ARM_CORTEX_A7 */
/* disabled: CONFIG_ARM_CORTEX_A8 */
/* disabled: CONFIG_ARM_CORTEX_A9 */
/* disabled: CONFIG_ARM_CORTEX_A15 */
/* disabled: CONFIG_ARM_CORTEX_A35 */
/* disabled: CONFIG_ARM_CORTEX_A53 */
/* disabled: CONFIG_ARM_CORTEX_A55 */
/* disabled: CONFIG_ARM_CORTEX_A57 */

/* disabled: CONFIG_ARCH_ARM_V7A */
/* disabled: CONFIG_ARCH_ARM_V7VE */

/* disabled: CONFIG_AARCH64_SERROR_IGNORE */

/* disabled: CONFIG_KERNEL_MCS */
/* disabled: CONFIG_ARM_PA_SIZE_BITS_40 */


/* disabled: CONFIG_DEBUG_DISABLE_L2_CACHE */
/* disabled: CONFIG_DEBUG_DISABLE_L1_ICACHE */
/* disabled: CONFIG_DEBUG_DISABLE_L1_DCACHE */
/* disabled: CONFIG_DEBUG_DISABLE_BRANCH_PREDICTION */
/* disabled: CONFIG_ARM_HYPERVISOR_SUPPORT */

/* disabled: CONFIG_AARCH64_VSPACE_S2_START_L1 */
/* disabled: CONFIG_ARM_HYP_ENABLE_VCPU_CP14_SAVE_AND_RESTORE */
/* disabled: CONFIG_ARM_ERRATA_430973 */
/* disabled: CONFIG_ARM_ERRATA_773022 */
/* disabled: CONFIG_ARM_SMMU */
/* disabled: CONFIG_TK1_SMMU */
/* disabled: CONFIG_ENABLE_A9_PREFETCHER */
/* disabled: CONFIG_EXPORT_PMU_USER */
/* disabled: CONFIG_DISABLE_WFI_WFE_TRAPS */
/* disabled: CONFIG_SMMU_INTERRUPT_ENABLE */
/* disabled: CONFIG_AARCH32_FPU_ENABLE_CONTEXT_SWITCH */

/* disabled: CONFIG_ALLOW_SMC_CALLS */

/* disabled: CONFIG_ARM_TLS_REG_TPIDRURO */


/* disabled: CONFIG_ARM_HAS_TLB_LOCK */
# 93 "/home/b/test/sel4test/cbuild/kernel/gen_config/kernel/gen_config.h"
/* disabled: CONFIG_EXCEPTION_FASTPATH */

/* disabled: CONFIG_SIGNAL_FASTPATH */


/* disabled: CONFIG_ENABLE_SMP_SUPPORT */

/* disabled: CONFIG_VERIFICATION_BUILD */
/* disabled: CONFIG_BINARY_VERIFICATION_BUILD */



/* disabled: CONFIG_KERNEL_INVOCATION_REPORT_ERROR_IPC */

/* disabled: CONFIG_BENCHMARK_GENERIC */
/* disabled: CONFIG_BENCHMARK_TRACK_KERNEL_ENTRIES */
/* disabled: CONFIG_BENCHMARK_TRACEPOINTS */
/* disabled: CONFIG_BENCHMARK_TRACK_UTILISATION */

/* disabled: CONFIG_ENABLE_BENCHMARKS */
/* disabled: CONFIG_KERNEL_LOG_BUFFER */





/* disabled: CONFIG_KERNEL_OPT_LEVEL_OS */
/* disabled: CONFIG_KERNEL_OPT_LEVEL_O0 */
/* disabled: CONFIG_KERNEL_OPT_LEVEL_O1 */
/* disabled: CONFIG_KERNEL_OPT_LEVEL_O3 */


/* disabled: CONFIG_KERNEL_FWHOLE_PROGRAM */
/* disabled: CONFIG_DANGEROUS_CODE_INJECTION */
/* disabled: CONFIG_DEBUG_DISABLE_PREFETCHERS */
/* disabled: CONFIG_SET_TLS_BASE_SELF */
/* disabled: CONFIG_CLZ_32 */
/* disabled: CONFIG_CLZ_64 */
/* disabled: CONFIG_CTZ_32 */
/* disabled: CONFIG_CTZ_64 */
/* disabled: CONFIG_CLZ_NO_BUILTIN */
/* disabled: CONFIG_CTZ_NO_BUILTIN */

/* disabled: CONFIG_EXPORT_VCNT_USER */
/* disabled: CONFIG_EXPORT_PTMR_USER */
/* disabled: CONFIG_EXPORT_VTMR_USER */
# 5 "/home/b/test/sel4test/cbuild/kernel/autoconf/autoconf.h" 2
# 12 "/home/b/test/sel4test/kernel/libsel4/include/sel4/config.h" 2
# 10 "/home/b/test/sel4test/kernel/include/config.h" 2

/* Set ENABLE_SMP_SUPPORT for kernel source files */
# 8 "/home/b/test/sel4test/kernel/src/api/faults.c" 2
# 1 "/home/b/test/sel4test/kernel/include/types.h" 1
/*
 * Copyright 2014, General Dynamics C4 Systems
 *
 * SPDX-License-Identifier: GPL-2.0-only
 */

       

# 1 "/home/b/test/sel4test/kernel/include/basic_types.h" 1
/*
 * Copyright 2014, General Dynamics C4 Systems
 *
 * SPDX-License-Identifier: GPL-2.0-only
 */

       

# 1 "/home/b/test/sel4test/kernel/include/stdint.h" 1
/*
 * Copyright 2014, General Dynamics C4 Systems
 *
 * SPDX-License-Identifier: GPL-2.0-only
 */

       

# 1 "/home/b/test/sel4test/kernel/include/64/mode/stdint.h" 1
/*
 * Copyright 2020, Data61, CSIRO (ABN 41 687 119 230)
 *
 * SPDX-License-Identifier: GPL-2.0-only
 */

       
# 10 "/home/b/test/sel4test/kernel/include/stdint.h" 2

typedef unsigned char uint8_t;
typedef unsigned short uint16_t;
typedef unsigned int uint32_t;
typedef unsigned long long uint64_t;

typedef signed char int8_t;
typedef signed short int16_t;
typedef signed int int32_t;
typedef signed long long int64_t;






typedef uint64_t uintmax_t;
typedef int64_t intmax_t;
# 10 "/home/b/test/sel4test/kernel/include/basic_types.h" 2
# 1 "/home/b/test/sel4test/kernel/include/arch/arm/arch/types.h" 1
/*
 * Copyright 2014, General Dynamics C4 Systems
 *
 * SPDX-License-Identifier: GPL-2.0-only
 */

       


# 1 "/home/b/test/sel4test/kernel/include/arch/arm/arch/64/mode/types.h" 1
/*
 * Copyright 2020, Data61, CSIRO (ABN 41 687 119 230)
 *
 * SPDX-License-Identifier: GPL-2.0-only
 */

       

# 1 "/home/b/test/sel4test/kernel/include/assert.h" 1
/*
 * Copyright 2014, General Dynamics C4 Systems
 *
 * SPDX-License-Identifier: GPL-2.0-only
 */

       


# 1 "/home/b/test/sel4test/kernel/include/util.h" 1
/*
 * Copyright 2014, General Dynamics C4 Systems
 *
 * SPDX-License-Identifier: GPL-2.0-only
 */

       
# 28 "/home/b/test/sel4test/kernel/include/util.h"
/* There is no difference between using 'ul' or 'lu' as suffix for numbers to
 * enforce a specific type besides the default 'int'. Just when it comes to the
 * printf() format specifiers, '%lu' is the only form that is supported. Thus
 * 'ul' is the preferred suffix to avoid confusion.
 */
# 48 "/home/b/test/sel4test/kernel/include/util.h"
/* Time constants are defined to use the 'unsigned long long'. Rationale is,
 * that the C rules define the calculation result is determined by largest type
 * involved. Enforcing the largest possible type C provides avoids pitfalls with
 * 32-bit overflows when values are getting quite large. Keep in mind that even
 * 2^32 milli-seconds roll over within 50 days, which is an uptime that embedded
 * systems will reach easily and it resembles not even two months in a calendar
 * calculation. In addition, using the largest integer type C currently defines
 * enforces that all calculations results need a cast back to a 32-bit type
 * explicitly. This might feel annoying, but practically it makes code more
 * robust and enforces thinking about potential overflows.
 * Note that at this stage of the includes, we do not have defined the type
 * uint64_t yet, so we can't use any definitions around it, but have to stick to
 * plain C types. Neither moving the time constant definitions behind the
 * uint64_t type definitions nor including the header with the uint64_t
 * definitions here is currently a feasible option.
 */
# 93 "/home/b/test/sel4test/kernel/include/util.h"
/** MODIFIES: */
void __builtin_unreachable(void);






/* Borrowed from linux/include/linux/compiler.h */







/* need that for compiling with c99 instead of gnu99 */


/* Wrapping an int literal such as 0 or 1 in a function to prevent it from being
   simplified away in verification while still allowing the compiler to optimise
   it out and do code elimination on the result. */
static inline int wrap_config_set(int x)
{
    return x;
}

/* Evaluate a CMake configuration setting at compile-time. */






/* Check the existence of a configuration setting, returning one value if it
 * exists and a different one if it does not */





/** MODIFIES:
    FNSPEC
        halt_spec: "\<Gamma> \<turnstile> {} Call halt_'proc {}"
*/
void halt(void) __attribute__((__noreturn__));
void memzero(void *s, unsigned long n);
void *memset(void *s, unsigned long c, unsigned long n) __attribute__((externally_visible));
void *memcpy(void *ptr_dst, const void *ptr_src, unsigned long n) __attribute__((externally_visible));
int __attribute__((__pure__)) strncmp(const char *s1, const char *s2, int n);
long __attribute__((__const__)) char_to_long(char c);
long __attribute__((__pure__)) str_to_long(const char *str);

/* Library functions for counting leading/trailing zeros.
 *
 * GCC/LLVM provides builtin function like __builtin_clzl() for this, which
 * either get translated to machine specific instructions or calls helper
 * functions like __clzsi2() that a compiler library is expected to implement.
 * At the time of writing this comment, the GCC documentation about the compiler
 * library (https://gcc.gnu.org/onlinedocs/gccint/Integer-library-routines.html)
 * is not very detailed and the signatures given for these helper functions
 * appear incorrect. For example, is says "int __clzsi2(unsigned int a)", but
 * both the GCC and LLVM libraries implement it in a way that is independent of
 * the implementation choices for the sizes of `unsigned int`. Instead, it
 * appears that `si` always signifies a 32-bit argument and `di` always
 * signifies a 64-bit argument. Tests with __builtin_clzl() on RISC-V have shown
 * that if 'unsigned long' is 32 bits __builtin_clzl() uses __clzsi2() and if
 * the type is 64 bits __builtin_clzl() uses __clzdi2(). Thus using the types
 * uint32_t and uint64_t from stdint.h in the signatures below is considered the
 * semantically correct way.
 * Note that we only emit actual function implementations for these functions if
 * CONFIG_CLZ_32 etc. are set. Otherwise, the compiler's internal implementation
 * may get used or compilation fails if there is no machine instruction.
 */

__attribute__((__const__)) int __clzsi2(uint32_t x);
__attribute__((__const__)) int __clzdi2(uint64_t x);
__attribute__((__const__)) int __ctzsi2(uint32_t x);
__attribute__((__const__)) int __ctzdi2(uint64_t x);

// Used for compile-time constants, so should always use the builtin.


// Count leading zeros.
// The CONFIG_CLZ_NO_BUILTIN macro may be used to expose the library function
// to the C parser for verification.

// If we use a compiler builtin, we cannot verify it, so we use the following
// annotations to hide the function body from the proofs, and axiomatise its
// behaviour.
// On the other hand, if we use our own implementation instead of the builtin,
// then we want to expose that implementation to the proofs, and therefore hide
// these annotations.
/** MODIFIES: */
/** DONT_TRANSLATE */
/** FNSPEC clzl_spec:
  "\<forall>s. \<Gamma> \<turnstile>
    {\<sigma>. s = \<sigma> \<and> x___unsigned_long_' s \<noteq> 0 }
      \<acute>ret__long :== PROC clzl(\<acute>x)
    \<lbrace> \<acute>ret__long = of_nat (word_clz (x___unsigned_long_' s)) \<rbrace>"
*/

static inline long
__attribute__((__const__)) clzl(unsigned long x)
{







    return __builtin_clzl(x);

}


// See comments on clzl.
/** MODIFIES: */
/** DONT_TRANSLATE */
/** FNSPEC clzll_spec:
  "\<forall>s. \<Gamma> \<turnstile>
    {\<sigma>. s = \<sigma> \<and> x___unsigned_longlong_' s \<noteq> 0 }
      \<acute>ret__longlong :== PROC clzll(\<acute>x)
    \<lbrace> \<acute>ret__longlong = of_nat (word_clz (x___unsigned_longlong_' s)) \<rbrace>"
*/

static inline long long
__attribute__((__const__)) clzll(unsigned long long x)
{



    return __builtin_clzll(x);

}

// Count trailing zeros.

// See comments on clzl.
/** MODIFIES: */
/** DONT_TRANSLATE */
/** FNSPEC ctzl_spec:
  "\<forall>s. \<Gamma> \<turnstile>
    {\<sigma>. s = \<sigma> \<and> x___unsigned_long_' s \<noteq> 0 }
      \<acute>ret__long :== PROC ctzl(\<acute>x)
    \<lbrace> \<acute>ret__long = of_nat (word_ctz (x___unsigned_long_' s)) \<rbrace>"
*/

static inline long
__attribute__((__const__)) ctzl(unsigned long x)
{
# 266 "/home/b/test/sel4test/kernel/include/util.h"
    // Here, we have __builtin_ctzl.
    return __builtin_ctzl(x);

}


// See comments on clzl.
/** MODIFIES: */
/** DONT_TRANSLATE */
/** FNSPEC ctzll_spec:
  "\<forall>s. \<Gamma> \<turnstile>
    {\<sigma>. s = \<sigma> \<and> x___unsigned_longlong_' s \<noteq> 0 }
      \<acute>ret__longlong :== PROC ctzll(\<acute>x)
    \<lbrace> \<acute>ret__longlong = of_nat (word_ctz (x___unsigned_longlong_' s)) \<rbrace>"
*/

static inline long long
__attribute__((__const__)) ctzll(unsigned long long x)
{
# 297 "/home/b/test/sel4test/kernel/include/util.h"
    return __builtin_ctzll(x);

}

int __builtin_popcountl(unsigned long x);

/** DONT_TRANSLATE */
static inline long
__attribute__((__const__)) popcountl(unsigned long mask)
{

    unsigned int count; // c accumulates the total bits set in v
    for (count = 0; mask; count++) {
        mask &= mask - 1; // clear the least significant bit set
    }

    return count;



}



/* Can be used to insert padding to the next L1 cache line boundary */
# 11 "/home/b/test/sel4test/kernel/include/assert.h" 2



void _fail(
    const char *str,
    const char *file,
    unsigned int line,
    const char *function
) __attribute__((__noreturn__));



void _assert_fail(
    const char *assertion,
    const char *file,
    unsigned int line,
    const char *function
) __attribute__((__noreturn__));
# 45 "/home/b/test/sel4test/kernel/include/assert.h"
/* Create an assert that triggers a compile error if the condition fails. We do
 * not include sel4/macros.h that provides SEL4_COMPILE_ASSERT() for two
 * reasons:
 * - The kernel's source internals shall not have any unnecessary dependency on
 *     the user interface headers.
 * - The kernel user API headers aims to be compiler agnostic and stick to the
 *     standard(s). As _Static_assert() is a c11 feature, the c99 used for
 *     kernel compilation would use a helper macro. While this works, it
 *     creates strange error messages when the condition fails. Since kernel
 *     compilation supports just gcc and clang, and both are known to provide
 *     _Static_assert() even in c99, we can just use this.
 *
 * Unfortunately, the C parser does not understand _Static_assert(), so there is
 * still the need for the helper macro there. In addition, the macro
 * unverified_compile_assert() exists, because some compile asserts contain
 * expressions that the C parser cannot handle, too.
 */
# 10 "/home/b/test/sel4test/kernel/include/arch/arm/arch/64/mode/types.h" 2

_Static_assert(sizeof(unsigned long) == 8, "long_is_64bits");




typedef uint64_t timestamp_t;
# 11 "/home/b/test/sel4test/kernel/include/arch/arm/arch/types.h" 2


typedef unsigned long word_t;
typedef signed long sword_t;
/* for printf() formatting */


typedef word_t vptr_t;
typedef word_t paddr_t;
typedef word_t pptr_t;
typedef word_t cptr_t;
typedef word_t node_id_t;
typedef word_t cpu_id_t;
typedef word_t dom_t;

typedef uint8_t hw_asid_t;

enum hwASIDConstants {
    hwASIDMax = 255,
    hwASIDBits = 8
};

typedef struct kernel_frame {
    paddr_t paddr;
    pptr_t pptr;
    int armExecuteNever;
    int userAvailable;
} kernel_frame_t;
# 11 "/home/b/test/sel4test/kernel/include/basic_types.h" 2

/* arch/types.h is supposed to define word_t and _seL4_word_fmt */




/* Using multiple macro layers may look strange, but this is required to make
 * the preprocessor fully evaluate all macro parameters first and then pass the
 * result as parameter to the next macro layer. This allows passing macros as
 * parameters also, and not just plain strings. The final concatenation will
 * always be from the strings behind all macros then - and not the macro names
 * that are passed as parameters.
 */
# 35 "/home/b/test/sel4test/kernel/include/basic_types.h"
/* The C parser from the verification toolchain requires declaring word_t
 * constants without casting integer values to word_t. Since the printf() format
 * specifiers are aligned with the C integer type suffixes, _seL4_word_fmt can
 * be used there also.
 */



enum _bool {
    false = 0,
    true = 1
};
typedef word_t bool_t;

/**
 * A region [start..end) of kernel-virtual memory.
 *
 * Empty when start == end. If end < start, the region wraps around, that is,
 * it represents the addresses in the set [start..-1] union [0..end). This is
 * possible after address translation and fine for e.g. device memory regions.
 */
typedef struct region {
    pptr_t start; /* inclusive */
    pptr_t end; /* exclusive */
} region_t;

/** A region [start..end) of physical memory addresses. */
typedef struct p_region {
    paddr_t start; /* inclusive */
    paddr_t end; /* exclusive */
} p_region_t;

/** A region [start..end) of user-virtual addresses. */
typedef struct v_region {
    vptr_t start; /* inclusive */
    vptr_t end; /* exclusive */
} v_region_t;




/* equivalent to a word_t except that we tell the compiler that we may alias with
 * any other type (similar to a char pointer) */
typedef word_t __attribute__((__may_alias__)) word_t_may_alias;

/* for libsel4 headers that the kernel shares */
typedef uint8_t seL4_Uint8;
typedef uint16_t seL4_Uint16;
typedef uint32_t seL4_Uint32;
typedef word_t seL4_Word;
typedef cptr_t seL4_CPtr;
typedef node_id_t seL4_NodeId;
typedef paddr_t seL4_PAddr;
typedef dom_t seL4_Domain;
# 10 "/home/b/test/sel4test/kernel/include/types.h" 2
# 1 "/home/b/test/sel4test/kernel/include/compound_types.h" 1
/*
 * Copyright 2014, General Dynamics C4 Systems
 *
 * SPDX-License-Identifier: GPL-2.0-only
 */

       


# 1 "/home/b/test/sel4test/kernel/include/api/types.h" 1
/*
 * Copyright 2014, General Dynamics C4 Systems
 *
 * SPDX-License-Identifier: GPL-2.0-only
 */

       




# 1 "/home/b/test/sel4test/cbuild/kernel/generated_prune/sel4/shared_types_gen.h" 1
# 13 "/home/b/test/sel4test/kernel/include/api/types.h" 2
# 1 "/home/b/test/sel4test/kernel/include/arch/arm/arch/api/types.h" 1
/*
 * Copyright 2014, General Dynamics C4 Systems
 *
 * SPDX-License-Identifier: GPL-2.0-only
 */

       

# 1 "/home/b/test/sel4test/kernel/libsel4/include/sel4/objecttype.h" 1
/*
 * Copyright 2020, Data61, CSIRO (ABN 41 687 119 230)
 *
 * SPDX-License-Identifier: BSD-2-Clause
 */

       
typedef enum api_object {
    seL4_UntypedObject,
    seL4_TCBObject,
    seL4_EndpointObject,
    seL4_NotificationObject,
    seL4_CapTableObject,




    seL4_NonArchObjectTypeCount,
} seL4_ObjectType;

__attribute__((deprecated("use seL4_NotificationObject"))) static const seL4_ObjectType seL4_AsyncEndpointObject =
    seL4_NotificationObject;

typedef seL4_Word api_object_t;
# 10 "/home/b/test/sel4test/kernel/include/arch/arm/arch/api/types.h" 2
# 1 "/home/b/test/sel4test/kernel/libsel4/sel4_arch_include/aarch64/sel4/sel4_arch/objecttype.h" 1
/*
 * Copyright 2020, Data61, CSIRO (ABN 41 687 119 230)
 *
 * SPDX-License-Identifier: BSD-2-Clause
 */

       

typedef enum _mode_object {
    seL4_ARM_HugePageObject = seL4_NonArchObjectTypeCount,
    seL4_ARM_VSpaceObject,
    seL4_ModeObjectTypeCount
} seL4_ModeObjectType;
# 11 "/home/b/test/sel4test/kernel/include/arch/arm/arch/api/types.h" 2
# 1 "/home/b/test/sel4test/kernel/libsel4/arch_include/arm/sel4/arch/objecttype.h" 1
/*
 * Copyright 2020, Data61, CSIRO (ABN 41 687 119 230)
 *
 * SPDX-License-Identifier: BSD-2-Clause
 */

       



typedef enum _object {
    seL4_ARM_SmallPageObject = seL4_ModeObjectTypeCount,
    seL4_ARM_LargePageObject,




    seL4_ARM_PageTableObject,






    seL4_ObjectTypeCount
} seL4_ArchObjectType;

typedef seL4_Word object_t;
# 12 "/home/b/test/sel4test/kernel/include/arch/arm/arch/api/types.h" 2

enum asidConstants {
    asidInvalid = 0
};

typedef word_t asid_t;
# 14 "/home/b/test/sel4test/kernel/include/api/types.h" 2

# 1 "/home/b/test/sel4test/kernel/libsel4/include/sel4/macros.h" 1
/*
 * Copyright 2020, Data61, CSIRO (ABN 41 687 119 230)
 *
 * SPDX-License-Identifier: BSD-2-Clause
 */

       
# 45 "/home/b/test/sel4test/kernel/libsel4/include/sel4/macros.h"
/* _Static_assert() is a c11 feature. Since the kernel is currently compiled
 * with c99, we have to emulate it. */
# 59 "/home/b/test/sel4test/kernel/libsel4/include/sel4/macros.h"
/*
 * Some compilers attempt to pack enums into the smallest possible type.
 * For ABI compatibility with the kernel, we need to ensure they remain
 * the same size as a 'long'.
 */
# 16 "/home/b/test/sel4test/kernel/include/api/types.h" 2
# 1 "/home/b/test/sel4test/kernel/libsel4/include/sel4/constants.h" 1
/*
 * Copyright 2020, Data61, CSIRO (ABN 41 687 119 230)
 *
 * SPDX-License-Identifier: BSD-2-Clause
 */

       







/* API arg values for breakpoint API, "type" arguments. */
typedef enum {
    seL4_DataBreakpoint = 0,
    seL4_InstructionBreakpoint,
    seL4_SingleStep,
    seL4_SoftwareBreakRequest,
    _enum_pad_seL4_BreakpointType = ((1ULL << ((sizeof(long)*8) - 1)) - 1)
} seL4_BreakpointType;

/* API arg values for breakpoint API, "access" arguments. */
typedef enum {
    seL4_BreakOnRead = 0,
    seL4_BreakOnWrite,
    seL4_BreakOnReadWrite,
    seL4_MaxBreakpointAccess,
    _enum_pad_seL4_BreakpointAccess = ((1ULL << ((sizeof(long)*8) - 1)) - 1)
} seL4_BreakpointAccess;

/* Format of a debug-exception message. */
typedef enum {
    seL4_DebugException_FaultIP,
    seL4_DebugException_ExceptionReason,
    seL4_DebugException_TriggerAddress,
    seL4_DebugException_BreakpointNumber,
    seL4_DebugException_Length,
    _enum_pad_seL4_DebugException_Msg = ((1ULL << ((sizeof(long)*8) - 1)) - 1)
} seL4_DebugException_Msg;


enum priorityConstants {
    seL4_InvalidPrio = -1,
    seL4_MinPrio = 0,
    seL4_MaxPrio = 256 - 1
};

/* seL4_MessageInfo_t defined in api/shared_types.bf */

enum seL4_MsgLimits {
    seL4_MsgLengthBits = 7,
    seL4_MsgExtraCapBits = 2,
    seL4_MsgMaxLength = 120
};



/* seL4_CapRights_t defined in shared_types_*.bf */


typedef enum {
    seL4_NoFailure = 0,
    seL4_InvalidRoot,
    seL4_MissingCapability,
    seL4_DepthMismatch,
    seL4_GuardMismatch,
    _enum_pad_seL4_LookupFailureType = ((1ULL << ((sizeof(long)*8) - 1)) - 1),
} seL4_LookupFailureType;

/* Flags to be used with seL4_TCB_Set_Flags */
typedef enum {
    seL4_TCBFlag_NoFlag = 0x0,
    seL4_TCBFlag_fpuDisabled = 0x1,

    _enum_pad_seL4_TCBFlag = ((1ULL << ((sizeof(long)*8) - 1)) - 1),
    seL4_TCBFlag_MASK = seL4_TCBFlag_NoFlag

                        | seL4_TCBFlag_fpuDisabled

} seL4_TCBFlag;
# 17 "/home/b/test/sel4test/kernel/include/api/types.h" 2
# 1 "/home/b/test/sel4test/kernel/libsel4/include/sel4/shared_types.h" 1
/*
 * Copyright 2020, Data61, CSIRO (ABN 41 687 119 230)
 *
 * SPDX-License-Identifier: BSD-2-Clause
 */

       

/* this file is shared between the kernel and libsel4 */

typedef struct seL4_IPCBuffer_ {
    seL4_MessageInfo_t tag;
    seL4_Word msg[seL4_MsgMaxLength];
    seL4_Word userData;
    seL4_Word caps_or_badges[((1ul<<(seL4_MsgExtraCapBits))-1)];
    seL4_CPtr receiveCNode;
    seL4_CPtr receiveIndex;
    seL4_Word receiveDepth;
} seL4_IPCBuffer __attribute__((__aligned__(sizeof(struct seL4_IPCBuffer_))));

typedef enum {
    seL4_CapFault_IP,
    seL4_CapFault_Addr,
    seL4_CapFault_InRecvPhase,
    seL4_CapFault_LookupFailureType,
    seL4_CapFault_BitsLeft,
    seL4_CapFault_DepthMismatch_BitsFound,
    seL4_CapFault_GuardMismatch_GuardFound = seL4_CapFault_DepthMismatch_BitsFound,
    seL4_CapFault_GuardMismatch_BitsFound,
    _enum_pad_seL4_CapFault_Msg = ((1ULL << ((sizeof(long)*8) - 1)) - 1),
} seL4_CapFault_Msg;
# 18 "/home/b/test/sel4test/kernel/include/api/types.h" 2
# 1 "/home/b/test/sel4test/kernel/include/machine/io.h" 1
/*
 * Copyright 2014, General Dynamics C4 Systems
 *
 * SPDX-License-Identifier: GPL-2.0-only
 */

       




/* io for dumping capdl */
unsigned char kernel_getDebugChar(void);






# 1 "/home/b/test/sel4test/kernel/include/stdarg.h" 1
/*
 * Copyright 2014, General Dynamics C4 Systems
 *
 * SPDX-License-Identifier: GPL-2.0-only
 */

       





typedef __builtin_va_list va_list;
# 21 "/home/b/test/sel4test/kernel/include/machine/io.h" 2

/* the actual output function */
void kernel_putDebugChar(unsigned char c);

/* This is the actual implementation of the kernel printing API. It must never
 * be called directly from anywhere except the function defined in this file.
 */
int impl_kvprintf(const char *format, va_list ap);
int impl_ksnvprintf(char *str, word_t size, const char *format, va_list ap);

/*
 *------------------------------------------------------------------------------
 * Kernel printing API
 *------------------------------------------------------------------------------
 */

/* Writes a character to the kernel output channel. This is used to implement
 * the syscall SysDebugPutChar.
 */
static inline void kernel_putchar(
    char c)
{
    /* Write to target specific debug output channel. */
    kernel_putDebugChar(c);
}

/* Writes a character to the active output channel. This is used by all code
 * related to printf(). Contrary to the common signature of putchar(), there is
 * no return value here.
 */
static inline void putchar(
    char c)
{
    /* Write to target specific debug output channel. Purposely, we do not call
     * kernel_putchar() here, as the kernel printf() channel is semantically
     * different from the syscall SysDebugPutChar channel. The unification
     * of both channels happens at the lower layer eventually
     */
    kernel_putDebugChar(c);
}

/* Writes the string and a trailing newline. There is no point to enforce a
 * kernel_puts(), as this is just a wrapper for putchar() anyway.
 */
static inline int puts(
    const char *str)
{
    if (str) {
        while (*str) {
            putchar(*str++);
        }
    }
    putchar('\n');
    /* Standards define that a non-negative number is returned on success. */
    return 0;
}

/* There should only be a kprintf() that all kernel code must use for printing,
 * but for convenience we provide a printf() here.
 */
static inline __attribute__((format(printf, 1, 2))) int printf(
    const char *format,
    ...)
{
    va_list args;
    __builtin_va_start(args,format);
    int ret = impl_kvprintf(format, args); /* will call putchar() eventually */
    __builtin_va_end(args);
    return ret;
}

/* Provide the standard snprintf() for write formatted data into a buffer, which
 * can then be printed or stored.
 */
static inline __attribute__((format(printf, 3, 4))) int snprintf(
    char *buf,
    word_t size,
    const char *format,
    ...)
{
    va_list args;
    __builtin_va_start(args,format);
    int ret = impl_ksnvprintf(buf, size, format, args);
    __builtin_va_end(args);
    return ret;
}
# 19 "/home/b/test/sel4test/kernel/include/api/types.h" 2

/* seL4_CapRights_t defined in mode/api/shared_types.bf */

typedef word_t prio_t;

/* The kernel uses ticks_t internally to represent time to make it easy to
 * interact with hardware timers. The userland API uses time in micro seconds,
 * which is represented by time_t in the kernel.
 */
typedef uint64_t ticks_t;
typedef uint64_t time_t;

enum domainConstants {
    minDom = 0,
    maxDom = 1 - 1,
    /* To analyse branches of control flow decisions based on the number of
     * domains without knowing their exact number, verification needs a C name
     * to relate to higher-level specs. */
    numDomains = 1
};

struct cap_transfer {
    cptr_t ctReceiveRoot;
    cptr_t ctReceiveIndex;
    word_t ctReceiveDepth;
};
typedef struct cap_transfer cap_transfer_t;

enum ctLimits {
    capTransferDataSize = 3
};

static inline seL4_CapRights_t __attribute__((__const__)) rightsFromWord(word_t w)
{
    seL4_CapRights_t seL4_CapRights;

    seL4_CapRights.words[0] = w;
    return seL4_CapRights;
}

static inline word_t __attribute__((__const__)) wordFromRights(seL4_CapRights_t seL4_CapRights)
{
    return seL4_CapRights.words[0] & ((1ul << (4)) - 1ul);
}

static inline cap_transfer_t __attribute__((__pure__)) capTransferFromWords(word_t *wptr)
{
    cap_transfer_t transfer;

    transfer.ctReceiveRoot = (cptr_t)wptr[0];
    transfer.ctReceiveIndex = (cptr_t)wptr[1];
    transfer.ctReceiveDepth = wptr[2];
    return transfer;
}

static inline seL4_MessageInfo_t __attribute__((__const__)) messageInfoFromWord_raw(word_t w)
{
    seL4_MessageInfo_t mi;

    mi.words[0] = w;
    return mi;
}

static inline seL4_MessageInfo_t __attribute__((__const__)) messageInfoFromWord(word_t w)
{
    seL4_MessageInfo_t mi;
    word_t len;

    mi.words[0] = w;

    len = seL4_MessageInfo_get_length(mi);
    if (len > seL4_MsgMaxLength) {
        mi = seL4_MessageInfo_set_length(mi, seL4_MsgMaxLength);
    }

    return mi;
}

static inline word_t __attribute__((__const__)) wordFromMessageInfo(seL4_MessageInfo_t mi)
{
    return mi.words[0];
}
# 113 "/home/b/test/sel4test/kernel/include/api/types.h"
/*
 * thread name is only available if the kernel is built in debug mode.
 */
# 132 "/home/b/test/sel4test/kernel/include/api/types.h"
/*
 * Print to serial a message helping userspace programmers to determine why the
 * kernel is not performing their requested operation.
 */
# 11 "/home/b/test/sel4test/kernel/include/compound_types.h" 2
# 1 "/home/b/test/sel4test/kernel/include/object/structures.h" 1
/*
 * Copyright 2014, General Dynamics C4 Systems
 *
 * SPDX-License-Identifier: GPL-2.0-only
 */

       




# 1 "/home/b/test/sel4test/cbuild/kernel/generated_prune/arch/object/structures_gen.h" 1
# 13 "/home/b/test/sel4test/kernel/include/object/structures.h" 2


# 1 "/home/b/test/sel4test/kernel/libsel4/arch_include/arm/sel4/arch/constants.h" 1
/*
 * Copyright 2020, Data61, CSIRO (ABN 41 687 119 230)
 *
 * SPDX-License-Identifier: BSD-2-Clause
 */

       
# 16 "/home/b/test/sel4test/kernel/include/object/structures.h" 2
# 1 "/home/b/test/sel4test/kernel/libsel4/sel4_arch_include/aarch64/sel4/sel4_arch/constants.h" 1
/*
 * Copyright 2020, Data61, CSIRO (ABN 41 687 119 230)
 *
 * SPDX-License-Identifier: BSD-2-Clause
 */

       





/* format of an unknown syscall message */
typedef enum {
    seL4_UnknownSyscall_X0,
    seL4_UnknownSyscall_X1,
    seL4_UnknownSyscall_X2,
    seL4_UnknownSyscall_X3,
    seL4_UnknownSyscall_X4,
    seL4_UnknownSyscall_X5,
    seL4_UnknownSyscall_X6,
    seL4_UnknownSyscall_X7,
    seL4_UnknownSyscall_FaultIP,
    seL4_UnknownSyscall_SP,
    seL4_UnknownSyscall_LR,
    seL4_UnknownSyscall_SPSR,
    seL4_UnknownSyscall_Syscall,
    /* length of an unknown syscall message */
    seL4_UnknownSyscall_Length,
    _enum_pad_seL4_UnknownSyscall_Msg = ((1ULL << ((sizeof(long)*8) - 1)) - 1),
} seL4_UnknownSyscall_Msg;

/* format of a user exception message */
typedef enum {
    seL4_UserException_FaultIP,
    seL4_UserException_SP,
    seL4_UserException_SPSR,
    seL4_UserException_Number,
    seL4_UserException_Code,
    /* length of a user exception */
    seL4_UserException_Length,
    _enum_pad_seL4_UserException_Msg = ((1ULL << ((sizeof(long)*8) - 1)) - 1),
} seL4_UserException_Msg;

/* format of a vm fault message */
typedef enum {
    seL4_VMFault_IP,
    seL4_VMFault_Addr,
    seL4_VMFault_PrefetchFault,
    seL4_VMFault_FSR,
    seL4_VMFault_Length,
    _enum_pad_seL4_VMFault_Msg = ((1ULL << ((sizeof(long)*8) - 1)) - 1),
} seL4_VMFault_Msg;

typedef enum {
    seL4_VGICMaintenance_IDX,
    seL4_VGICMaintenance_Length,
    _enum_pad_seL4_VGICMaintenance_Msg = ((1ULL << ((sizeof(long)*8) - 1)) - 1),
} seL4_VGICMaintenance_Msg;

typedef enum {
    seL4_VPPIEvent_IRQ,
    seL4_VPPIEvent_Length,
    _enum_pad_seL4_VPPIEvent_Msg = ((1ULL << ((sizeof(long)*8) - 1)) - 1),
} seL4_VPPIEvent_Msg;


typedef enum {
    seL4_VCPUFault_HSR,
    seL4_VCPUFault_Length,
    _enum_pad_seL4_VCPUFault_Msg = ((1ULL << ((sizeof(long)*8) - 1)) - 1),
} seL4_VCPUFault_Msg;

typedef enum {
    /* System control registers EL1 */
    seL4_VCPUReg_SCTLR = 0,
    seL4_VCPUReg_CPACR,
    seL4_VCPURegSaveRange_start, /* begin vcpu save/restore reg range */
    seL4_VCPUReg_TTBR0 = seL4_VCPURegSaveRange_start,
    seL4_VCPUReg_TTBR1,
    seL4_VCPUReg_TCR,
    seL4_VCPUReg_MAIR,
    seL4_VCPUReg_AMAIR,
    seL4_VCPUReg_CIDR,
    seL4_VCPUReg_ACTLR,

    /* exception handling registers EL1 */
    seL4_VCPUReg_AFSR0,
    seL4_VCPUReg_AFSR1,
    seL4_VCPUReg_ESR,
    seL4_VCPUReg_FAR,
    seL4_VCPUReg_ISR,
    seL4_VCPUReg_VBAR,

    /* thread pointer/ID registers EL0/EL1 */
    seL4_VCPUReg_TPIDR_EL1,

    /* Virtualisation Multiprocessor ID Register */
    seL4_VCPUReg_VMPIDR_EL2,

    /* general registers x0 to x30 have been saved by traps.S */
    seL4_VCPUReg_SP_EL1,
    seL4_VCPUReg_ELR_EL1,
    seL4_VCPUReg_SPSR_EL1, // 32-bit
    seL4_VCPURegSaveRange_end = seL4_VCPUReg_SPSR_EL1, /* end vcpu save/restore reg range */

    /* generic timer registers, to be completed */
    seL4_VCPUReg_CNTV_CTL,
    seL4_VCPUReg_CNTV_CVAL,
    seL4_VCPUReg_CNTVOFF,
    seL4_VCPUReg_CNTKCTL_EL1,

    seL4_VCPUReg_Num,
    _enum_pad_seL4_VCPUReg = ((1ULL << ((sizeof(long)*8) - 1)) - 1),
} seL4_VCPUReg;
# 168 "/home/b/test/sel4test/kernel/libsel4/sel4_arch_include/aarch64/sel4/sel4_arch/constants.h"
/* object sizes - 2^n */
# 213 "/home/b/test/sel4test/kernel/libsel4/sel4_arch_include/aarch64/sel4/sel4_arch/constants.h"
/* word size */


/* Untyped size limits */




typedef int __assert_failed_seL4_PageTableEntryBits_seL4_PageTableIndexBits_seL4_PageTableBits[((3) + (9) == 12) ? 1 : -1] __attribute__((unused));;
typedef int __assert_failed_seL4_WordSizeBits_seL4_ASIDPoolIndexBits_seL4_ASIDPoolBits[((3) + (9) == 12) ? 1 : -1] __attribute__((unused));;
typedef int __assert_failed_seL4_VSpaceEntryBits_seL4_VSpaceIndexBits_seL4_VSpaceBits[((3) + (9) == 12) ? 1 : -1] __attribute__((unused));;
# 233 "/home/b/test/sel4test/kernel/libsel4/sel4_arch_include/aarch64/sel4/sel4_arch/constants.h"
/* IPC buffer is 1024 bytes, giving size bits of 10 */
# 255 "/home/b/test/sel4test/kernel/libsel4/sel4_arch_include/aarch64/sel4/sel4_arch/constants.h"
/* First address in the virtual address space that is not accessible to user level */
# 17 "/home/b/test/sel4test/kernel/include/object/structures.h" 2
# 1 "/home/b/test/sel4test/kernel/include/benchmark/benchmark_utilisation_.h" 1
/*
 * Copyright 2020, Data61, CSIRO (ABN 41 687 119 230)
 *
 * SPDX-License-Identifier: GPL-2.0-only
 */

       
# 18 "/home/b/test/sel4test/kernel/include/object/structures.h" 2

enum irq_state {
    IRQInactive = 0,
    IRQSignal = 1,
    IRQTimer = 2,



    IRQReserved
};
typedef word_t irq_state_t;

typedef struct dschedule {
    dom_t domain;
    word_t length;
} dschedule_t;

enum asidSizeConstants {
    asidHighBits = 7,
    asidLowBits = 9
};

/* Arch-independent object types */
enum endpoint_state {
    EPState_Idle = 0,
    EPState_Send = 1,
    EPState_Recv = 2
};
typedef word_t endpoint_state_t;

enum notification_state {
    NtfnState_Idle = 0,
    NtfnState_Waiting = 1,
    NtfnState_Active = 2
};
typedef word_t notification_state_t;
# 68 "/home/b/test/sel4test/kernel/include/object/structures.h"
// We would like the actual 'tcb' region (the portion that contains the tcb_t) of the tcb
// to be as large as possible, but it still needs to be aligned. As the TCB object contains
// two sub objects the largest we can make either sub object whilst preserving size alignment
// is half the total size. To halve an object size defined in bits we just subtract 1
//
// A diagram of a TCB kernel object that is created from untyped:
//  _______________________________________
// |     |             |                   |
// |     |             |                   |
// |cte_t|   unused    |       tcb_t       |
// |     |(debug_tcb_t)|                   |
// |_____|_____________|___________________|
// 0     a             b                   c
// a = tcbCNodeEntries * sizeof(cte_t)
// b = BIT(TCB_SIZE_BITS)
// c = BIT(seL4_TCBBits)
//






/* Generate a tcb_t or cte_t pointer from a tcb block reference */




/* Generate a cte_t pointer from a tcb_t pointer */
# 112 "/home/b/test/sel4test/kernel/include/object/structures.h"
static inline cap_t __attribute__((__const__)) Zombie_new(word_t number, word_t type, word_t ptr)
{
    word_t mask;

    if (type == (1ul << (6))) {
        mask = ((1ul << (4 + 1)) - 1ul);
    } else {
        mask = ((1ul << (type + 1)) - 1ul);
    }

    return cap_zombie_cap_new((ptr & ~mask) | (number & mask), type);
}

static inline word_t __attribute__((__const__)) cap_zombie_cap_get_capZombieBits(cap_t cap)
{
    word_t type = cap_zombie_cap_get_capZombieType(cap);
    if (type == (1ul << (6))) {
        return 4;
    }
    return ((type) & ((1ul << (6)) - 1ul)); /* cnode radix */
}

static inline word_t __attribute__((__const__)) cap_zombie_cap_get_capZombieNumber(cap_t cap)
{
    word_t radix = cap_zombie_cap_get_capZombieBits(cap);
    return cap_zombie_cap_get_capZombieID(cap) & ((1ul << (radix + 1)) - 1ul);
}

static inline word_t __attribute__((__const__)) cap_zombie_cap_get_capZombiePtr(cap_t cap)
{
    word_t radix = cap_zombie_cap_get_capZombieBits(cap);
    return cap_zombie_cap_get_capZombieID(cap) & ~((1ul << (radix + 1)) - 1ul);
}

static inline cap_t __attribute__((__const__)) cap_zombie_cap_set_capZombieNumber(cap_t cap, word_t n)
{
    word_t radix = cap_zombie_cap_get_capZombieBits(cap);
    word_t ptr = cap_zombie_cap_get_capZombieID(cap) & ~((1ul << (radix + 1)) - 1ul);
    return cap_zombie_cap_set_capZombieID(cap, ptr | (n & ((1ul << (radix + 1)) - 1ul)));
}

/* Capability table entry (CTE) */
struct cte {
    cap_t cap;
    mdb_node_t cteMDBNode;
};
typedef struct cte cte_t;



/* Thread state */
enum _thread_state {
    ThreadState_Inactive = 0,
    ThreadState_Running,
    ThreadState_Restart,
    ThreadState_BlockedOnReceive,
    ThreadState_BlockedOnSend,
    ThreadState_BlockedOnReply,
    ThreadState_BlockedOnNotification,



    ThreadState_IdleThreadState
};
typedef word_t _thread_state_t;

/* A TCB CNode and a TCB are always allocated together, and adjacently.
 * The CNode comes first. */
enum tcb_cnode_index {
    /* CSpace root */
    tcbCTable = 0,

    /* VSpace root */
    tcbVTable = 1,
# 197 "/home/b/test/sel4test/kernel/include/object/structures.h"
    /* Reply cap slot */
    tcbReply = 2,

    /* TCB of most recent IPC sender */
    tcbCaller = 3,

    /* IPC buffer cap slot */
    tcbBuffer = 4,

    tcbCNodeEntries
};
typedef word_t tcb_cnode_index_t;

# 1 "/home/b/test/sel4test/kernel/include/arch/arm/arch/object/structures.h" 1
/*
 * Copyright 2014, General Dynamics C4 Systems
 *
 * SPDX-License-Identifier: GPL-2.0-only
 */

       

# 1 "/home/b/test/sel4test/kernel/include/arch/arm/arch/64/mode/object/structures.h" 1
/*
 * Copyright 2020, Data61, CSIRO (ABN 41 687 119 230)
 *
 * SPDX-License-Identifier: GPL-2.0-only
 */

       







# 1 "/home/b/test/sel4test/cbuild/kernel/generated_prune/arch/object/structures_gen.h" 1
# 16 "/home/b/test/sel4test/kernel/include/arch/arm/arch/64/mode/object/structures.h" 2
# 1 "/home/b/test/sel4test/kernel/include/arch/arm/arch/machine/hardware.h" 1
/*
 * Copyright 2014, General Dynamics C4 Systems
 *
 * SPDX-License-Identifier: GPL-2.0-only
 */

       


# 1 "/home/b/test/sel4test/kernel/include/arch/arm/arch/64/mode/machine/hardware.h" 1
/*
 * Copyright 2020, Data61, CSIRO (ABN 41 687 119 230)
 *
 * SPDX-License-Identifier: GPL-2.0-only
 */

       






/* Extract the n-level PT index from a virtual address:
 * - n is page table level counting from the root page table,
 * - NUM_PT_LEVELS are either 3 or 4 page table levels depending
 *   on whether the address range being translated is 48bits, 44 bits or 40 bits.
 * - If translating an address in the kernel addrspace NUM_PT_LEVELS = 4 always.
 * - PageTables always have 512 slots (PT_INDEX_BITS = 9) but if there are only
 *   3 total levels then the root level is implemented with 4 concatenated tables
 *   meaning 2048 slots (UPT_LEVELS = 3 => seL4_VSpaceIndexBits = 12)
 *
 * PT_LEVEL_SHIFT(n) == PT_INDEX_BITS * (NUM_PT_LEVELS - n) + seL4_PageBits
 * GET_PT_INDEX(addr, n) == (addr >> PT_LEVEL_SHIFT(n)) & MASK(PT_INDEX_BITS)
 */
# 47 "/home/b/test/sel4test/kernel/include/arch/arm/arch/64/mode/machine/hardware.h"
/* Control register fields */
# 64 "/home/b/test/sel4test/kernel/include/arch/arm/arch/64/mode/machine/hardware.h"
enum vm_page_size {
    ARMSmallPage,
    ARMLargePage,
    ARMHugePage
};
typedef word_t vm_page_size_t;

enum frameSizeConstants {
    ARMSmallPageBits = 12,
    ARMLargePageBits = 21,
    ARMHugePageBits = 30
};

static inline word_t __attribute__((__const__)) pageBitsForSize(vm_page_size_t pagesize)
{
    switch (pagesize) {
    case ARMSmallPage:
        return ARMSmallPageBits;

    case ARMLargePage:
        return ARMLargePageBits;

    case ARMHugePage:
        return ARMHugePageBits;

    default:
        _fail("Invalid page size", "/home/b/test/sel4test/kernel/include/arch/arm/arch/64/mode/machine/hardware.h", 90, __func__);
    }
}
# 11 "/home/b/test/sel4test/kernel/include/arch/arm/arch/machine/hardware.h" 2


enum vm_fault_type {
    ARMDataAbort = 0,
    ARMPrefetchAbort = 1
};
typedef word_t vm_fault_type_t;
# 36 "/home/b/test/sel4test/kernel/include/arch/arm/arch/machine/hardware.h"
/*
 * Used to align the big kernel lock to the exclusive reservation granule size.
 * Without this nearby writes can delay atomic operations implemented with looping
 * exclusive load/store instructions for an undefined time.
 *
 * Usually equal to L1_CACHE_LINE_SIZE, but 2k is the maximum for SMP systems.
 *
 * ARM Architecture Reference Manual ARMv7-A and ARMv7-R edition, chapter A3.4.5
 * Load-Exclusive and Store-Exclusive usage restrictions, page 122 states:
 *
 * "The architecture sets an upper limit of 2048 bytes on the size of a region
 *  that can be marked as exclusive."
 *
 * ARM Architecture Reference Manual ARMv8 for A-profile architecture, chapter
 * B2.9.5 Load-Exclusive and Store-Exclusive instruction usage restrictions,
 * page 216 states:
 *
 * "The architecture sets an upper limit of 2048 bytes on the Exclusives
 *  reservation granule that can be marked as exclusive."
 */
# 17 "/home/b/test/sel4test/kernel/include/arch/arm/arch/64/mode/object/structures.h" 2
# 1 "/home/b/test/sel4test/kernel/include/arch/arm/arch/machine/registerset.h" 1
/*
 * Copyright 2014, General Dynamics C4 Systems
 *
 * SPDX-License-Identifier: GPL-2.0-only
 */

       

# 1 "/home/b/test/sel4test/kernel/include/arch/arm/arch/64/mode/machine/registerset.h" 1
/*
 * Copyright 2020, Data61, CSIRO (ABN 41 687 119 230)
 *
 * SPDX-License-Identifier: GPL-2.0-only
 */

       


# 1 "/home/b/test/sel4test/kernel/include/arch/arm/arch/machine/debug_conf.h" 1
/*
 * Copyright 2020, Data61, CSIRO (ABN 41 687 119 230)
 *
 * SPDX-License-Identifier: GPL-2.0-only
 */

       



/* These are used to force specific outcomes for various combinations of
 * settings for the state of CONFIG_ARM_HYPERVISOR_SUPPORT,
 * CONFIG_ARM_HYP_ENABLE_VCPU_CP14_SAVE_AND_RESTORE and
 * CONFIG_HARDWARE_DEBUG_API.
 */
# 37 "/home/b/test/sel4test/kernel/include/arch/arm/arch/machine/debug_conf.h"
/* If HARDWARE_DEBUG_API is set, then we must save/retore native threads. */
# 11 "/home/b/test/sel4test/kernel/include/arch/arm/arch/64/mode/machine/registerset.h" 2

/* CurrentEL register */



/* PSTATE register */
# 26 "/home/b/test/sel4test/kernel/include/arch/arm/arch/64/mode/machine/registerset.h"
/* DAIF register */






/* ESR register */
# 44 "/home/b/test/sel4test/kernel/include/arch/arm/arch/64/mode/machine/registerset.h"
/* ID_AA64PFR0_EL1 register */



/* CPACR_EL1 register */


/*
 * We cannot allow async aborts in the verified kernel, but they are useful
 * in identifying invalid memory access bugs so we enable them in debug mode.
 */
# 69 "/home/b/test/sel4test/kernel/include/arch/arm/arch/64/mode/machine/registerset.h"
/* Offsets within the user context, these need to match the order in
 * register_t below */
# 85 "/home/b/test/sel4test/kernel/include/arch/arm/arch/64/mode/machine/registerset.h"
# 1 "/home/b/test/sel4test/kernel/libsel4/sel4_plat_include/phytium-pi/sel4/plat/api/constants.h" 1
/*

 * Copyright 2025, seL4 Project

 *

 * SPDX-License-Identifier: BSD-2-Clause

 */
# 7 "/home/b/test/sel4test/kernel/libsel4/sel4_plat_include/phytium-pi/sel4/plat/api/constants.h"
       


# 1 "/home/b/test/sel4test/kernel/libsel4/arch_include/arm/sel4/arch/constants_cortex_a72.h" 1
/*
 * Copyright 2020, Data61, CSIRO (ABN 41 687 119 230)
 *
 * SPDX-License-Identifier: BSD-2-Clause
 */

       







/* Cortex-A72 Manual, Section 10.3 */
# 11 "/home/b/test/sel4test/kernel/libsel4/sel4_plat_include/phytium-pi/sel4/plat/api/constants.h" 2





/* otherwise this is defined at the arch level */
# 86 "/home/b/test/sel4test/kernel/include/arch/arm/arch/64/mode/machine/registerset.h" 2

/* These are the indices of the registers in the saved thread context.
 * The values are determined by the order in which they're saved in the trap handler. */
enum _register {
    X0 = 0, /* 0x00 */
    capRegister = 0,
    badgeRegister = 0,

    X1 = 1, /* 0x08 */
    msgInfoRegister = 1,

    X2 = 2, /* 0x10 */
    X3 = 3, /* 0x18 */
    X4 = 4, /* 0x20 */
    X5 = 5, /* 0x28 */
    X6 = 6, /* 0x30 */



    X7 = 7, /* 0x38 */
    X8 = 8, /* 0x40 */



    X9 = 9, /* 0x48 */
    X10 = 10, /* 0x50 */
    X11 = 11, /* 0x58 */
    X12 = 12, /* 0x60 */
    X13 = 13, /* 0x68 */
    X14 = 14, /* 0x70 */
    X15 = 15, /* 0x78 */
    X16 = 16, /* 0x80 */
    X17 = 17, /* 0x88 */
    X18 = 18, /* 0x90 */
    X19 = 19, /* 0x98 */
    X20 = 20, /* 0xa0 */
    X21 = 21, /* 0xa8 */
    X22 = 22, /* 0xb0 */
    X23 = 23, /* 0xb8 */
    X24 = 24, /* 0xc0 */
    X25 = 25, /* 0xc8 */
    X26 = 26, /* 0xd0 */
    X27 = 27, /* 0xd8 */
    X28 = 28, /* 0xe0 */
    X29 = 29, /* 0xe8 */

    X30 = 30, /* 0xf0 */
    LR = 30,

    /* End of GP registers, the following are additional kernel-saved state. */

    SP_EL0 = 31, /* 0xf8 */
    ELR_EL1 = 32, /* 0x100 */
    NextIP = 32, /* LR_svc */
    SPSR_EL1 = 33, /* 0x108 */

    FaultIP = 34, /* 0x110 */
    /* user readable/writable thread ID register.
     * name comes from the ARM manual */
    TPIDR_EL0 = 35,
    /* user readonly thread ID register. */
    TPIDRRO_EL0 = 36,

    TLS_BASE = TPIDR_EL0,



    n_contextRegisters = 37,
};



_Static_assert(SP_EL0 *sizeof(word_t) == (31 * 8), "sp_offset_correct");
_Static_assert(ELR_EL1 *sizeof(word_t) == (32 * 8), "lr_svc_offset_correct");
_Static_assert(FaultIP *sizeof(word_t) == (34 * 8), "faultinstruction_offset_correct");

typedef word_t register_t;

enum messageSizes {
    n_msgRegisters = 4,
    n_frameRegisters = 17,
    n_gpRegisters = 19,
    n_exceptionMessage = 3,
    n_syscallMessage = 12,



};
# 236 "/home/b/test/sel4test/kernel/include/arch/arm/arch/64/mode/machine/registerset.h"
extern const register_t msgRegisters[];
extern const register_t frameRegisters[];
extern const register_t gpRegisters[];


typedef struct debug_register_pair {
    word_t cr, vr;
} debug_register_pair_t;

typedef struct user_breakpoint_state {
    /* We don't use context comparisons */
    debug_register_pair_t breakpoint[6],
                          watchpoint[4];
    uint32_t used_breakpoints_bf;
    word_t n_instructions;
    bool_t single_step_enabled;
} user_breakpoint_state_t;



typedef struct user_fpu_state {
    uint64_t vregs[64];
    uint32_t fpsr;
    uint32_t fpcr;
} user_fpu_state_t;


/*
 * The "word_t registers" member of this struct must come first, because in
 * head.S, we assume that an "ldr %0, =ksCurThread" will point to the beginning
 * of the current thread's registers. The assert below should help.
 */
struct user_context {
    word_t registers[n_contextRegisters];

    word_t _padding;
    user_fpu_state_t fpuState;


    user_breakpoint_state_t breakpointState;

};
typedef struct user_context user_context_t;

_Static_assert(__builtin_offsetof(user_context_t, registers) == 0, "registers_are_first_member_of_user_context");



/* Aligning saves one cycle per LDP/STP instruction: */
_Static_assert(__builtin_offsetof(user_context_t, fpuState) % 16 == 0, "vregs_are_16_bytes_aligned");




void Arch_initBreakpointContext(user_context_t *context);


static inline void Arch_initContext(user_context_t *context)
{
    context->registers[SPSR_EL1] = ((1 << 6) | 0 | 0);

    Arch_initBreakpointContext(context);

}
# 10 "/home/b/test/sel4test/kernel/include/arch/arm/arch/machine/registerset.h" 2
# 18 "/home/b/test/sel4test/kernel/include/arch/arm/arch/64/mode/object/structures.h" 2

typedef struct arch_tcb {
    user_context_t tcbContext;



} arch_tcb_t;

enum vm_rights {
    VMKernelOnly = 0,
    VMReadWrite = 1,
    VMReadOnly = 3
};
typedef word_t vm_rights_t;
# 44 "/home/b/test/sel4test/kernel/include/arch/arm/arch/64/mode/object/structures.h"
/* Otherwise we use a 4-level translation */
typedef pte_t vspace_root_t;
# 57 "/home/b/test/sel4test/kernel/include/arch/arm/arch/64/mode/object/structures.h"
/* Generate a vcpu_t pointer from a vcpu block reference */



struct asid_pool {
    asid_map_t array[(1ul << (asidLowBits))];
};
typedef struct asid_pool asid_pool_t;

/* Generic fastpath.c code expects pde_t for stored_hw_asid
 * that's a workaround in the time being.
 */
typedef pte_t pde_t;
# 84 "/home/b/test/sel4test/kernel/include/arch/arm/arch/64/mode/object/structures.h"
static inline word_t __attribute__((__const__)) cap_get_archCapSizeBits(cap_t cap)
{
    cap_tag_t ctag;

    ctag = cap_get_capType(cap);

    switch (ctag) {
    case cap_frame_cap:
        return pageBitsForSize(cap_frame_cap_get_capFSize(cap));

    case cap_page_table_cap:
        return 12;

    case cap_vspace_cap:
        return 12;

    case cap_asid_pool_cap:
        return 12;

    case cap_asid_control_cap:
        return 0;






    case cap_sgi_signal_cap:
        return 0;


    default:
        /* Unreachable, but GCC can't figure that out */
        return 0;
    }
}

static inline bool_t __attribute__((__const__)) cap_get_archCapIsPhysical(cap_t cap)
{
    cap_tag_t ctag;

    ctag = cap_get_capType(cap);

    switch (ctag) {

    case cap_frame_cap:
        return true;

    case cap_page_table_cap:
        return true;

    case cap_vspace_cap:
        return true;

    case cap_asid_pool_cap:
        return true;

    case cap_asid_control_cap:
        return false;






    case cap_sgi_signal_cap:
        return false;


    default:
        /* Unreachable, but GCC can't figure that out */
        return false;
    }
}

static inline void *__attribute__((__const__)) cap_get_archCapPtr(cap_t cap)
{
    cap_tag_t ctag;

    ctag = cap_get_capType(cap);

    switch (ctag) {
    case cap_frame_cap:
        return (void *)(cap_frame_cap_get_capFBasePtr(cap));

    case cap_page_table_cap:
        return ((pte_t *)(cap_page_table_cap_get_capPTBasePtr(cap)));

    case cap_vspace_cap:
        return ((vspace_root_t *)(cap_vspace_cap_get_capVSBasePtr(cap)));

    case cap_asid_control_cap:
        return ((void *)0);

    case cap_asid_pool_cap:
        return ((asid_pool_t*)cap_asid_pool_cap_get_capASIDPool(cap));






    case cap_sgi_signal_cap:
        return ((void *)0);


    default:
        /* Unreachable, but GCC can't figure that out */
        return ((void *)0);
    }
}

static inline bool_t pte_pte_page_ptr_get_present(pte_t *pt)
{
    return (pte_ptr_get_pte_type(pt) == pte_pte_page);
}

static inline bool_t pte_pte_table_ptr_get_present(pte_t *pt)
{
    return (pte_ptr_get_pte_type(pt) == pte_pte_table);
}

static inline bool_t pte_4k_page_ptr_get_present(pte_t *pt)
{
    return (pte_ptr_get_pte_type(pt) == pte_pte_4k_page);
}

static inline bool_t pte_ptr_get_valid(pte_t *pt)
{
    return (pte_ptr_get_pte_type(pt) != pte_pte_invalid);
}

static inline bool_t pte_is_page_type(pte_t pte)
{
    return pte_get_pte_type(pte) == pte_pte_4k_page ||
           pte_get_pte_type(pte) == pte_pte_page;
}

/** Return base address for both of pte_4k_page and pte_page */
static inline uint64_t pte_get_page_base_address(pte_t pte)
{
    do { if (!(pte_is_page_type(pte))) { _assert_fail("pte_is_page_type(pte)", "/home/b/test/sel4test/kernel/include/arch/arm/arch/64/mode/object/structures.h", 225, __func__); } } while(0);
    return pte.words[0] & 0xfffffffff000ull;
}

/** Return base address for both of pte_4k_page and pte_page */
static inline uint64_t pte_page_ptr_get_page_base_address(pte_t *pt)
{
    return pte_get_page_base_address(*pt);
}
# 10 "/home/b/test/sel4test/kernel/include/arch/arm/arch/object/structures.h" 2



static inline bool_t __attribute__((__const__)) Arch_isCapRevocable(cap_t derivedCap, cap_t srcCap)
{
    switch (cap_get_capType(derivedCap)) {


    case cap_sgi_signal_cap:
        return cap_get_capType(srcCap) == cap_irq_control_cap;
# 28 "/home/b/test/sel4test/kernel/include/arch/arm/arch/object/structures.h"
    default:
        return false;
    }
}
# 211 "/home/b/test/sel4test/kernel/include/object/structures.h" 2

struct user_data {
    word_t words[(1ul << (12)) / sizeof(word_t)];
};
typedef struct user_data user_data_t;

struct user_data_device {
    word_t words[(1ul << (12)) / sizeof(word_t)];
};
typedef struct user_data_device user_data_device_t;

static inline word_t __attribute__((__const__)) wordFromVMRights(vm_rights_t vm_rights)
{
    return (word_t)vm_rights;
}

static inline vm_rights_t __attribute__((__const__)) vmRightsFromWord(word_t w)
{
    return (vm_rights_t)w;
}

static inline vm_attributes_t __attribute__((__const__)) vmAttributesFromWord(word_t w)
{
    vm_attributes_t attr;

    attr.words[0] = w;
    return attr;
}






struct tcb {
    /* arch specific tcb state (including context)*/
    arch_tcb_t tcbArch;

    /* Thread state, 3 words */
    thread_state_t tcbState;

    /* Currently only used for seL4_TCBFlag_fpuDisabled */
    word_t tcbFlags; /* seL4_TCBFlag */

    /* Notification that this TCB is bound to. If this is set, when this TCB waits on
     * any sync endpoint, it may receive a signal from a Notification object.
     * 1 word*/
    notification_t *tcbBoundNotification;

    /* Current fault, 2 words */
    seL4_Fault_t tcbFault;

    /* Current lookup failure, 2 words */
    lookup_fault_t tcbLookupFailure;

    /* Domain, 1 byte (padded to 1 word) */
    dom_t tcbDomain;

    /*  maximum controlled priority, 1 byte (padded to 1 word) */
    prio_t tcbMCP;

    /* Priority, 1 byte (padded to 1 word) */
    prio_t tcbPriority;
# 283 "/home/b/test/sel4test/kernel/include/object/structures.h"
    /* Timeslice remaining, 1 word */
    word_t tcbTimeSlice;

    /* Capability pointer to thread fault handler, 1 word */
    cptr_t tcbFaultHandler;


    /* userland virtual address of thread IPC buffer, 1 word */
    word_t tcbIPCBuffer;






    /* Previous and next pointers for scheduler queues , 2 words */
    struct tcb *tcbSchedNext;
    struct tcb *tcbSchedPrev;
    /* Previous and next pointers for endpoint and notification queues, 2 words */
    struct tcb *tcbEPNext;
    struct tcb *tcbEPPrev;





};
typedef struct tcb tcb_t;


/* This debug_tcb object is inserted into the 'unused' region of a TCB object
   for debug build configurations. */
struct debug_tcb {

    /* Pointers for list of all tcbs that is maintained
     * when CONFIG_DEBUG_BUILD is enabled, 2 words */
    struct tcb *tcbDebugNext;
    struct tcb *tcbDebugPrev;
    /* Use any remaining space for a thread name */
    char tcbName[];

};
typedef struct debug_tcb debug_tcb_t;
# 403 "/home/b/test/sel4test/kernel/include/object/structures.h"
/* Ensure object sizes are sane */
_Static_assert(sizeof(cte_t) == (1ul << (5)), "cte_size_sane");
_Static_assert((4 + 5) <= (12 - 1), "tcb_cte_size_sane");
_Static_assert((1ul << ((12 - 1))) >= sizeof(tcb_t), "tcb_size_sane");

_Static_assert((1ul << ((12 - 1) - 1)) < sizeof(tcb_t), "tcb_size_not_excessive");

_Static_assert(sizeof(endpoint_t) == (1ul << (4)), "ep_size_sane");
_Static_assert(sizeof(notification_t) == (1ul << (5)), "notification_size_sane");

/* Check the IPC buffer is the right size */
_Static_assert(sizeof(seL4_IPCBuffer) == (1ul << (10)), "ipc_buf_size_sane");







/* helper functions */

static inline word_t __attribute__((__const__))
isArchCap(cap_t cap)
{
    return (cap_get_capType(cap) % 2);
}
# 12 "/home/b/test/sel4test/kernel/include/compound_types.h" 2


struct pde_range {
    pde_t *base;
    word_t length;
};
typedef struct pde_range pde_range_t;

struct pte_range {
    pte_t *base;
    word_t length;
};
typedef struct pte_range pte_range_t;

typedef cte_t *cte_ptr_t;

struct extra_caps {
    cte_ptr_t excaprefs[((1ul<<(seL4_MsgExtraCapBits))-1)];
};
typedef struct extra_caps extra_caps_t;
# 11 "/home/b/test/sel4test/kernel/include/types.h" 2
# 9 "/home/b/test/sel4test/kernel/src/api/faults.c" 2
# 1 "/home/b/test/sel4test/kernel/include/api/faults.h" 1
/*
 * Copyright 2020, Data61, CSIRO (ABN 41 687 119 230)
 *
 * SPDX-License-Identifier: GPL-2.0-only
 */

       




word_t setMRs_fault(tcb_t *sender, tcb_t *receiver, word_t *receiveIPCBuffer);
word_t Arch_setMRs_fault(tcb_t *sender, tcb_t *receiver, word_t *receiveIPCBuffer, word_t faultType);

bool_t handleFaultReply(tcb_t *receiver, tcb_t *sender);
bool_t Arch_handleFaultReply(tcb_t *receiver, tcb_t *sender, word_t faultType);
# 10 "/home/b/test/sel4test/kernel/src/api/faults.c" 2
# 1 "/home/b/test/sel4test/kernel/include/api/syscall.h" 1
/*
 * Copyright 2014, General Dynamics C4 Systems
 *
 * SPDX-License-Identifier: GPL-2.0-only
 */

       


# 1 "/home/b/test/sel4test/kernel/include/machine.h" 1
/*
 * Copyright 2014, General Dynamics C4 Systems
 *
 * SPDX-License-Identifier: GPL-2.0-only
 */

       

# 1 "/home/b/test/sel4test/kernel/include/plat/default/plat/machine.h" 1
/*
 * Copyright 2020, Data61, CSIRO (ABN 41 687 119 230)
 *
 * SPDX-License-Identifier: GPL-2.0-only
 */

# 1 "/home/b/test/sel4test/cbuild/kernel/gen_headers/plat/machine/devices_gen.h" 1
/*
 * Copyright 2020, Data61, CSIRO (ABN 41 687 119 230)
 *
 * SPDX-License-Identifier: GPL-2.0-only
 */

/*
 * This file is autogenerated by <kernel>/tools/hardware/outputs/c_header.py.
 */

       






# 1 "/home/b/test/sel4test/kernel/include/arch/arm/arch/64/mode/hardware.h" 1
/*
 * Copyright 2020, Data61, CSIRO (ABN 41 687 119 230)
 *
 * SPDX-License-Identifier: GPL-2.0-only
 */

       






/* EL2 kernel address map:
 *
 * The EL2 mode kernel uses TTBR0_EL2 which covers the range of
 * 0x0 - 0x0000ffffffffffff (48 bits of vaddrspace).
 *
 * The CPU on the TX1 only allows for 48-bits of addressable virtual memory.
 * Within the seL4 EL2 kernel's separate vaddrspace, it uses
 * the 512 GiB at the top of that 48 bits of addressable
 * virtual memory.
 *
 * In EL2 there is no canonical-high portion of the address space since
 * address tagging is not supported in EL2. Therefore the kernel is linked
 * to the canonical lower portion of the address space (all the unused top bits
 * are set to 0, not 1).
 *
 * The memory map used by the EL2 kernel's separate address space
 * ends up looking something like this:
 *
 * +-----------------------------------+ <- 0xFFFFFFFF_FFFFFFFF
 * | Canonical high portion - unusable |
 * | virtual addrs                     |
 * +-----------------------------------+ <- PPTR_TOP: 256TiB mark (top of 48 bits)
 * | seL4 EL2 kernel                   |    ^
 * |                                   |    |
 * |                                   |    512GiB
 * |                                   |    of EL2 kernel windowing
 * |                                   |    into memory.
 * |                                   |    |
 * |                                   |    v
 * +-----------------------------------+ <- PPTR_BASE: 256TiB minus 512GiB.
 * | Unused virtual addresses within   |    ^
 * | the EL2 kernel's                  |    |
 * | separate vaddrspace.              |    Rest of the
 * |                                   |    EL2 kernel
 * |                                   |    vaddrspace, unused,
 * |                                   |    which is the rest of
 * |                                   |    the lower 256 TiB.
 * |                                   |    |
 * |                                   |    v
 * +-----------------------------------+ <- 0x0
 *
 * !defined(CONFIG_ARM_HYPERVISOR_SUPPORT)
 *
 *          2^64 +-------------------+
 *               | Kernel Page PDPT  | --+
 *   2^64 - 2^39 +-------------------+ PPTR_BASE
 *               |    TLB Bitmaps    |   |
 *               +-------------------+   |
 *               |                   |   |
 *               |     Unmapped      |   |
 *               |                   |   |
 *   2^64 - 2^48 +-------------------+   |
 *               |                   |   |
 *               |   Unaddressable   |   |
 *               |                   |   |
 *          2^48 +-------------------+ USER_TOP
 *               |                   |   |
 *               |       User        |   |
 *               |                   |   |
 *           0x0 +-------------------+   |
 *                                       |
 *                         +-------------+
 *                         |
 *                         v
 *          2^64 +-------------------+
 *               |                   |
 *               |                   |     +------+
 *               | Kernel Page Table | --> |  PD  | ----+
 *               |                   |     +------+     |
 *               |                   |                  |
 *   2^64 - 2^30 +-------------------+ PPTR_TOP         |
 *               |                   |                  |
 *               |  Physical Memory  |                  |
 *               |       Window      |                  |
 *               |                   |                  |
 *               +-------------------+                  |
 *               |                   |                  |
 *               |                   |     +------+     |
 *               |    Kernel ELF     | --> |  PD  |     |
 *               |                   |     +------+     |
 *               |                   |                  |
 *               +-------------------+ KERNEL_ELF_BASE  |
 *               |                   |                  |
 *               |  Physical Memory  |                  |
 *               |       Window      |                  |
 *               |                   |                  |
 *   2^64 - 2^39 +-------------------+ PPTR_BASE        |
 *                                                      |
 *                                +---------------------+
 *                                |
 *                                v
 *   2^64 - 2^21 + 2^12 +-------------------+
 *                      |                   |
 *                      |  Kernel Devices   |
 *                      |                   |
 *          2^64 - 2^21 +-------------------+ KDEV_BASE
 *
 *
 * defined(CONFIG_ARM_HYPERVISOR_SUPPORT)
 *
 *          2^64 +-------------------+
 *               |                   |
 *               |   Unaddressable   |
 *               |                   |
 *          2^48 +-------------------+
 *               | Kernel Page PDPT  | --+
 *   2^48 - 2^39 +-------------------+ PPTR_BASE
 *               |    TLB Bitmaps    |   |
 *               +-------------------+   |
 *               |                   |   |
 *               |     Unmapped      |   |
 *               |                   |   |
 *           0x0 +-------------------+   |
 *                                       |
 *                         +-------------+
 *                         |
 *                         v
 *          2^48 +-------------------+
 *               |                   |
 *               |                   |     +------+
 *               | Kernel Page Table | --> |  PD  | ----+
 *               |                   |     +------+     |
 *               |                   |                  |
 *   2^48 - 2^30 +-------------------+ PPTR_TOP         |
 *               |                   |                  |
 *               |  Physical Memory  |                  |
 *               |       Window      |                  |
 *               |                   |                  |
 *               +-------------------+                  |
 *               |                   |                  |
 *               |                   |     +------+     |
 *               |    Kernel ELF     | --> |  PD  |     |
 *               |                   |     +------+     |
 *               |                   |                  |
 *               +-------------------+ KERNEL_ELF_BASE  |
 *               |                   |                  |
 *               |  Physical Memory  |                  |
 *               |       Window      |                  |
 *               |                   |                  |
 *   2^48 - 2^39 +-------------------+ PPTR_BASE        |
 *                                                      |
 *                                +---------------------+
 *                                |
 *                                v
 *   2^48 - 2^21 + 2^12 +-------------------+
 *                      |                   |
 *                      |  Kernel Devices   |
 *                      |                   |
 *          2^48 - 2^21 +-------------------+ KDEV_BASE
 *
 */

/* last accessible virtual address in user space */


/* The first physical address to map into the kernel's physical memory
 * window */


/* The base address in virtual memory to use for the 1:1 physical memory
 * mapping */






/* Top of the physical memory window */






/* The physical memory address to use for mapping the kernel ELF */

/* For use by the linker (only integer constants allowed) */


/* The base address in virtual memory to use for the kernel ELF mapping */

/* For use by the linker (only integer constants allowed) */


/* This is a page table mapping at the end of the virtual address space
 * to map objects with 4KiB pages rather than 4MiB large pages. */






/* The base address in virtual memory to use for the kernel device
 * mapping region. These are mapped in the kernel page table. */


/* The log buffer is placed before the device region */



/* All PPTR addresses must be canonical to be able to be stored in caps or objects.
   Check that all UTs that are created will have valid address in the PPTR space.
   For non-hyp, PPTR_BASE is in the top part of the address space and device untyped
   addresses are allowed to be large enough to overflow and be in the bottom half of
   the address space.  However, when the kernel is in EL2 it is not possible to safely
   overflow without going into address ranges that are non-canonical.  These static
   asserts check that the kernel config won't lead to UTs being created that aren't
   representable. */
_Static_assert(17592186044416 <= (1ul << (47)), "ut_max_less_than_canonical");;
# 19 "/home/b/test/sel4test/cbuild/kernel/gen_headers/plat/machine/devices_gen.h" 2
# 1 "/home/b/test/sel4test/kernel/include/linker.h" 1
/*
 * Copyright 2014, General Dynamics C4 Systems
 *
 * SPDX-License-Identifier: GPL-2.0-only
 */

       


# 1 "/home/b/test/sel4test/kernel/include/arch/arm/arch/linker.h" 1
/*
 * Copyright 2020, Data61, CSIRO (ABN 41 687 119 230)
 *
 * SPDX-License-Identifier: BSD-2-Clause
 */

       
/* Place-holder for ARM-related linker definitions */
# 11 "/home/b/test/sel4test/kernel/include/linker.h" 2

/* code that is only used during kernel bootstrapping */


/* read-only data only used during kernel bootstrapping */


/* read/write data only used during kernel bootstrapping */


/* node-local bss data that is only used during kernel bootstrapping */


/* data will be aligned to n bytes in a special BSS section */


/* data that will be mapped into and permitted to be used in the restricted SKIM
 * address space */


/* bss data that is permitted to be used in the restricted SKIM address space */
# 20 "/home/b/test/sel4test/cbuild/kernel/gen_headers/plat/machine/devices_gen.h" 2


/* Wrap raw physBase location constant to give it a symbolic name in C that's
 * visible to verification. This is necessary as there are no real constants
 * in C except enums, and enums constants must fit in an int.
 */
static inline __attribute__((__const__)) word_t physBase(void)
{
    return 0xc0000000;
}

/* INTERRUPTS */
/* INTERRUPT_VGIC_MAINTENANCE generated from /interrupt-controller@30800000 */

/* INTERRUPT_VTIMER_EVENT generated from /timer */

/* KERNEL_TIMER_IRQ generated from /timer */





/* KERNEL DEVICES */




static const kernel_frame_t __attribute__((__section__(".boot.rodata"))) kernel_device_frames[] = {

    /* /soc/uart@2800d000 */
    {
        .paddr = 0x2800d000,
        .pptr = (0xffffffffffe00000ul + 0x0),
        .armExecuteNever = true,
        .userAvailable = true
    },

    /* /interrupt-controller@30800000 */
    {
        .paddr = 0x30800000,
        .pptr = (0xffffffffffe00000ul + 0x1000),
        .armExecuteNever = true,
        .userAvailable = false
    },
    {
        .paddr = 0x30801000,
        /* contains GICD_PPTR */
        .pptr = 0xffffffffffe00000ul + 0x2000,
        .armExecuteNever = true,
        .userAvailable = false
    },
    {
        .paddr = 0x30802000,
        /* contains GICD_PPTR */
        .pptr = 0xffffffffffe00000ul + 0x3000,
        .armExecuteNever = true,
        .userAvailable = false
    },
    {
        .paddr = 0x30803000,
        /* contains GICD_PPTR */
        .pptr = 0xffffffffffe00000ul + 0x4000,
        .armExecuteNever = true,
        .userAvailable = false
    },
    {
        .paddr = 0x30804000,
        /* contains GICD_PPTR */
        .pptr = 0xffffffffffe00000ul + 0x5000,
        .armExecuteNever = true,
        .userAvailable = false
    },
    {
        .paddr = 0x30805000,
        /* contains GICD_PPTR */
        .pptr = 0xffffffffffe00000ul + 0x6000,
        .armExecuteNever = true,
        .userAvailable = false
    },
    {
        .paddr = 0x30806000,
        /* contains GICD_PPTR */
        .pptr = 0xffffffffffe00000ul + 0x7000,
        .armExecuteNever = true,
        .userAvailable = false
    },
    {
        .paddr = 0x30807000,
        /* contains GICD_PPTR */
        .pptr = 0xffffffffffe00000ul + 0x8000,
        .armExecuteNever = true,
        .userAvailable = false
    },
    {
        .paddr = 0x30808000,
        /* contains GICD_PPTR */
        .pptr = 0xffffffffffe00000ul + 0x9000,
        .armExecuteNever = true,
        .userAvailable = false
    },
    {
        .paddr = 0x30809000,
        /* contains GICD_PPTR */
        .pptr = 0xffffffffffe00000ul + 0xa000,
        .armExecuteNever = true,
        .userAvailable = false
    },
    {
        .paddr = 0x3080a000,
        /* contains GICD_PPTR */
        .pptr = 0xffffffffffe00000ul + 0xb000,
        .armExecuteNever = true,
        .userAvailable = false
    },
    {
        .paddr = 0x3080b000,
        /* contains GICD_PPTR */
        .pptr = 0xffffffffffe00000ul + 0xc000,
        .armExecuteNever = true,
        .userAvailable = false
    },
    {
        .paddr = 0x3080c000,
        /* contains GICD_PPTR */
        .pptr = 0xffffffffffe00000ul + 0xd000,
        .armExecuteNever = true,
        .userAvailable = false
    },
    {
        .paddr = 0x3080d000,
        /* contains GICD_PPTR */
        .pptr = 0xffffffffffe00000ul + 0xe000,
        .armExecuteNever = true,
        .userAvailable = false
    },
    {
        .paddr = 0x3080e000,
        /* contains GICD_PPTR */
        .pptr = 0xffffffffffe00000ul + 0xf000,
        .armExecuteNever = true,
        .userAvailable = false
    },
    {
        .paddr = 0x3080f000,
        /* contains GICD_PPTR */
        .pptr = 0xffffffffffe00000ul + 0x10000,
        .armExecuteNever = true,
        .userAvailable = false
    },
    /* /interrupt-controller@30800000 */
    {
        .paddr = 0x30880000,
        .pptr = (0xffffffffffe00000ul + 0x11000),
        .armExecuteNever = true,
        .userAvailable = false
    },
    {
        .paddr = 0x30881000,
        /* contains GICR_PPTR */
        .pptr = 0xffffffffffe00000ul + 0x12000,
        .armExecuteNever = true,
        .userAvailable = false
    },
    {
        .paddr = 0x30882000,
        /* contains GICR_PPTR */
        .pptr = 0xffffffffffe00000ul + 0x13000,
        .armExecuteNever = true,
        .userAvailable = false
    },
    {
        .paddr = 0x30883000,
        /* contains GICR_PPTR */
        .pptr = 0xffffffffffe00000ul + 0x14000,
        .armExecuteNever = true,
        .userAvailable = false
    },
    {
        .paddr = 0x30884000,
        /* contains GICR_PPTR */
        .pptr = 0xffffffffffe00000ul + 0x15000,
        .armExecuteNever = true,
        .userAvailable = false
    },
    {
        .paddr = 0x30885000,
        /* contains GICR_PPTR */
        .pptr = 0xffffffffffe00000ul + 0x16000,
        .armExecuteNever = true,
        .userAvailable = false
    },
    {
        .paddr = 0x30886000,
        /* contains GICR_PPTR */
        .pptr = 0xffffffffffe00000ul + 0x17000,
        .armExecuteNever = true,
        .userAvailable = false
    },
    {
        .paddr = 0x30887000,
        /* contains GICR_PPTR */
        .pptr = 0xffffffffffe00000ul + 0x18000,
        .armExecuteNever = true,
        .userAvailable = false
    },
    {
        .paddr = 0x30888000,
        /* contains GICR_PPTR */
        .pptr = 0xffffffffffe00000ul + 0x19000,
        .armExecuteNever = true,
        .userAvailable = false
    },
    {
        .paddr = 0x30889000,
        /* contains GICR_PPTR */
        .pptr = 0xffffffffffe00000ul + 0x1a000,
        .armExecuteNever = true,
        .userAvailable = false
    },
    {
        .paddr = 0x3088a000,
        /* contains GICR_PPTR */
        .pptr = 0xffffffffffe00000ul + 0x1b000,
        .armExecuteNever = true,
        .userAvailable = false
    },
    {
        .paddr = 0x3088b000,
        /* contains GICR_PPTR */
        .pptr = 0xffffffffffe00000ul + 0x1c000,
        .armExecuteNever = true,
        .userAvailable = false
    },
    {
        .paddr = 0x3088c000,
        /* contains GICR_PPTR */
        .pptr = 0xffffffffffe00000ul + 0x1d000,
        .armExecuteNever = true,
        .userAvailable = false
    },
    {
        .paddr = 0x3088d000,
        /* contains GICR_PPTR */
        .pptr = 0xffffffffffe00000ul + 0x1e000,
        .armExecuteNever = true,
        .userAvailable = false
    },
    {
        .paddr = 0x3088e000,
        /* contains GICR_PPTR */
        .pptr = 0xffffffffffe00000ul + 0x1f000,
        .armExecuteNever = true,
        .userAvailable = false
    },
    {
        .paddr = 0x3088f000,
        /* contains GICR_PPTR */
        .pptr = 0xffffffffffe00000ul + 0x20000,
        .armExecuteNever = true,
        .userAvailable = false
    },
    {
        .paddr = 0x30890000,
        /* contains GICR_PPTR */
        .pptr = 0xffffffffffe00000ul + 0x21000,
        .armExecuteNever = true,
        .userAvailable = false
    },
    {
        .paddr = 0x30891000,
        /* contains GICR_PPTR */
        .pptr = 0xffffffffffe00000ul + 0x22000,
        .armExecuteNever = true,
        .userAvailable = false
    },
    {
        .paddr = 0x30892000,
        /* contains GICR_PPTR */
        .pptr = 0xffffffffffe00000ul + 0x23000,
        .armExecuteNever = true,
        .userAvailable = false
    },
    {
        .paddr = 0x30893000,
        /* contains GICR_PPTR */
        .pptr = 0xffffffffffe00000ul + 0x24000,
        .armExecuteNever = true,
        .userAvailable = false
    },
    {
        .paddr = 0x30894000,
        /* contains GICR_PPTR */
        .pptr = 0xffffffffffe00000ul + 0x25000,
        .armExecuteNever = true,
        .userAvailable = false
    },
    {
        .paddr = 0x30895000,
        /* contains GICR_PPTR */
        .pptr = 0xffffffffffe00000ul + 0x26000,
        .armExecuteNever = true,
        .userAvailable = false
    },
    {
        .paddr = 0x30896000,
        /* contains GICR_PPTR */
        .pptr = 0xffffffffffe00000ul + 0x27000,
        .armExecuteNever = true,
        .userAvailable = false
    },
    {
        .paddr = 0x30897000,
        /* contains GICR_PPTR */
        .pptr = 0xffffffffffe00000ul + 0x28000,
        .armExecuteNever = true,
        .userAvailable = false
    },
    {
        .paddr = 0x30898000,
        /* contains GICR_PPTR */
        .pptr = 0xffffffffffe00000ul + 0x29000,
        .armExecuteNever = true,
        .userAvailable = false
    },
    {
        .paddr = 0x30899000,
        /* contains GICR_PPTR */
        .pptr = 0xffffffffffe00000ul + 0x2a000,
        .armExecuteNever = true,
        .userAvailable = false
    },
    {
        .paddr = 0x3089a000,
        /* contains GICR_PPTR */
        .pptr = 0xffffffffffe00000ul + 0x2b000,
        .armExecuteNever = true,
        .userAvailable = false
    },
    {
        .paddr = 0x3089b000,
        /* contains GICR_PPTR */
        .pptr = 0xffffffffffe00000ul + 0x2c000,
        .armExecuteNever = true,
        .userAvailable = false
    },
    {
        .paddr = 0x3089c000,
        /* contains GICR_PPTR */
        .pptr = 0xffffffffffe00000ul + 0x2d000,
        .armExecuteNever = true,
        .userAvailable = false
    },
    {
        .paddr = 0x3089d000,
        /* contains GICR_PPTR */
        .pptr = 0xffffffffffe00000ul + 0x2e000,
        .armExecuteNever = true,
        .userAvailable = false
    },
    {
        .paddr = 0x3089e000,
        /* contains GICR_PPTR */
        .pptr = 0xffffffffffe00000ul + 0x2f000,
        .armExecuteNever = true,
        .userAvailable = false
    },
    {
        .paddr = 0x3089f000,
        /* contains GICR_PPTR */
        .pptr = 0xffffffffffe00000ul + 0x30000,
        .armExecuteNever = true,
        .userAvailable = false
    },
};

/* Elements in kernel_device_frames may be enabled in specific configurations
 * only, but the ARRAY_SIZE() macro will automatically take care of this.
 * However, one corner case remains unsolved where all elements are disabled
 * and this becomes an empty array effectively. Then the C parser used in the
 * formal verification process will fail, because it follows the strict C rules
 * which do not allow empty arrays. Luckily, we have not met this case yet...
 */


/* PHYSICAL MEMORY */
static const p_region_t __attribute__((__section__(".boot.rodata"))) avail_p_regs[] = {
    /* /memory@b0000000 */
    {
        .start = 0xc0000000,
        .end = 0xde000000
    },
    /* /memory@b0000000 */
    {
        .start = 0xde420000,
        .end = 0xf0000000
    },
};
# 8 "/home/b/test/sel4test/kernel/include/plat/default/plat/machine.h" 2
# 1 "/home/b/test/sel4test/cbuild/kernel/gen_headers/plat/platform_gen.h" 1
/*
 * Copyright 2020, Data61, CSIRO (ABN 41 687 119 230)
 *
 * SPDX-License-Identifier: GPL-2.0-only
 */

       
# 17 "/home/b/test/sel4test/cbuild/kernel/gen_headers/plat/platform_gen.h"
enum IRQConstants {
    maxIRQ = 256
};



# 1 "/home/b/test/sel4test/kernel/include/arch/arm/arch/machine/gic_v3.h" 1
/*
 * Copyright 2019, DornerWorks
 * Copyright 2019, Data61, CSIRO (ABN 41 687 119 230)
 *
 * SPDX-License-Identifier: GPL-2.0-only
 */

/*
 * Arm Generic Interrupt Controller v3
 */

       

/* tell the kernel we have the set trigger feature */





# 1 "/home/b/test/sel4test/kernel/include/arch/arm/arch/64/mode/smp/smp.h" 1
/*
 * Copyright 2020, Data61, CSIRO (ABN 41 687 119 230)
 *
 * SPDX-License-Identifier: GPL-2.0-only
 */

       



# 1 "/home/b/test/sel4test/kernel/include/kernel/stack.h" 1
/*
 * Copyright 2020, Data61, CSIRO (ABN 41 687 119 230)
 *
 * SPDX-License-Identifier: GPL-2.0-only
 */

       



# 1 "/home/b/test/sel4test/kernel/include/arch/arm/arch/64/mode/kernel/stack.h" 1
/*
 * Copyright 2020, Data61, CSIRO (ABN 41 687 119 230)
 *
 * SPDX-License-Identifier: GPL-2.0-only
 */

       
# 12 "/home/b/test/sel4test/kernel/include/kernel/stack.h" 2

/* These are the stacks used in kernel, shared between architectures/modes.
 * CONFIG_KERNEL_STACK_BITS is defined in kernel/Kconfig. The physical/offset
 * address of the stack is per-arch-mode aligned. KERNEL_STACK_ALIGNMENT is
 * defined for each arch/mode in <mode/kernel/stack.h>
 */
extern char kernel_stack_alloc[1][(1ul << (12))];
# 12 "/home/b/test/sel4test/kernel/include/arch/arm/arch/64/mode/smp/smp.h" 2
# 21 "/home/b/test/sel4test/kernel/include/arch/arm/arch/machine/gic_v3.h" 2
# 1 "/home/b/test/sel4test/kernel/include/model/statedata.h" 1
/*
 * Copyright 2020, Data61, CSIRO (ABN 41 687 119 230)
 *
 * SPDX-License-Identifier: GPL-2.0-only
 */

       




# 1 "/home/b/test/sel4test/kernel/include/object/tcb.h" 1
/*
 * Copyright 2014, General Dynamics C4 Systems
 *
 * SPDX-License-Identifier: GPL-2.0-only
 */

       


# 1 "/home/b/test/sel4test/kernel/include/api/failures.h" 1
/*
 * Copyright 2014, General Dynamics C4 Systems
 *
 * SPDX-License-Identifier: GPL-2.0-only
 */

       


# 1 "/home/b/test/sel4test/kernel/libsel4/include/sel4/errors.h" 1
/*
 * Copyright 2020, Data61, CSIRO (ABN 41 687 119 230)
 *
 * SPDX-License-Identifier: BSD-2-Clause
 */

       

typedef enum {
    seL4_NoError = 0,
    seL4_InvalidArgument,
    seL4_InvalidCapability,
    seL4_IllegalOperation,
    seL4_RangeError,
    seL4_AlignmentError,
    seL4_FailedLookup,
    seL4_TruncatedMessage,
    seL4_DeleteFirst,
    seL4_RevokeFirst,
    seL4_NotEnoughMemory,

    /* This should always be the last item in the list
     * so it gives a count of the number of errors in the
     * enum.
     */
    seL4_NumErrors
} seL4_Error;
# 11 "/home/b/test/sel4test/kernel/include/api/failures.h" 2

/* These datatypes differ markedly from haskell, due to the
 * different implementation of the various fault monads */


enum exception {
    EXCEPTION_NONE,
    EXCEPTION_FAULT,
    EXCEPTION_LOOKUP_FAULT,
    EXCEPTION_SYSCALL_ERROR,
    EXCEPTION_PREEMPTED
};
typedef word_t exception_t;

typedef word_t syscall_error_type_t;

struct syscall_error {
    word_t invalidArgumentNumber;
    word_t invalidCapNumber;
    word_t rangeErrorMin;
    word_t rangeErrorMax;
    word_t memoryLeft;
    bool_t failedLookupWasSource;

    syscall_error_type_t type;
};
typedef struct syscall_error syscall_error_t;
# 47 "/home/b/test/sel4test/kernel/include/api/failures.h"
extern lookup_fault_t current_lookup_fault;
extern seL4_Fault_t current_fault;
extern syscall_error_t current_syscall_error;
# 11 "/home/b/test/sel4test/kernel/include/object/tcb.h" 2


# 1 "/home/b/test/sel4test/kernel/include/machine/registerset.h" 1
/*
 * Copyright 2014, General Dynamics C4 Systems
 *
 * SPDX-License-Identifier: GPL-2.0-only
 */

       






typedef enum {
    MessageID_Syscall,
    MessageID_Exception,



} MessageID_t;






extern const register_t fault_messages[][(((n_syscallMessage)>(n_exceptionMessage))?(n_syscallMessage):(n_exceptionMessage))] __attribute__((externally_visible));

static inline void setRegister(tcb_t *thread, register_t reg, word_t w)
{
    thread->tcbArch.tcbContext.registers[reg] = w;
}

static inline word_t __attribute__((__pure__)) getRegister(tcb_t *thread, register_t reg)
{
    return thread->tcbArch.tcbContext.registers[reg];
}
# 14 "/home/b/test/sel4test/kernel/include/object/tcb.h" 2
# 1 "/home/b/test/sel4test/kernel/include/object/cnode.h" 1
/*
 * Copyright 2014, General Dynamics C4 Systems
 *
 * SPDX-License-Identifier: GPL-2.0-only
 */

       





exception_t decodeCNodeInvocation(word_t invLabel, word_t length,
                                  cap_t cap, word_t *buffer);
exception_t invokeCNodeRevoke(cte_t *destSlot);
exception_t invokeCNodeDelete(cte_t *destSlot);
exception_t invokeCNodeCancelBadgedSends(cap_t cap);
exception_t invokeCNodeInsert(cap_t cap, cte_t *srcSlot, cte_t *destSlot);
exception_t invokeCNodeMove(cap_t cap, cte_t *srcSlot, cte_t *destSlot);
exception_t invokeCNodeRotate(cap_t cap1, cap_t cap2, cte_t *slot1,
                              cte_t *slot2, cte_t *slot3);
void cteInsert(cap_t newCap, cte_t *srcSlot, cte_t *destSlot);
void cteMove(cap_t newCap, cte_t *srcSlot, cte_t *destSlot);
void capSwapForDelete(cte_t *slot1, cte_t *slot2);
void cteSwap(cap_t cap1, cte_t *slot1, cap_t cap2, cte_t *slot2);
exception_t cteRevoke(cte_t *slot);
exception_t cteDelete(cte_t *slot, bool_t exposed);
void cteDeleteOne(cte_t *slot);
void insertNewCap(cte_t *parent, cte_t *slot, cap_t cap);
bool_t __attribute__((__pure__)) isMDBParentOf(cte_t *cte_a, cte_t *cte_b);
exception_t ensureNoChildren(cte_t *slot);
exception_t ensureEmptySlot(cte_t *slot);
bool_t __attribute__((__pure__)) isFinalCapability(cte_t *cte);
bool_t __attribute__((__pure__)) slotCapLongRunningDelete(cte_t *slot);
cte_t *getReceiveSlots(tcb_t *thread, word_t *buffer);
cap_transfer_t __attribute__((__pure__)) loadCapTransfer(word_t *buffer);


exception_t invokeCNodeSaveCaller(cte_t *destSlot);
void setupReplyMaster(tcb_t *thread);
# 15 "/home/b/test/sel4test/kernel/include/object/tcb.h" 2


/* Maximum length of the tcb name, including null terminator */

_Static_assert(((1ul << (12 -1)) - (tcbCNodeEntries * sizeof(cte_t)) - sizeof(debug_tcb_t)) > 0, "tcb_name_fits");


struct tcb_queue {
    tcb_t *head;
    tcb_t *end;
};
typedef struct tcb_queue tcb_queue_t;

static inline unsigned int setMR(tcb_t *receiver, word_t *receiveIPCBuffer,
                                 unsigned int offset, word_t reg)
{
    if (offset >= n_msgRegisters) {
        if (receiveIPCBuffer) {
            receiveIPCBuffer[offset + 1] = reg;
            return offset + 1;
        } else {
            return n_msgRegisters;
        }
    } else {
        setRegister(receiver, msgRegisters[offset], reg);
        return offset + 1;
    }
}

void tcbSchedEnqueue(tcb_t *tcb);
void tcbSchedAppend(tcb_t *tcb);
void tcbSchedDequeue(tcb_t *tcb);
tcb_queue_t tcb_queue_remove(tcb_queue_t queue, tcb_t *tcb);

static inline bool_t __attribute__((__pure__)) tcb_queue_empty(tcb_queue_t queue)
{
    return queue.head == ((void *)0);
}

static inline tcb_queue_t tcb_queue_prepend(tcb_queue_t queue, tcb_t *tcb)
{
    if (tcb_queue_empty(queue)) {
        queue.end = tcb;
    } else {
        tcb->tcbSchedNext = queue.head;
        queue.head->tcbSchedPrev = tcb;
    }

    queue.head = tcb;

    return queue;
}

static inline tcb_queue_t tcb_queue_append(tcb_queue_t queue, tcb_t *tcb)
{
    if (tcb_queue_empty(queue)) {
        queue.head = tcb;
    } else {
        tcb->tcbSchedPrev = queue.end;
        queue.end->tcbSchedNext = tcb;
    }

    queue.end = tcb;

    return queue;
}

/* Insert a TCB into a queue immediately before another item in the queue
   (the queue must initially contain at least two items) */
static inline void tcb_queue_insert(tcb_t *tcb, tcb_t *after)
{
    tcb_t *before;
    before = after->tcbSchedPrev;

    do { if (!(before != ((void *)0))) { _assert_fail("before != NULL", "/home/b/test/sel4test/kernel/include/object/tcb.h", 89, __func__); } } while(0);
    do { if (!(before != after)) { _assert_fail("before != after", "/home/b/test/sel4test/kernel/include/object/tcb.h", 90, __func__); } } while(0);

    tcb->tcbSchedPrev = before;
    tcb->tcbSchedNext = after;

    after->tcbSchedPrev = tcb;
    before->tcbSchedNext = tcb;
}


void tcbDebugAppend(tcb_t *tcb);
void tcbDebugRemove(tcb_t *tcb);
# 167 "/home/b/test/sel4test/kernel/include/object/tcb.h"
tcb_queue_t tcbEPAppend(tcb_t *tcb, tcb_queue_t queue);
tcb_queue_t tcbEPDequeue(tcb_t *tcb, tcb_queue_t queue);

void setupCallerCap(tcb_t *sender, tcb_t *receiver, bool_t canGrant);
void deleteCallerCap(tcb_t *receiver);


word_t copyMRs(tcb_t *sender, word_t *sendBuf, tcb_t *receiver,
               word_t *recvBuf, word_t n);
exception_t decodeTCBInvocation(word_t invLabel, word_t length, cap_t cap,
                                cte_t *slot, bool_t call, word_t *buffer);
exception_t decodeCopyRegisters(cap_t cap, word_t length, word_t *buffer);
exception_t decodeReadRegisters(cap_t cap, word_t length, bool_t call,
                                word_t *buffer);
exception_t decodeWriteRegisters(cap_t cap, word_t length, word_t *buffer);
exception_t decodeTCBConfigure(cap_t cap, word_t length,
                               cte_t *slot, word_t *buffer);
exception_t decodeSetPriority(cap_t cap, word_t length, word_t *buffer);
exception_t decodeSetMCPriority(cap_t cap, word_t length, word_t *buffer);



exception_t decodeSetSchedParams(cap_t cap, word_t length, word_t *buffer);

exception_t decodeSetIPCBuffer(cap_t cap, word_t length,
                               cte_t *slot, word_t *buffer);
exception_t decodeSetSpace(cap_t cap, word_t length,
                           cte_t *slot, word_t *buffer);
exception_t decodeDomainInvocation(word_t invLabel, word_t length, word_t *buffer);
exception_t decodeBindNotification(cap_t cap);
exception_t decodeUnbindNotification(cap_t cap);
# 218 "/home/b/test/sel4test/kernel/include/object/tcb.h"
enum thread_control_flag {
    thread_control_update_priority = 0x1,
    thread_control_update_ipc_buffer = 0x2,
    thread_control_update_space = 0x4,
    thread_control_update_mcp = 0x8,
};


typedef word_t thread_control_flag_t;

exception_t invokeTCB_Suspend(tcb_t *thread);
exception_t invokeTCB_Resume(tcb_t *thread);
# 245 "/home/b/test/sel4test/kernel/include/object/tcb.h"
exception_t invokeTCB_ThreadControl(tcb_t *target, cte_t *slot, cptr_t faultep,
                                    prio_t mcp, prio_t priority, cap_t cRoot_newCap,
                                    cte_t *cRoot_srcSlot, cap_t vRoot_newCap,
                                    cte_t *vRoot_srcSlot, word_t bufferAddr,
                                    cap_t bufferCap, cte_t *bufferSrcSlot,
                                    thread_control_flag_t updateFlags);

exception_t invokeTCB_CopyRegisters(tcb_t *dest, tcb_t *src,
                                    bool_t suspendSource, bool_t resumeTarget,
                                    bool_t transferFrame, bool_t transferInteger,
                                    word_t transferArch);
exception_t invokeTCB_ReadRegisters(tcb_t *src, bool_t suspendSource,
                                    word_t n, word_t arch, bool_t call);
exception_t invokeTCB_WriteRegisters(tcb_t *dest, bool_t resumeTarget,
                                     word_t n, word_t arch, word_t *buffer);
exception_t invokeTCB_NotificationControl(tcb_t *tcb, notification_t *ntfnPtr);
void invokeDomainSetSet(tcb_t *tcb, dom_t domain);

cptr_t __attribute__((__pure__)) getExtraCPtr(word_t *bufferPtr, word_t i);
void setExtraBadge(word_t *bufferPtr, word_t badge, word_t i);

exception_t lookupExtraCaps(tcb_t *thread, word_t *bufferPtr, seL4_MessageInfo_t info);
word_t setMRs_syscall_error(tcb_t *thread, word_t *receiveIPCBuffer);
word_t __attribute__((__const__)) Arch_decodeTransfer(word_t flags);
exception_t __attribute__((__const__)) Arch_performTransfer(word_t arch, tcb_t *tcb_src,
                                       tcb_t *tcb_dest);


void setThreadName(tcb_t *thread, const char *name);
# 13 "/home/b/test/sel4test/kernel/include/model/statedata.h" 2
# 38 "/home/b/test/sel4test/kernel/include/model/statedata.h"
/* UP states are declared as VISIBLE so that they are accessible in assembly */
# 58 "/home/b/test/sel4test/kernel/include/model/statedata.h"

extern tcb_queue_t ksReadyQueues[(1 * 256)] __attribute__((externally_visible));
extern word_t ksReadyQueuesL1Bitmap[1] __attribute__((externally_visible));
extern word_t ksReadyQueuesL2Bitmap[1][((256 + (1 << 6) - 1) / (1 << 6))] __attribute__((externally_visible));
extern tcb_t *ksCurThread __attribute__((externally_visible));
extern tcb_t *ksIdleThread __attribute__((externally_visible));
extern tcb_t *ksSchedulerAction __attribute__((externally_visible));
# 76 "/home/b/test/sel4test/kernel/include/model/statedata.h"
/* The thread using the FPU, or NULL if FPU state is invalid */
extern tcb_t * ksCurFPUOwner __attribute__((externally_visible));



extern tcb_t * ksDebugTCBs __attribute__((externally_visible));
# 92 "/home/b/test/sel4test/kernel/include/model/statedata.h"
;

extern word_t ksNumCPUs;






extern word_t ksWorkUnitsCompleted;
extern irq_state_t intStateIRQTable[];
extern cte_t intStateIRQNode[];

extern const dschedule_t ksDomSchedule[];
extern const word_t ksDomScheduleLength;
extern word_t ksDomScheduleIdx;
extern dom_t ksCurDomain;



extern word_t ksDomainTime;


extern char ksIdleThreadTCB[1][(1ul << (12))];
# 22 "/home/b/test/sel4test/kernel/include/arch/arm/arch/machine/gic_v3.h" 2
# 1 "/home/b/test/sel4test/kernel/include/arch/arm/armv/armv8-a/64/armv/machine.h" 1
/*
 * Copyright 2020, Data61, CSIRO (ABN 41 687 119 230)
 *
 * SPDX-License-Identifier: GPL-2.0-only
 */

       

static inline void wfi(void)
{
    __asm__ volatile("wfi" ::: "memory");
}

static inline void dsb(void)
{
    __asm__ volatile("dsb sy" ::: "memory");
}

static inline void dsb_ishst(void)
{
    __asm__ volatile("dsb ishst" ::: "memory");
}

static inline void dmb(void)
{
    __asm__ volatile("dmb sy" ::: "memory");
}

static inline void isb(void)
{
    __asm__ volatile("isb sy" ::: "memory");
}
# 23 "/home/b/test/sel4test/kernel/include/arch/arm/arch/machine/gic_v3.h" 2

# 1 "/home/b/test/sel4test/kernel/include/arch/arm/arch/machine/gic_common.h" 1
/*
 * Copyright 2020, Data61, CSIRO (ABN 41 687 119 230)
 *
 * SPDX-License-Identifier: GPL-2.0-only
 */

       



# 1 "/home/b/test/sel4test/kernel/include/machine/interrupt.h" 1
/*
 * Copyright 2020, Data61, CSIRO (ABN 41 687 119 230)
 *
 * SPDX-License-Identifier: GPL-2.0-only
 */

       





/**
 * irq_t is an identifier that represents a hardware interrupt.
 * irq handler capabilities refer to an irq_t which is then used by the
 * kernel to track irq state. An irq_t is also used to interface with an
 * interrupt controller driver using the functions below.
 * For most configurations an irq_t is a word_t type and the irq_t values
 * directly map to hardware irq numbers and are also used as indexes into the
 * kernel's irq cnode that it uses for tracking state.
 * However on SMP configurations where there can be multiple irq_t identifiers
 * for a single hardware irq number, such as when there are core local interrupts,
 * irq_t cannot be assumed to be only a hardware irq number.
 * In this case, irq_t can be defined as a struct containing additional information.
 *
 * Macros are provided to hide this structural difference across configurations:
 * CORE_IRQ_TO_IRQT: converts from a core id and hw irq number to an irq_t
 * IRQT_TO_IDX: converts an irq_t to an index in the irq cnode. It is also used
 *   to encode the irq_t as a single word_t type for sending over IPIs.
 * IDX_TO_IRQT: converts an index in the irq cnode to an irq_t
 * IRQT_TO_CORE: extracts the core out of an irq_t
 * IRQT_TO_IRQL extracts a hw irq out of an irq_t.
 *
 * It is expected that interrupt controller drivers that support SMP provide
 * implementations of these Macros.
 * Currently only Arm SMP configurations use this scheme.
 */






typedef word_t irq_t;







/**
 * Return a currently pending IRQ.
 *
 * This function can be called multiple times and needs to return the same IRQ
 * until ackInterrupt is called. getActiveIRQ returns irqInvalid if no interrupt
 * is pending. It is assumed that if isIRQPending is true, then getActiveIRQ
 * will not return irqInvalid. irqInvalid is a per platform constant that cannot
 * correspond to an actual IRQ raised by the platform.
 *
 * @return     The active IRQ. irqInvalid if no IRQ is pending.
 */
static inline irq_t getActiveIRQ(void);

/**
 * Checks if an IRQ is currently pending in the hardware.
 *
 * isIRQPending is used to determine whether to preempt long running operations
 * at various preemption points throughout the kernel. If this returns true, it
 * means that if the Kernel were to return to user mode, it would then
 * immediately take an interrupt.
 *
 * @return     True if irq pending, False otherwise.
 */
static inline bool_t isIRQPending(void);

/**
 * maskInterrupt disables and enables IRQs.
 *
 * When an IRQ is disabled, it should not raise an interrupt on the processor.
 *
 * @param[in]  disable  True to disable IRQ, False to enable IRQ
 * @param[in]  irq      The IRQ to modify
 */
static inline void maskInterrupt(bool_t disable, irq_t irq);

/**
 * Acks the interrupt
 *
 * ackInterrupt is used by the kernel to indicate it has processed the interrupt
 * delivery and getActiveIRQ is now able to return a different IRQ number. Note
 * that this is called after a notification has been signalled to user level,
 * but before user level has handled the cause and does not imply that the cause
 * of the interrupt has been handled.
 *
 * @param[in]  irq   IRQ to ack
 */
static inline void ackInterrupt(irq_t irq);

/**
 * Deactivates the interrupt
 *
 * When the interrupt controller supports delegating the interrupt to a lower
 * privilege level, this function can be called to signal the completion of
 * interrupt processing so that the interrupt state machine can be moved out of
 * the active state.
 *
 * Currently only supported by gicv3 driver.
 *
 * @param[in]  irq   The interrupt request
 */
static inline void deactivateInterrupt(irq_t irq);

/**
 * Called when getActiveIRQ returns irqInvalid while the kernel is handling an
 * interrupt entry. An implementation is not required to do anything here, but
 * can report the spurious IRQ or try prevent it from reoccurring.
 */
static inline void handleSpuriousIRQ(void);

/**
 * Handle a platform-reserved IRQ.
 *
 * Platform specific implementation for handling IRQs for interrupts that are
 * reserved and not made available to user-level. Will be called if getActiveIRQ
 * returns an IRQ number that is reserved. After this function returns,
 * ackInterrupt will likely be immediately called after.
 *
 * @param[in]  irq   The irq
 */
static inline void handleReservedIRQ(irq_t irq);
# 12 "/home/b/test/sel4test/kernel/include/arch/arm/arch/machine/gic_common.h" 2

/* Shift positions for GICD_SGIR register */




/* Special IRQ's */



/* CPU specific IRQ's */




/* Shared Peripheral Interrupts */
# 59 "/home/b/test/sel4test/kernel/include/arch/arm/arch/machine/gic_common.h"
irq_t irqInvalid = (uint16_t) -1;


/* Setters/getters helpers for hardware irqs */




/*
 * The only sane way to get an GIC IRQ number that can be properly
 * ACKED later is through the int_ack register. Unfortunately, reading
 * this register changes the interrupt state to pending so future
 * reads will not return the same value For this reason, we have a
 * global variable to store the IRQ number.
 */
extern word_t active_irq[1];

static inline void handleSpuriousIRQ(void)
{
}

void initIRQController(void);
# 25 "/home/b/test/sel4test/kernel/include/arch/arm/arch/machine/gic_v3.h" 2
# 35 "/home/b/test/sel4test/kernel/include/arch/arm/arch/machine/gic_v3.h"
/* Register bits */
# 54 "/home/b/test/sel4test/kernel/include/arch/arm/arch/machine/gic_v3.h"
/* System registers for GIC CPU interface */
# 66 "/home/b/test/sel4test/kernel/include/arch/arm/arch/machine/gic_v3.h"
/* Virt control registers */
# 113 "/home/b/test/sel4test/kernel/include/arch/arm/arch/machine/gic_v3.h"
/* Helpers for VGIC */
# 135 "/home/b/test/sel4test/kernel/include/arch/arm/arch/machine/gic_v3.h"
/* Memory map for GIC distributor */
struct gic_dist_map {
    uint32_t ctlr; /* 0x0000 */
    uint32_t typer; /* 0x0004 */
    uint32_t iidr; /* 0x0008 */
    uint32_t res0; /* 0x000C */
    uint32_t statusr; /* 0x0010 */
    uint32_t res1[11]; /* [0x0014, 0x0040) */
    uint32_t setspi_nsr; /* 0x0040 */
    uint32_t res2; /* 0x0044 */
    uint32_t clrspi_nsr; /* 0x0048 */
    uint32_t res3; /* 0x004C */
    uint32_t setspi_sr; /* 0x0050 */
    uint32_t res4; /* 0x0054 */
    uint32_t clrspi_sr; /* 0x0058 */
    uint32_t res5[9]; /* [0x005C, 0x0080) */
    uint32_t igrouprn[32]; /* [0x0080, 0x0100) */

    uint32_t isenablern[32]; /* [0x100, 0x180) */
    uint32_t icenablern[32]; /* [0x180, 0x200) */
    uint32_t ispendrn[32]; /* [0x200, 0x280) */
    uint32_t icpendrn[32]; /* [0x280, 0x300) */
    uint32_t isactivern[32]; /* [0x300, 0x380) */
    uint32_t icactivern[32]; /* [0x380, 0x400) */

    uint32_t ipriorityrn[255]; /* [0x400, 0x7FC) */
    uint32_t res6; /* 0x7FC */

    uint32_t itargetsrn[254]; /* [0x800, 0xBF8) */
    uint32_t res7[2]; /* 0xBF8 */

    uint32_t icfgrn[64]; /* [0xC00, 0xD00) */
    uint32_t igrpmodrn[64]; /* [0xD00, 0xE00) */
    uint32_t nsacrn[64]; /* [0xE00, 0xF00) */
    uint32_t sgir; /* 0xF00 */
    uint32_t res8[3]; /* [0xF04, 0xF10) */
    uint32_t cpendsgirn[4]; /* [0xF10, 0xF20) */
    uint32_t spendsgirn[4]; /* [0xF20, 0xF30) */
    uint32_t res9[5236]; /* [0x0F30, 0x6100) */

    uint64_t iroutern[960]; /* [0x6100, 0x7F00) irouter<n> to configure IRQs
                                     * with INTID from 32 to 1019. iroutern[0] is the
                                     * interrupt routing for SPI 32 */
};

/* __builtin_offsetof is not in the verification C subset, so we can only check this in
   non-verification builds. We specifically do not declare a macro for the builtin, because
   we do not want break the verification subset by accident. */
_Static_assert(0x6100 == __builtin_offsetof(struct gic_dist_map, iroutern), "error_in_gic_dist_map");
                                                                                      ;

/* Memory map for GIC Redistributor Registers for control and physical LPI's */
struct gic_rdist_map { /* Starting */
    uint32_t ctlr; /* 0x0000 */
    uint32_t iidr; /* 0x0004 */
    uint64_t typer; /* 0x0008 */
    uint32_t statusr; /* 0x0010 */
    uint32_t waker; /* 0x0014 */
    uint32_t res0[10]; /* 0x0018 */
    uint64_t setlpir; /* 0x0040 */
    uint64_t clrlpir; /* 0x0048 */
    uint32_t res1[8]; /* 0x0050 */
    uint64_t propbaser; /* 0x0070 */
    uint64_t pendbaser; /* 0x0078 */
    uint32_t res2[8]; /* 0x0080 */
    uint64_t invlpir; /* 0x00a0 */
    uint32_t res3[2]; /* 0x00a8 */
    uint64_t invallr; /* 0x00b0 */
    uint32_t res4[2]; /* 0x00b8 */
    uint32_t syncr; /* 0x00c0 */
};

/* Memory map for the GIC Redistributor Registers for the SGI and PPI's */
struct gic_rdist_sgi_ppi_map { /* Starting */
    uint32_t res0[32]; /* 0x0000 */
    uint32_t igroupr0; /* 0x0080 */
    uint32_t res1[31]; /* 0x0084 */
    uint32_t isenabler0; /* 0x0100 */
    uint32_t res2[31]; /* 0x0104 */
    uint32_t icenabler0; /* 0x0180 */
    uint32_t res3[31]; /* 0x0184 */
    uint32_t ispendr0; /* 0x0200 */
    uint32_t res4[31]; /* 0x0204 */
    uint32_t icpendr0; /* 0x0280 */
    uint32_t res5[31]; /* 0x0284 */
    uint32_t isactiver0; /* 0x0300 */
    uint32_t res6[31]; /* 0x0304 */
    uint32_t icactiver0; /* 0x0380 */
    uint32_t res7[31]; /* 0x0384 */
    uint32_t ipriorityrn[8]; /* 0x0400 */
    uint32_t res8[504]; /* 0x0420 */
    uint32_t icfgr0; /* 0x0C00 */
    uint32_t icfgr1; /* 0x0C04 */
    uint32_t res9[62]; /* 0x0C08 */
    uint32_t igrpmodr0; /* 0x0D00*/
    uint32_t res10[63]; /* 0x0D04 */
    uint32_t nsacr; /* 0x0E00 */
};

extern volatile struct gic_dist_map *const gic_dist;
extern volatile struct gic_rdist_map *gic_rdist_map[1];
extern volatile struct gic_rdist_sgi_ppi_map *gic_rdist_sgi_ppi_map[1];

/* Helpers */
static inline void gic_enable_clr(word_t irq)
{
    int word = ((irq) >> 5u);
    int bit = ((irq) & 0x1f);
    /* Using |= here is detrimental to your health */
    if (irq < 32u) {
        gic_rdist_sgi_ppi_map[0lu]->icenabler0 = (1ul << (bit));
    } else {
        gic_dist->icenablern[word] = (1ul << (bit));
    }

}

static inline void gic_enable_set(word_t irq)
{
    int word = ((irq) >> 5u);
    int bit = ((irq) & 0x1f);

    if (irq < 32u) {
        gic_rdist_sgi_ppi_map[0lu]->isenabler0 = (1ul << (bit));
    } else {
        gic_dist->isenablern[word] = (1ul << (bit));
    }

}

static inline irq_t getActiveIRQ(void)
{
    irq_t irq;

    if (!(((active_irq[0lu]) & ((1ul << (16u)) - 1ul)) < 1020u)) {
        word_t val = 0;
        __asm__ volatile("mrs %x0," "S3_0_C12_C12_0" : "=r"(val));
        active_irq[0lu] = val;
    }

    if ((((active_irq[0lu]) & ((1ul << (16u)) - 1ul)) < 1020u)) {
        irq = (active_irq[0lu] & ((1ul << (16u)) - 1ul));
    } else {
        irq = irqInvalid;
    }

    return irq;
}

/*
 * GIC has 4 states:
 * seL4 expects two states: active->inactive.
 * We ignore the active state in GIC to conform
 */
/** MODIFIES: phantom_machine_state */
/** DONT_TRANSLATE */
static inline bool_t isIRQPending(void)
{
    word_t val = 0;
    /* Check for pending IRQs in group 1: ICC_HPPIR1_EL1 */
    __asm__ volatile("mrs %x0," "S3_0_C12_C12_2" : "=r"(val));
    return (((val) & ((1ul << (16u)) - 1ul)) < 1020u);
}

static inline void maskInterrupt(bool_t disable, irq_t irq)
{




    if (disable) {
        gic_enable_clr((irq));
    } else {
        gic_enable_set((irq));
    }
}


static inline void deactivateInterrupt(irq_t irq)
{
    word_t hw_irq = (irq);
    /* Perform deactivation of hw_irq */
    do { word_t _v = hw_irq; __asm__ volatile("msr " "S3_0_C12_C11_1" ",%x0" :: "r" (_v)); }while(0);
}

static inline void ackInterrupt(irq_t irq)
{
    do { if (!((((active_irq[0lu]) & ((1ul << (16u)) - 1ul)) < 1020u) && (active_irq[0lu] & ((1ul << (16u)) - 1ul)) == (irq))) { _assert_fail("IS_IRQ_VALID(active_irq[CURRENT_CPU_INDEX()]) && (active_irq[CURRENT_CPU_INDEX()] & IRQ_MASK) == IRQT_TO_IRQ(irq)", "/home/b/test/sel4test/kernel/include/arch/arm/arch/machine/gic_v3.h", 322, __func__); } } while(0)
                                                                               ;
    active_irq[0lu] = 1023u;

    word_t hw_irq = (irq);

    /* Set End of Interrupt for active IRQ: ICC_EOIR1_EL1 */
    /* Perform priority drop for current IRQ */
    do { word_t _v = hw_irq; __asm__ volatile("msr " "S3_0_C12_C12_1" ",%x0" :: "r" (_v)); }while(0);

    // If the IRQ is not going to user level then we need to deactivate it too.
    if (__builtin_expect(!!(hw_irq > maxIRQ), 0) ||
        intStateIRQTable[(irq)] != IRQSignal) {
        /* There needs to be an isb() to ensure completion of the system
         * register write in ackInterrupt
         */
        isb();
        deactivateInterrupt(irq);
    }

}
# 24 "/home/b/test/sel4test/cbuild/kernel/gen_headers/plat/platform_gen.h" 2
# 1 "/home/b/test/sel4test/kernel/include/drivers/timer/arm_generic.h" 1
/*
 * Copyright 2020, Data61, CSIRO (ABN 41 687 119 230)
 *
 * SPDX-License-Identifier: GPL-2.0-only
 */

       


# 1 "/home/b/test/sel4test/kernel/include/arch/arm/arch/64/mode/machine.h" 1
/*
 * Copyright 2020, Data61, CSIRO (ABN 41 687 119 230)
 *
 * SPDX-License-Identifier: GPL-2.0-only
 */

       






# 1 "/home/b/test/sel4test/kernel/include/plat/default/plat/machine/hardware.h" 1
/*
 * Copyright 2020, Data61, CSIRO (ABN 41 687 119 230)
 *
 * SPDX-License-Identifier: GPL-2.0-only
 */

       
# 15 "/home/b/test/sel4test/kernel/include/arch/arm/arch/64/mode/machine.h" 2

# 1 "/home/b/test/sel4test/kernel/include/arch/arm/arch/model/smp.h" 1
/*
 * Copyright 2020, Data61, CSIRO (ABN 41 687 119 230)
 *
 * SPDX-License-Identifier: GPL-2.0-only
 */

       



# 1 "/home/b/test/sel4test/kernel/include/model/smp.h" 1
/*
 * Copyright 2020, Data61, CSIRO (ABN 41 687 119 230)
 *
 * SPDX-License-Identifier: GPL-2.0-only
 */

       



# 1 "/home/b/test/sel4test/kernel/include/arch/arm/arch/model/statedata.h" 1
/*
 * Copyright 2014, General Dynamics C4 Systems
 *
 * SPDX-License-Identifier: GPL-2.0-only
 */

       


# 1 "/home/b/test/sel4test/kernel/include/arch/arm/arch/object/vcpu.h" 1
/*
 * Copyright 2014, General Dynamics C4 Systems
 *
 * SPDX-License-Identifier: GPL-2.0-only
 */

       
# 207 "/home/b/test/sel4test/kernel/include/arch/arm/arch/object/vcpu.h"
/* used in boot.c with a guard, use a marco to avoid exposing vcpu_t */


static inline void VGICMaintenance(void) {}
# 11 "/home/b/test/sel4test/kernel/include/arch/arm/arch/model/statedata.h" 2





/* TODO: add ARM-dependent fields here */
/* Bitmask of all cores should receive the reschedule IPI */
extern word_t ipiReschedulePending __attribute__((externally_visible));
# 29 "/home/b/test/sel4test/kernel/include/arch/arm/arch/model/statedata.h"
;


extern user_breakpoint_state_t armKSNullBreakpointState __attribute__((externally_visible));
# 12 "/home/b/test/sel4test/kernel/include/model/smp.h" 2
# 12 "/home/b/test/sel4test/kernel/include/arch/arm/arch/model/smp.h" 2
# 17 "/home/b/test/sel4test/kernel/include/arch/arm/arch/64/mode/machine.h" 2


# 1 "/home/b/test/sel4test/kernel/include/arch/arm/arch/64/mode/machine_pl2.h" 1
/*
 * Copyright 2020, Data61, CSIRO (ABN 41 687 119 230)
 *
 * SPDX-License-Identifier: GPL-2.0-only
 */

       
# 25 "/home/b/test/sel4test/kernel/include/arch/arm/arch/64/mode/machine_pl2.h"
static inline void writeTPIDR_EL2(word_t reg) {}
static inline word_t readTPIDR_EL2(void)
{
    return 0;
}



/* used in other files without guards */
static inline void setCurrentPDPL2(paddr_t pa) {}
static inline void invalidateHypTLB(void) {}
static inline void writeContextIDPL2(word_t pd_val) {}
static inline void writeContextIDAndPD(word_t id, word_t pd_val) {}
# 20 "/home/b/test/sel4test/kernel/include/arch/arm/arch/64/mode/machine.h" 2
# 46 "/home/b/test/sel4test/kernel/include/arch/arm/arch/64/mode/machine.h"
word_t __attribute__((__pure__)) getRestartPC(tcb_t *thread);
void setNextPC(tcb_t *thread, word_t v);

static inline word_t getProcessorID(void)
{
    word_t processor_id;
    __asm__ volatile("mrs %x0," "midr_el1" : "=r"(processor_id));
    return processor_id;
}

static inline word_t readSystemControlRegister(void)
{
    word_t scr;
    __asm__ volatile("mrs %x0," "sctlr_el1" : "=r"(scr));
    return scr;
}

static inline void writeSystemControlRegister(word_t scr)
{
    do { word_t _v = scr; __asm__ volatile("msr " "sctlr_el1" ",%x0" :: "r" (_v)); }while(0);
}

static inline word_t readAuxiliaryControlRegister(void)
{
    word_t acr;
    __asm__ volatile("mrs %x0," "actlr_el1" : "=r"(acr));
    return acr;
}

static inline void writeAuxiliaryControlRegister(word_t acr)
{
    do { word_t _v = acr; __asm__ volatile("msr " "actlr_el1" ",%x0" :: "r" (_v)); }while(0);
}

static inline void writeTPIDR_EL0(word_t reg)
{
    do { word_t _v = reg; __asm__ volatile("msr " "tpidr_el0" ",%x0" :: "r" (_v)); }while(0);
}

static inline word_t readTPIDR_EL0(void)
{
    word_t reg;
    __asm__ volatile("mrs %x0," "tpidr_el0" : "=r"(reg));
    return reg;
}

static inline void writeTPIDRRO_EL0(word_t reg)
{
    do { word_t _v = reg; __asm__ volatile("msr " "tpidrro_el0" ",%x0" :: "r" (_v)); }while(0);
}

static inline word_t readTPIDRRO_EL0(void)
{
    word_t reg;
    __asm__ volatile("mrs %x0," "tpidrro_el0" : "=r"(reg));
    return reg;
}

static inline void writeTPIDR_EL1(word_t reg)
{
    do { word_t _v = reg; __asm__ volatile("msr " "tpidr_el1" ",%x0" :: "r" (_v)); }while(0);
}

static inline word_t readTPIDR_EL1(void)
{
    word_t reg;
    __asm__ volatile("mrs %x0," "tpidr_el1" : "=r"(reg));
    return reg;
}

static void arm_save_thread_id(tcb_t *thread)
{
    setRegister(thread, TPIDR_EL0, readTPIDR_EL0());
    setRegister(thread, TPIDRRO_EL0, readTPIDRRO_EL0());
}

static void arm_load_thread_id(tcb_t *thread)
{
    writeTPIDR_EL0(getRegister(thread, TPIDR_EL0));
    writeTPIDRRO_EL0(getRegister(thread, TPIDRRO_EL0));
}
# 155 "/home/b/test/sel4test/kernel/include/arch/arm/arch/64/mode/machine.h"
/* Check if the elfloader set up the TCR_EL2 correctly. */
static inline bool_t checkTCR_EL2(void)
{
    word_t tcr_el2 = 0;
    __asm__ volatile("mrs %x0," "tcr_el2" : "=r"(tcr_el2));

    return (tcr_el2 == ((16) | (1ul << (8)) | (1ul << (10)) | (3 << 12) | (0 << 14) | (4 << 16) | ((1ul << (23)) | (1ul << (31)))));
}

static inline void setCurrentKernelVSpaceRoot(ttbr_t ttbr)
{
    dsb();
    if (wrap_config_set(0)) {
        do { word_t _v = ttbr.words[0]; __asm__ volatile("msr " "ttbr0_el2" ",%x0" :: "r" (_v)); }while(0);
        dsb();
        isb();
        __asm__ volatile("ic ialluis");
        dsb();
    } else {
        do { word_t _v = ttbr.words[0]; __asm__ volatile("msr " "ttbr1_el1" ",%x0" :: "r" (_v)); }while(0);
    }
    isb();
}

static inline void setCurrentUserVSpaceRoot(ttbr_t ttbr)
{
    dsb();
    if (wrap_config_set(0)) {
        do { word_t _v = ttbr.words[0]; __asm__ volatile("msr " "vttbr_el2" ",%x0" :: "r" (_v)); }while(0);
    } else {
        do { word_t _v = ttbr.words[0]; __asm__ volatile("msr " "ttbr0_el1" ",%x0" :: "r" (_v)); }while(0);
    }
    isb();
}

static inline word_t getVTTBR(void)
{
    word_t vttbr;
    __asm__ volatile("mrs %x0," "vttbr_el2" : "=r"(vttbr));
    return vttbr;
}

static inline void setKernelStack(word_t stack_address)
{
    if (wrap_config_set(0)) {
        writeTPIDR_EL2(stack_address);
    } else {
        writeTPIDR_EL1(stack_address);
    }
}

static inline void setVtable(pptr_t addr)
{
    dsb();
    if (wrap_config_set(0)) {
        do { word_t _v = addr; __asm__ volatile("msr " "vbar_el2" ",%x0" :: "r" (_v)); }while(0);
    } else {
        do { word_t _v = addr; __asm__ volatile("msr " "vbar_el1" ",%x0" :: "r" (_v)); }while(0);
    }
    isb();
}

static inline void invalidateLocalTLB_EL2(void)
{
    __asm__ volatile("tlbi alle2");
}

static inline void invalidateLocalTLB_EL1(void)
{
    __asm__ volatile("tlbi alle1");
}

static inline void invalidateLocalTLB(void)
{
    dsb();
    if (wrap_config_set(0)) {
        invalidateLocalTLB_EL2();
        dsb();
        invalidateLocalTLB_EL1();
    } else {
        __asm__ volatile("tlbi vmalle1");
    }
    dsb();
    isb();
}

static inline void invalidateLocalTLB_ASID(asid_t asid)
{
    do { if (!(asid < (1ul << (16)))) { _assert_fail("asid < BIT(16)", "/home/b/test/sel4test/kernel/include/arch/arm/arch/64/mode/machine.h", 243, __func__); } } while(0);

    dsb();
    __asm__ volatile("tlbi aside1, %0" : : "r"(asid << 48));
    dsb();
    isb();
}

static inline void invalidateLocalTLB_VAASID(word_t mva_plus_asid)
{
    dsb();
    __asm__ volatile("tlbi vae1, %0" : : "r"(mva_plus_asid));
    dsb();
    isb();
}

/* Invalidate all stage 1 and stage 2 translations used at
 * EL1 with the current VMID which is specified by vttbr_el2 */
static inline void invalidateLocalTLB_VMALLS12E1(void)
{
    __asm__ volatile("tlbi vmalls12e1");
    dsb();
    isb();
}

/* Invalidate IPA with the current VMID */
static inline void invalidateLocalTLB_IPA(word_t ipa)
{
    __asm__ volatile("tlbi ipas2e1, %0" :: "r"(ipa));
    dsb();
    __asm__ volatile("tlbi vmalle1");
    dsb();
    isb();
}

void lockTLBEntry(vptr_t vaddr);

static inline void cleanByVA(vptr_t vaddr, paddr_t paddr)
{
    __asm__ volatile("dc cvac, %0" : : "r"(vaddr));
    dmb();
}

static inline void cleanByVA_PoU(vptr_t vaddr, paddr_t paddr)
{
    __asm__ volatile("dc cvau, %0" : : "r"(vaddr));
    dmb();
}

static inline void invalidateByVA(vptr_t vaddr, paddr_t paddr)
{
    __asm__ volatile("dc ivac, %0" : : "r"(vaddr));
    dmb();
}

static inline void invalidateByVA_I(vptr_t vaddr, paddr_t paddr)
{
    __asm__ volatile("ic ivau, %0" : : "r"(vaddr));
    dsb();
    isb();
}

static inline void invalidate_I_PoU(void)
{



    __asm__ volatile("ic iallu");

    isb();
}

static inline void cleanInvalByVA(vptr_t vaddr, paddr_t paddr)
{
    __asm__ volatile("dc civac, %0" : : "r"(vaddr));
    dsb();
}

static inline void branchFlush(vptr_t vaddr, paddr_t paddr)
{

}



static inline word_t __attribute__((__pure__)) getESR(void)
{
    word_t ESR;
    if (wrap_config_set(0)) {
        __asm__ volatile("mrs %x0," "esr_el2" : "=r"(ESR));
    } else {
        __asm__ volatile("mrs %x0," "esr_el1" : "=r"(ESR));
    }
    return ESR;
}

static inline word_t __attribute__((__pure__)) getFAR(void)
{
    word_t FAR;
    if (wrap_config_set(0)) {
        __asm__ volatile("mrs %x0," "far_el2" : "=r"(FAR));
    } else {
        __asm__ volatile("mrs %x0," "far_el1" : "=r"(FAR));
    }
    return FAR;
}

static inline word_t ats1e2r(word_t va)
{
    word_t par;
    __asm__ volatile("at s1e2r, %0" :: "r"(va));
    isb();
    __asm__ volatile("mrs %x0," "par_el1" : "=r"(par));
    return par;
}

static inline word_t ats1e1r(word_t va)
{
    word_t par;
    __asm__ volatile("at s1e1r, %0" :: "r"(va));
    isb();
    __asm__ volatile("mrs %x0," "par_el1" : "=r"(par));
    return par;
}


static inline word_t ats2e0r(word_t va)
{
    word_t par;
    __asm__ volatile("at s12e0r, %0" :: "r"(va));
    isb();
    __asm__ volatile("mrs %x0," "par_el1" : "=r"(par));
    return par;
}

void arch_clean_invalidate_caches(void);
void arch_clean_invalidate_L1_caches(word_t type);

static inline paddr_t addressTranslateS1(vptr_t vaddr)
{
    return ats1e1r(vaddr);
}
# 11 "/home/b/test/sel4test/kernel/include/drivers/timer/arm_generic.h" 2

/* ARM generic timer implementation */
# 41 "/home/b/test/sel4test/kernel/include/drivers/timer/arm_generic.h"
# 1 "/home/b/test/sel4test/kernel/include/arch/arm/arch/machine/timer.h" 1
/*
 * Copyright 2020, Data61, CSIRO (ABN 41 687 119 230)
 *
 * SPDX-License-Identifier: GPL-2.0-only
 */

       




/* convert to khz first to avoid overflow */
# 58 "/home/b/test/sel4test/kernel/include/arch/arm/arch/machine/timer.h"
# 1 "/home/b/test/sel4test/kernel/include/arch/arm/arch/64/mode/machine/timer.h" 1
/*
 * Copyright 2020, Data61, CSIRO (ABN 41 687 119 230)
 *
 * SPDX-License-Identifier: GPL-2.0-only
 */

       
# 59 "/home/b/test/sel4test/kernel/include/arch/arm/arch/machine/timer.h" 2


/* but multiply by timer tick ms */






void initTimer(void);
# 42 "/home/b/test/sel4test/kernel/include/drivers/timer/arm_generic.h" 2
static inline void resetTimer(void)
{
    do { word_t _v = ((50000000llu / 1000llu) * 2); __asm__ volatile("msr " "cntv_tval_el0" ",%x0" :: "r" (_v)); }while(0);
    /* Ensure that the timer deasserts the IRQ before GIC EOIR/DIR.
     * This is sufficient to remove the pending state from the GICR
     * and avoid the interrupt happening twice because of the level
     * sensitive configuration. */
    isb();
}


__attribute__((__section__(".boot.text"))) void initGenericTimer(void);
# 25 "/home/b/test/sel4test/cbuild/kernel/gen_headers/plat/platform_gen.h" 2

/* #undef CONFIGURE_SMMU */




/* #undef CONFIGURE_SMMU */
# 9 "/home/b/test/sel4test/kernel/include/plat/default/plat/machine.h" 2
# 10 "/home/b/test/sel4test/kernel/include/machine.h" 2

# 1 "/home/b/test/sel4test/kernel/include/hardware.h" 1
/*
 * Copyright 2014, General Dynamics C4 Systems
 *
 * SPDX-License-Identifier: GPL-2.0-only
 */
       

/* Each architecture defines a set of constants in #defines. These
 * constants describe the memory regions of the kernel's portion of the
 * address space including the physical memory window, the kernel ELF
 * region, and the device region.
 *
 *  - USER_TOP: The first address after the end of user memory
 *
 *  - PADDR_BASE: The first physical address mapped in the kernel's
 *    physical memory window.
 *  - PPTR_BASE: The first virtual address of the kernel's physical
 *    memory window.
 *  - PPTR_TOP: The first virtual address after the end of the kernel's
 *    physical memory window.
 *
 *  - KERNEL_ELF_PADDR_BASE: The first physical address used to map the
 *    initial kernel image. The kernel ELF is mapped contiguously
 *    starting at this address.
 *  - KERNEL_ELF_BASE: The first virtual address used to map the initial
 *    kernel image.
 *
 *  - KDEV_BASE: The first virtual address used to map devices.
 */

/* The offset from a physical address to a virtual address in the
 * physical memory window. */


/* The last address in the physical memory region mapped into the
 * physical memory window */


/* The kernel base offset is a way to translate the kernel image segment
 * from virtual to physical. This translation must be a single offset
 * for for the entire segment (i.e. the kernel image must be contiguous
 * both virtually and physically) */




/* This symbol is generated by the linker and marks the last valid
 * address in the kernel's virtual region */
extern char ki_end[1];
# 12 "/home/b/test/sel4test/kernel/include/machine.h" 2

/* When translating a physical address into an address accessible to the
 * kernel via virtual addressing we always use the mapping of the memory
 * into the physical memory window, even if the mapping originally
 * referred to a kernel virtual address. */
static inline void *__attribute__((__const__)) ptrFromPAddr(paddr_t paddr)
{
    return (void *)(paddr + (0xffffff8000000000ul - 0x0ul));
}

/* When obtaining a physical address from a reference to any object in
 * the physical mapping window, this function must be used. */
static inline paddr_t __attribute__((__const__)) addrFromPPtr(const void *pptr)
{
    return (paddr_t)pptr - (0xffffff8000000000ul - 0x0ul);
}

/* When obtaining a physical address from a reference to an address from
 * the kernel ELF mapping, this function must be used. */
static inline paddr_t __attribute__((__const__)) addrFromKPPtr(const void *pptr)
{
    do { if (!((paddr_t)pptr >= ((0xffffff8000000000ul - 0x0ul) + physBase()))) { _assert_fail("(paddr_t)pptr >= KERNEL_ELF_BASE", "/home/b/test/sel4test/kernel/include/machine.h", 33, __func__); } } while(0);
    do { if (!((paddr_t)pptr <= ((paddr_t)ki_end))) { _assert_fail("(paddr_t)pptr <= KERNEL_ELF_TOP", "/home/b/test/sel4test/kernel/include/machine.h", 34, __func__); } } while(0);
    return (paddr_t)pptr - (((0xffffff8000000000ul - 0x0ul) + physBase()) - physBase());
}





static inline region_t __attribute__((__const__)) paddr_to_pptr_reg(const p_region_t p_reg)
{
    return (region_t) {
        .start = (paddr_t)ptrFromPAddr(p_reg.start),
        .end = (paddr_t)ptrFromPAddr(p_reg.end)
    };
}

static inline p_region_t __attribute__((__const__)) pptr_to_paddr_reg(const region_t reg)
{
    return (p_region_t) {
        .start = addrFromPPtr((const void *)reg.start),
        .end = addrFromPPtr((const void *)reg.end),
    };
}
# 11 "/home/b/test/sel4test/kernel/include/api/syscall.h" 2


# 1 "/home/b/test/sel4test/kernel/include/kernel/vspace.h" 1
/*
 * Copyright 2014, General Dynamics C4 Systems
 *
 * SPDX-License-Identifier: GPL-2.0-only
 */
       


# 1 "/home/b/test/sel4test/kernel/include/arch/arm/arch/kernel/vspace.h" 1
/*
 * Copyright 2014, General Dynamics C4 Systems
 *
 * SPDX-License-Identifier: GPL-2.0-only
 */

       





# 1 "/home/b/test/sel4test/kernel/include/arch/arm/arch/64/mode/kernel/vspace.h" 1
/*
 * Copyright 2020, Data61, CSIRO (ABN 41 687 119 230)
 *
 * SPDX-License-Identifier: GPL-2.0-only
 */

       
# 16 "/home/b/test/sel4test/kernel/include/arch/arm/arch/64/mode/kernel/vspace.h"
/* ==================== BOOT CODE FINISHES HERE ==================== */

bool_t __attribute__((__const__)) isVTableRoot(cap_t cap);
bool_t __attribute__((__const__)) isValidNativeRoot(cap_t cap);

void unmapPageTable(asid_t asid, vptr_t vaddr, pte_t *pt);
void unmapPage(vm_page_size_t page_size, asid_t asid, vptr_t vptr, pptr_t pptr);

void deleteASIDPool(asid_t base, asid_pool_t *pool);
void deleteASID(asid_t asid, vspace_root_t *vspace);




asid_map_t findMapForASID(asid_t asid);




static const region_t __attribute__((__section__(".boot.rodata"))) *mode_reserved_region = ((void *)0);





static inline exception_t performASIDPoolInvocation(asid_t asid, asid_pool_t *poolPtr, cte_t *vspaceCapSlot)
{
    cap_t cap = vspaceCapSlot->cap;
    asid_map_t asid_map = asid_map_asid_map_vspace_new(




                              /* vspace_root: reference to vspace root page table object */
                              cap_vspace_cap_get_capVSBasePtr(cap)




                          );
    cap = cap_vspace_cap_set_capVSMappedASID(cap, asid);
    cap = cap_vspace_cap_set_capVSIsMapped(cap, 1);
    vspaceCapSlot->cap = cap;

    poolPtr->array[asid & ((1ul << (asidLowBits)) - 1ul)] = asid_map;
    return EXCEPTION_NONE;
}

void increaseASIDBindCB(asid_t asid);
void decreaseASIDBindCB(asid_t asid);
# 14 "/home/b/test/sel4test/kernel/include/arch/arm/arch/kernel/vspace.h" 2



cap_t create_it_address_space(cap_t root_cnode_cap, v_region_t it_v_reg);
cap_t create_unmapped_it_frame_cap(pptr_t pptr, bool_t use_large);
cap_t create_mapped_it_frame_cap(cap_t pd_cap, pptr_t pptr, vptr_t vptr, asid_t asid, bool_t use_large,
                                 bool_t executable);

void map_kernel_window(void);
void map_kernel_frame(paddr_t paddr, pptr_t vaddr, vm_rights_t vm_rights, vm_attributes_t vm_attributes);
void activate_kernel_vspace(void);
void write_it_asid_pool(cap_t it_ap_cap, cap_t it_pd_cap);

/* ==================== BOOT CODE FINISHES HERE ==================== */

/* need a fake array to get the pointer from the linker script */
extern char arm_vector_table[1];

word_t *__attribute__((__pure__)) lookupIPCBuffer(bool_t isReceiver, tcb_t *thread);
exception_t handleVMFault(tcb_t *thread, vm_fault_type_t vm_faultType);
void setVMRoot(tcb_t *tcb);
bool_t __attribute__((__const__)) isValidVTableRoot(cap_t cap);
exception_t checkValidIPCBuffer(vptr_t vptr, cap_t cap);

vm_rights_t __attribute__((__const__)) maskVMRights(vm_rights_t vm_rights,
                               seL4_CapRights_t cap_rights_mask);

exception_t decodeARMMMUInvocation(word_t invLabel, word_t length, cptr_t cptr,
                                   cte_t *cte, cap_t cap, bool_t call, word_t *buffer);


void Arch_userStackTrace(tcb_t *tptr);
# 10 "/home/b/test/sel4test/kernel/include/kernel/vspace.h" 2
# 14 "/home/b/test/sel4test/kernel/include/api/syscall.h" 2
# 1 "/home/b/test/sel4test/cbuild/kernel/gen_headers/arch/api/syscall.h" 1
/*
 * Copyright 2020, Data61, CSIRO (ABN 41 687 119 230)
 *
 * SPDX-License-Identifier: GPL-2.0-only
 */


/* This header was generated by kernel/tools/syscall_header_gen.py.
 *
 * To add a system call number, edit kernel/libsel4/include/api/syscall.xml
 *
 */
       
# 34 "/home/b/test/sel4test/cbuild/kernel/gen_headers/arch/api/syscall.h"
enum syscall {
    SysCall = -1,
    SysReplyRecv = -2,
    SysSend = -3,
    SysNBSend = -4,
    SysRecv = -5,
    SysReply = -6,
    SysYield = -7,
    SysNBRecv = -8,

    SysDebugPutChar = -9,
    SysDebugDumpScheduler = -10,


    SysDebugHalt = -11,
    SysDebugCapIdentify = -12,
    SysDebugSnapshot = -13,
    SysDebugNameThread = -14,


    SysDebugSendIPI = -15,
# 84 "/home/b/test/sel4test/cbuild/kernel/gen_headers/arch/api/syscall.h"
};
typedef word_t syscall_t;

/* System call names */

static char *syscall_names[] __attribute__((unused)) = {
         [1] = "Call",
         [2] = "ReplyRecv",
         [3] = "Send",
         [4] = "NBSend",
         [5] = "Recv",
         [6] = "Reply",
         [7] = "Yield",
         [8] = "NBRecv",
};
# 15 "/home/b/test/sel4test/kernel/include/api/syscall.h" 2
# 1 "/home/b/test/sel4test/kernel/include/api/debug.h" 1
/*
 * Copyright 2014, General Dynamics C4 Systems
 *
 * SPDX-License-Identifier: GPL-2.0-only
 */




       

# 1 "/home/b/test/sel4test/kernel/include/benchmark/benchmark_track.h" 1
/*
 * Copyright 2016, General Dynamics C4 Systems
 *
 * SPDX-License-Identifier: GPL-2.0-only
 */

       


# 1 "/home/b/test/sel4test/kernel/include/arch/arm/arch/benchmark.h" 1
/*
 * Copyright 2014, General Dynamics C4 Systems
 *
 * SPDX-License-Identifier: GPL-2.0-only
 */

       
# 11 "/home/b/test/sel4test/kernel/include/benchmark/benchmark_track.h" 2
# 1 "/home/b/test/sel4test/kernel/libsel4/include/sel4/benchmark_track_types.h" 1
/*
 * Copyright 2020, Data61, CSIRO (ABN 41 687 119 230)
 *
 * SPDX-License-Identifier: BSD-2-Clause
 */

       






/* the following code can be used at any point in the kernel
 * to determine detail about the kernel entry point */
typedef enum {
    Entry_Interrupt,
    Entry_UnknownSyscall,
    Entry_UserLevelFault,
    Entry_DebugFault,
    Entry_VMFault,
    Entry_Syscall,
    Entry_UnimplementedDevice,

    Entry_VCPUFault,




} entry_type_t;

/**
 * @brief Kernel entry logging
 *
 * Encapsulates useful info about the cause of the kernel entry
 */
typedef struct __attribute__((packed)) kernel_entry {
    seL4_Word path: 3;
    union {
        struct {
            seL4_Word core: 3;
            seL4_Word word: 26;
        };
        /* Tracked kernel entry info filled from outside this file */
        struct {
            seL4_Word syscall_no: 4;
            seL4_Word cap_type: 5;
            seL4_Word is_fastpath: 1;
            seL4_Word invocation_tag: 19;
        };
    };
} kernel_entry_t;
# 12 "/home/b/test/sel4test/kernel/include/benchmark/benchmark_track.h" 2


# 1 "/home/b/test/sel4test/kernel/include/kernel/cspace.h" 1
/*
 * Copyright 2014, General Dynamics C4 Systems
 *
 * SPDX-License-Identifier: GPL-2.0-only
 */

       






struct lookupCap_ret {
    exception_t status;
    cap_t cap;
};
typedef struct lookupCap_ret lookupCap_ret_t;

struct lookupCapAndSlot_ret {
    exception_t status;
    cap_t cap;
    cte_t *slot;
};
typedef struct lookupCapAndSlot_ret lookupCapAndSlot_ret_t;

struct lookupSlot_raw_ret {
    exception_t status;
    cte_t *slot;
};
typedef struct lookupSlot_raw_ret lookupSlot_raw_ret_t;

struct lookupSlot_ret {
    exception_t status;
    cte_t *slot;
};
typedef struct lookupSlot_ret lookupSlot_ret_t;

struct resolveAddressBits_ret {
    exception_t status;
    cte_t *slot;
    word_t bitsRemaining;
};
typedef struct resolveAddressBits_ret resolveAddressBits_ret_t;

lookupCap_ret_t lookupCap(tcb_t *thread, cptr_t cPtr);
lookupCapAndSlot_ret_t lookupCapAndSlot(tcb_t *thread, cptr_t cPtr);
lookupSlot_raw_ret_t lookupSlot(tcb_t *thread, cptr_t capptr);
lookupSlot_ret_t lookupSlotForCNodeOp(bool_t isSource,
                                      cap_t root, cptr_t capptr,
                                      word_t depth);
lookupSlot_ret_t lookupSourceSlot(cap_t root, cptr_t capptr,
                                  word_t depth);
lookupSlot_ret_t lookupTargetSlot(cap_t root, cptr_t capptr,
                                  word_t depth);
lookupSlot_ret_t lookupPivotSlot(cap_t root, cptr_t capptr,
                                 word_t depth);
resolveAddressBits_ret_t resolveAddressBits(cap_t nodeCap,
                                            cptr_t capptr,
                                            word_t n_bits);
# 15 "/home/b/test/sel4test/kernel/include/benchmark/benchmark_track.h" 2





extern kernel_entry_t ksKernelEntry;
# 50 "/home/b/test/sel4test/kernel/include/benchmark/benchmark_track.h"
static inline void benchmark_debug_syscall_start(word_t cptr, word_t msgInfo, word_t syscall)
{
    seL4_MessageInfo_t info = messageInfoFromWord_raw(msgInfo);
    lookupCapAndSlot_ret_t lu_ret = lookupCapAndSlot(ksCurThread, cptr);
    ksKernelEntry.path = Entry_Syscall;
    ksKernelEntry.syscall_no = -syscall;
    ksKernelEntry.cap_type = cap_get_capType(lu_ret.cap);
    ksKernelEntry.invocation_tag = seL4_MessageInfo_get_label(info);
}
# 13 "/home/b/test/sel4test/kernel/include/api/debug.h" 2



# 1 "/home/b/test/sel4test/kernel/include/kernel/thread.h" 1
/*
 * Copyright 2014, General Dynamics C4 Systems
 *
 * SPDX-License-Identifier: GPL-2.0-only
 */

       




# 1 "/home/b/test/sel4test/kernel/include/arch/arm/arch/machine.h" 1
/*
 * Copyright 2014, General Dynamics C4 Systems
 *
 * SPDX-License-Identifier: GPL-2.0-only
 */

       
# 16 "/home/b/test/sel4test/kernel/include/arch/arm/arch/machine.h"
void map_kernel_devices(void);

void initL2Cache(void);

void initIRQController(void);
void cpu_initLocalIRQController(void);
void setIRQTrigger(irq_t irq, bool_t trigger);



bool_t plat_SGITargetValid(word_t target);
void plat_sendSGI(word_t irq, word_t target);

static inline void plat_cleanL2Range(paddr_t start, paddr_t end);
static inline void plat_invalidateL2Range(paddr_t start, paddr_t end);
static inline void plat_cleanInvalidateL2Range(paddr_t start, paddr_t end);
static inline void plat_cleanInvalidateL2Cache(void);

void cleanInvalidateCacheRange_RAM(word_t start, word_t end, paddr_t pstart);
void cleanCacheRange_RAM(word_t start, word_t end, paddr_t pstart);
void cleanCacheRange_PoU(word_t start, word_t end, paddr_t pstart);
void invalidateCacheRange_RAM(word_t start, word_t end, paddr_t pstart);
void invalidateCacheRange_I(word_t start, word_t end, paddr_t pstart);
void branchFlushRange(word_t start, word_t end, paddr_t pstart);

void clean_D_PoU(void);
void cleanInvalidate_D_PoC(void);
void cleanInvalidate_L1D(void);
void cleanCaches_PoU(void);
void cleanInvalidateL1Caches(void);

/* Cleaning memory before user-level access. Does not flush cache. */
static inline void clearMemory(word_t *ptr, word_t bits)
{
    memzero(ptr, (1ul << (bits)));
}

/* Cleaning memory before page table walker access */
static inline void clearMemory_PT(word_t *ptr, word_t bits)
{
    memzero(ptr, (1ul << (bits)));
    cleanCacheRange_PoU((word_t)ptr, (word_t)ptr + (1ul << (bits)) - 1,
                        addrFromPPtr(ptr));
}
# 68 "/home/b/test/sel4test/kernel/include/arch/arm/arch/machine.h"
/* Update the value of the actual register to hold the expected value */
static inline exception_t Arch_setTLSRegister(word_t tls_base)
{
    /* This register is saved and restored on kernel exit and entry so
     * we only update it in the saved context. */
    setRegister(ksCurThread, TLS_BASE, tls_base);
    return EXCEPTION_NONE;
}
# 13 "/home/b/test/sel4test/kernel/include/kernel/thread.h" 2






static inline __attribute__((__const__)) word_t ready_queues_index(word_t dom, word_t prio)
{
    if (numDomains > 1) {
        return dom * 256 + prio;
    } else {
        do { if (!(dom == 0)) { _assert_fail("dom == 0", "/home/b/test/sel4test/kernel/include/kernel/thread.h", 24, __func__); } } while(0);
        return prio;
    }
}

static inline __attribute__((__const__)) word_t prio_to_l1index(word_t prio)
{
    return (prio >> 6);
}

static inline __attribute__((__const__)) word_t l1index_to_prio(word_t l1index)
{
    return (l1index << 6);
}

static inline bool_t __attribute__((__pure__)) isRunnable(const tcb_t *thread)
{
    switch (thread_state_get_tsType(thread->tcbState)) {
    case ThreadState_Running:
    case ThreadState_Restart:



        return true;

    default:
        return false;
    }
}

static inline __attribute__((__const__)) word_t invert_l1index(word_t l1index)
{
    word_t inverted = (((256 + (1 << 6) - 1) / (1 << 6)) - 1 - l1index);
    do { if (!(inverted < ((256 + (1 << 6) - 1) / (1 << 6)))) { _assert_fail("inverted < L2_BITMAP_SIZE", "/home/b/test/sel4test/kernel/include/kernel/thread.h", 57, __func__); } } while(0);
    return inverted;
}

static inline prio_t getHighestPrio(word_t dom)
{
    word_t l1index;
    word_t l2index;
    word_t l1index_inverted;

    /* it's undefined to call clzl on 0 */
    do { if (!(ksReadyQueuesL1Bitmap[dom] != 0)) { _assert_fail("NODE_STATE(ksReadyQueuesL1Bitmap)[dom] != 0", "/home/b/test/sel4test/kernel/include/kernel/thread.h", 68, __func__); } } while(0);

    l1index = (1 << 6) - 1 - clzl(ksReadyQueuesL1Bitmap[dom]);
    l1index_inverted = invert_l1index(l1index);
    do { if (!(ksReadyQueuesL2Bitmap[dom][l1index_inverted] != 0)) { _assert_fail("NODE_STATE(ksReadyQueuesL2Bitmap)[dom][l1index_inverted] != 0", "/home/b/test/sel4test/kernel/include/kernel/thread.h", 72, __func__); } } while(0);
    l2index = (1 << 6) - 1 - clzl(ksReadyQueuesL2Bitmap[dom][l1index_inverted]);
    return (l1index_to_prio(l1index) | l2index);
}

static inline bool_t isHighestPrio(word_t dom, prio_t prio)
{
    return ksReadyQueuesL1Bitmap[dom] == 0 ||
           prio >= getHighestPrio(dom);
}

static inline bool_t __attribute__((__pure__)) isBlocked(const tcb_t *thread)
{
    switch (thread_state_get_tsType(thread->tcbState)) {
    case ThreadState_BlockedOnReceive:
    case ThreadState_BlockedOnSend:
    case ThreadState_BlockedOnNotification:
    case ThreadState_BlockedOnReply:
        return true;

    default:
        return false;
    }
}

static inline bool_t __attribute__((__pure__)) isStopped(const tcb_t *thread)
{
    switch (thread_state_get_tsType(thread->tcbState)) {
    case ThreadState_Inactive:
    case ThreadState_BlockedOnReceive:
    case ThreadState_BlockedOnSend:
    case ThreadState_BlockedOnNotification:
    case ThreadState_BlockedOnReply:
        return true;

    default:
        return false;
    }
}
# 163 "/home/b/test/sel4test/kernel/include/kernel/thread.h"
void Arch_switchToThread(tcb_t *tcb);
void Arch_switchToIdleThread(void);
void Arch_configureIdleThread(tcb_t *tcb);
void Arch_activateIdleThread(tcb_t *tcb);

void idle_thread(void);

void configureIdleThread(tcb_t *tcb);
void activateThread(void);
void suspend(tcb_t *target);
void restart(tcb_t *target);
void doIPCTransfer(tcb_t *sender, endpoint_t *endpoint,
                   word_t badge, bool_t grant, tcb_t *receiver);



void doReplyTransfer(tcb_t *sender, tcb_t *receiver, cte_t *slot, bool_t grant);
void timerTick(void);

void doNormalTransfer(tcb_t *sender, word_t *sendBuffer, endpoint_t *endpoint,
                      word_t badge, bool_t canGrant, tcb_t *receiver,
                      word_t *receiveBuffer);
void doFaultTransfer(word_t badge, tcb_t *sender, tcb_t *receiver,
                     word_t *receiverIPCBuffer);
void doNBRecvFailedTransfer(tcb_t *thread);
void schedule(void);
void chooseThread(void);
void switchToThread(tcb_t *thread);
void switchToIdleThread(void);
void prepareSetDomain(tcb_t *tptr, dom_t dom);
void setDomain(tcb_t *tptr, dom_t dom);
void setPriority(tcb_t *tptr, prio_t prio);
void setMCPriority(tcb_t *tptr, prio_t mcp);
void scheduleTCB(tcb_t *tptr);
void possibleSwitchTo(tcb_t *tptr);
void setThreadState(tcb_t *tptr, _thread_state_t ts);
void rescheduleRequired(void);

/* declare that the thread has had its registers (in its user_context_t) modified and it
 * should ignore any 'efficient' restores next time it is run, and instead restore all
 * registers into their correct place */
void Arch_postModifyRegisters(tcb_t *tptr);

/* Updates a threads FaultIP to match its NextIP. This is used to indicate that a
 * thread has completed its fault and by updating the restartPC means that if the thread
 * should get restarted in the future for any reason it is restart in such a way as to
 * not cause the fault again. */
static inline void updateRestartPC(tcb_t *tcb)
{
    setRegister(tcb, FaultIP, getRegister(tcb, NextIP));
}
# 17 "/home/b/test/sel4test/kernel/include/api/debug.h" 2



static inline void debug_printKernelEntryReason(void)
{
    printf("\nKernel entry via ");
    switch (ksKernelEntry.path) {
    case Entry_Interrupt:
        printf("Interrupt, irq %lu\n", (unsigned long) ksKernelEntry.word);
        break;
    case Entry_UnknownSyscall:
        printf("Unknown syscall, word: %lu", (unsigned long) ksKernelEntry.word);
        break;
    case Entry_VMFault:
        printf("VM Fault, fault type: %lu\n", (unsigned long) ksKernelEntry.word);
        break;
    case Entry_UserLevelFault:
        printf("User level fault, number: %lu", (unsigned long) ksKernelEntry.word);
        break;

    case Entry_DebugFault:
        printf("Debug fault. Fault Vaddr: 0x%lx", (unsigned long) ksKernelEntry.word);
        break;

    case Entry_Syscall:
        printf("Syscall, number: %ld, %s\n", (long) ksKernelEntry.syscall_no, syscall_names[ksKernelEntry.syscall_no]);
        if (ksKernelEntry.syscall_no == -SysSend ||
            ksKernelEntry.syscall_no == -SysNBSend ||
            ksKernelEntry.syscall_no == -SysCall) {

            printf("Cap type: %lu, Invocation tag: %lu\n", (unsigned long) ksKernelEntry.cap_type,
                   (unsigned long) ksKernelEntry.invocation_tag);
        }
        break;

    case Entry_VCPUFault:
        printf("VCPUFault\n");
        break;






    default:
        printf("Unknown (%u)\n", ksKernelEntry.path);
        break;

    }
}

/* Prints the user context and stack trace of the current thread */
static inline void debug_printUserState(void)
{
    tcb_t *tptr = ksCurThread;
    printf("Current thread: %s\n", ((debug_tcb_t *)(((cte_t *)((word_t)(tptr)&~((1ul << (12)) - 1ul)))+(tcbCNodeEntries)))->tcbName);
    printf("Next instruction address: %lx\n", getRestartPC(tptr));
    printf("Stack:\n");
    Arch_userStackTrace(tptr);
}

static inline void debug_printTCB(tcb_t *tcb)
{
    printf("%40s\t", ((debug_tcb_t *)(((cte_t *)((word_t)(tcb)&~((1ul << (12)) - 1ul)))+(tcbCNodeEntries)))->tcbName);
    char *state;
    switch (thread_state_get_tsType(tcb->tcbState)) {
    case ThreadState_Inactive:
        state = "inactive";
        break;
    case ThreadState_Running:
        state = "running";
        break;
    case ThreadState_Restart:
        state = "restart";
        break;
    case ThreadState_BlockedOnReceive:
        state = "blocked on recv";
        break;
    case ThreadState_BlockedOnSend:
        state = "blocked on send";
        break;
    case ThreadState_BlockedOnReply:
        state = "blocked on reply";
        break;
    case ThreadState_BlockedOnNotification:
        state = "blocked on ntfn";
        break;





    case ThreadState_IdleThreadState:
        state = "idle";
        break;
    default:
        _fail("Unknown thread state", "/home/b/test/sel4test/kernel/include/api/debug.h", 113, __func__);
    }

    word_t core = 0;
    printf("%15s\t%p\t%20lu\t%lu", state, (void *) getRestartPC(tcb), tcb->tcbPriority, core);



    printf("\n");
}

static inline void debug_dumpScheduler(void)
{
    printf("Dumping all tcbs!\n");
    printf("Name                                    \tState          \tIP                  \t Prio \t Core%s\n",
           wrap_config_set(0) ? "\t InReleaseQueue" : "");
    printf("--------------------------------------------------------------------------------------\n");
    for (tcb_t *curr = ksDebugTCBs; curr != ((void *)0); curr = ((debug_tcb_t *)(((cte_t *)((word_t)(curr)&~((1ul << (12)) - 1ul)))+(tcbCNodeEntries)))->tcbDebugNext) {
        debug_printTCB(curr);
    }
}
# 16 "/home/b/test/sel4test/kernel/include/api/syscall.h" 2
# 32 "/home/b/test/sel4test/kernel/include/api/syscall.h"
exception_t handleSyscall(syscall_t syscall);
exception_t handleInterruptEntry(void);
exception_t handleUnknownSyscall(word_t w);
exception_t handleUserLevelFault(word_t w_a, word_t w_b);
exception_t handleVMFaultEvent(vm_fault_type_t vm_faultType);

static inline word_t __attribute__((__pure__)) getSyscallArg(word_t i, word_t *ipc_buffer)
{
    if (i < n_msgRegisters) {
        return getRegister(ksCurThread, msgRegisters[i]);
    }

    do { if (!(ipc_buffer != ((void *)0))) { _assert_fail("ipc_buffer != NULL", "/home/b/test/sel4test/kernel/include/api/syscall.h", 44, __func__); } } while(0);
    return ipc_buffer[i + 1];
}

extern extra_caps_t current_extra_caps;
# 11 "/home/b/test/sel4test/kernel/src/api/faults.c" 2

# 1 "/home/b/test/sel4test/kernel/include/arch/arm/arch/kernel/thread.h" 1
/*
 * Copyright 2014, General Dynamics C4 Systems
 *
 * SPDX-License-Identifier: GPL-2.0-only
 */

       

# 1 "/home/b/test/sel4test/kernel/include/arch/arm/arch/64/mode/kernel/thread.h" 1
/*
 * Copyright 2017, General Dynamics C4 Systems
 *
 * SPDX-License-Identifier: GPL-2.0-only
 */

       

static inline word_t __attribute__((__const__)) sanitiseRegister(register_t reg, word_t v, bool_t archInfo)
{
    if (reg == SPSR_EL1) {
        if (archInfo) {
            switch (v & 0x1f) {
            case 0:
            case 4:
            case 5:
                return v;
            default:
                break;
            }
        }
        return (v & 0xf0000000) | ((1 << 6) | 0 | 0);
    } else {
        return v;
    }
}

static inline bool_t __attribute__((__const__)) Arch_getSanitiseRegisterInfo(tcb_t *thread)
{



    return 0;

}
# 10 "/home/b/test/sel4test/kernel/include/arch/arm/arch/kernel/thread.h" 2
# 13 "/home/b/test/sel4test/kernel/src/api/faults.c" 2
# 1 "/home/b/test/sel4test/kernel/include/machine/debug.h" 1
/*
 * Copyright 2020, Data61, CSIRO (ABN 41 687 119 230)
 *
 * SPDX-License-Identifier: GPL-2.0-only
 */

       





# 1 "/home/b/test/sel4test/kernel/include/arch/arm/arch/machine/debug.h" 1
/*
 * Copyright 2020, Data61, CSIRO (ABN 41 687 119 230)
 *
 * SPDX-License-Identifier: GPL-2.0-only
 */

       





# 1 "/home/b/test/sel4test/kernel/include/arch/arm/arch/64/mode/machine/debug.h" 1
/*
 * Copyright 2020, Data61, CSIRO (ABN 41 687 119 230)
 *
 * SPDX-License-Identifier: GPL-2.0-only
 */

       



void aarch64_restore_user_debug_context(tcb_t *target_thread);

/** Determines and carries out what needs to be done for a debug exception.
 *
 * This could be handling a single-stepping exception, or a breakpoint or
 * watchpoint.
 */
seL4_Fault_t handleUserLevelDebugException(word_t esr, word_t fault_vaddr);
bool_t isDebugFault(word_t esr);






/** Generates read functions for the CP14 control and value registers.
 */
# 85 "/home/b/test/sel4test/kernel/include/arch/arm/arch/64/mode/machine/debug.h"
/** Generates write functions for the CP14 control and value registers.
 */
# 145 "/home/b/test/sel4test/kernel/include/arch/arm/arch/64/mode/machine/debug.h"
exception_t handleDebugFaultEvent(word_t esr);
# 14 "/home/b/test/sel4test/kernel/include/arch/arm/arch/machine/debug.h" 2
# 1 "/home/b/test/sel4test/kernel/include/arch/arm/armv/armv8-a/64/armv/debug.h" 1
/*
 * Copyright 2020, Data61, CSIRO (ABN 41 687 119 230)
 *
 * SPDX-License-Identifier: GPL-2.0-only
 */

       



enum v8_breakpoint_type {
    DBGBCR_TYPE_UNLINKED_INSTRUCTION_MATCH = 0u,
    DBGBCR_TYPE_LINKED_INSTRUCTION_MATCH = 0x1u,

    DBGBCR_TYPE_UNLINKED_CONTEXT_ID_MATCH = 0x2u,
    DBGBCR_TYPE_LINKED_CONTEXT_ID_MATCH = 0x3u,

    DBGBCR_TYPE_UNLINKED_INSTRUCTION_MISMATCH = 0x4u,
    DBGBCR_TYPE_LINKED_INSTRUCTION_MISMATCH = 0x5u,

    DBGBCR_TYPE_UNLINKED_CONTEXTIDR_EL1_MATCH = 0x6u,
    DBGBCR_TYPE_LINKED_CONTEXTIDR_EL1_MATCH = 0x7u,

    DBGBCR_TYPE_UNLINKED_VMID_MATCH = 0x8u,
    DBGBCR_TYPE_LINKED_VMID_MATCH = 0x9u,

    DBGBCR_TYPE_UNLINKED_CONTEXT_ID_AND_VMID_MATCH = 0xAu,
    DBGBCR_TYPE_LINKED_CONTEXT_ID_AND_VMID_MATCH = 0xBu,

    DBGBCR_TYPE_UNLINKED_CONTEXTIDR_EL2_MATCH = 0xCu,
    DBGBCR_TYPE_LINKED_CONTEXTIDR_EL2_MATCH = 0xDu,

    DBGBCR_TYPE_UNLINKED_FULL_CONTEXT_ID_MATCH = 0xEu,
    DBGBCR_TYPE_LINKED_FULL_CONTEXT_ID_MATCH = 0xFu,
};

static inline dbg_bcr_t Arch_setupBcr(dbg_bcr_t bcr, bool_t is_match)
{
    if (is_match) {
        bcr = dbg_bcr_set_breakpointType(bcr, DBGBCR_TYPE_UNLINKED_INSTRUCTION_MATCH);
    } else {
        bcr = dbg_bcr_set_breakpointType(bcr, DBGBCR_TYPE_UNLINKED_INSTRUCTION_MISMATCH);
    }
    return bcr;
}

static inline bool_t Arch_breakpointIsSingleStepping(tcb_t *t, uint16_t bp_num)
{
    /* AARCH64 does not use breakpoints for single-stepping */
    return false;
}
# 15 "/home/b/test/sel4test/kernel/include/arch/arm/arch/machine/debug.h" 2


void restore_user_debug_context(tcb_t *target_thread);
void saveAllBreakpointState(tcb_t *t);
void loadAllDisabledBreakpointState(void);




static inline word_t readBcrCp(uint16_t bp_num) { word_t ret; switch (bp_num) { case 1: __asm__ volatile("mrs %x0," "DBGBCR" "1" "_EL1" : "=r"(ret)); return ret; case 2: __asm__ volatile("mrs %x0," "DBGBCR" "2" "_EL1" : "=r"(ret)); return ret; case 3: __asm__ volatile("mrs %x0," "DBGBCR" "3" "_EL1" : "=r"(ret)); return ret; case 4: __asm__ volatile("mrs %x0," "DBGBCR" "4" "_EL1" : "=r"(ret)); return ret; case 5: __asm__ volatile("mrs %x0," "DBGBCR" "5" "_EL1" : "=r"(ret)); return ret; case 6: __asm__ volatile("mrs %x0," "DBGBCR" "6" "_EL1" : "=r"(ret)); return ret; case 7: __asm__ volatile("mrs %x0," "DBGBCR" "7" "_EL1" : "=r"(ret)); return ret; case 8: __asm__ volatile("mrs %x0," "DBGBCR" "8" "_EL1" : "=r"(ret)); return ret; case 9: __asm__ volatile("mrs %x0," "DBGBCR" "9" "_EL1" : "=r"(ret)); return ret; case 10: __asm__ volatile("mrs %x0," "DBGBCR" "10" "_EL1" : "=r"(ret)); return ret; case 11: __asm__ volatile("mrs %x0," "DBGBCR" "11" "_EL1" : "=r"(ret)); return ret; case 12: __asm__ volatile("mrs %x0," "DBGBCR" "12" "_EL1" : "=r"(ret)); return ret; case 13: __asm__ volatile("mrs %x0," "DBGBCR" "13" "_EL1" : "=r"(ret)); return ret; case 14: __asm__ volatile("mrs %x0," "DBGBCR" "14" "_EL1" : "=r"(ret)); return ret; case 15: __asm__ volatile("mrs %x0," "DBGBCR" "15" "_EL1" : "=r"(ret)); return ret; default: do { if (!(bp_num == 0)) { _assert_fail("bp_num == 0", "/home/b/test/sel4test/kernel/include/arch/arm/arch/machine/debug.h", 24, __func__); } } while(0); __asm__ volatile("mrs %x0," "DBGBCR" "0" "_EL1" : "=r"(ret)); return ret; } }
static inline word_t readBvrCp(uint16_t bp_num) { word_t ret; switch (bp_num) { case 1: __asm__ volatile("mrs %x0," "DBGBVR" "1" "_EL1" : "=r"(ret)); return ret; case 2: __asm__ volatile("mrs %x0," "DBGBVR" "2" "_EL1" : "=r"(ret)); return ret; case 3: __asm__ volatile("mrs %x0," "DBGBVR" "3" "_EL1" : "=r"(ret)); return ret; case 4: __asm__ volatile("mrs %x0," "DBGBVR" "4" "_EL1" : "=r"(ret)); return ret; case 5: __asm__ volatile("mrs %x0," "DBGBVR" "5" "_EL1" : "=r"(ret)); return ret; case 6: __asm__ volatile("mrs %x0," "DBGBVR" "6" "_EL1" : "=r"(ret)); return ret; case 7: __asm__ volatile("mrs %x0," "DBGBVR" "7" "_EL1" : "=r"(ret)); return ret; case 8: __asm__ volatile("mrs %x0," "DBGBVR" "8" "_EL1" : "=r"(ret)); return ret; case 9: __asm__ volatile("mrs %x0," "DBGBVR" "9" "_EL1" : "=r"(ret)); return ret; case 10: __asm__ volatile("mrs %x0," "DBGBVR" "10" "_EL1" : "=r"(ret)); return ret; case 11: __asm__ volatile("mrs %x0," "DBGBVR" "11" "_EL1" : "=r"(ret)); return ret; case 12: __asm__ volatile("mrs %x0," "DBGBVR" "12" "_EL1" : "=r"(ret)); return ret; case 13: __asm__ volatile("mrs %x0," "DBGBVR" "13" "_EL1" : "=r"(ret)); return ret; case 14: __asm__ volatile("mrs %x0," "DBGBVR" "14" "_EL1" : "=r"(ret)); return ret; case 15: __asm__ volatile("mrs %x0," "DBGBVR" "15" "_EL1" : "=r"(ret)); return ret; default: do { if (!(bp_num == 0)) { _assert_fail("bp_num == 0", "/home/b/test/sel4test/kernel/include/arch/arm/arch/machine/debug.h", 25, __func__); } } while(0); __asm__ volatile("mrs %x0," "DBGBVR" "0" "_EL1" : "=r"(ret)); return ret; } }
static inline word_t readWcrCp(uint16_t bp_num) { word_t ret; switch (bp_num) { case 1: __asm__ volatile("mrs %x0," "DBGWCR" "1" "_EL1" : "=r"(ret)); return ret; case 2: __asm__ volatile("mrs %x0," "DBGWCR" "2" "_EL1" : "=r"(ret)); return ret; case 3: __asm__ volatile("mrs %x0," "DBGWCR" "3" "_EL1" : "=r"(ret)); return ret; case 4: __asm__ volatile("mrs %x0," "DBGWCR" "4" "_EL1" : "=r"(ret)); return ret; case 5: __asm__ volatile("mrs %x0," "DBGWCR" "5" "_EL1" : "=r"(ret)); return ret; case 6: __asm__ volatile("mrs %x0," "DBGWCR" "6" "_EL1" : "=r"(ret)); return ret; case 7: __asm__ volatile("mrs %x0," "DBGWCR" "7" "_EL1" : "=r"(ret)); return ret; case 8: __asm__ volatile("mrs %x0," "DBGWCR" "8" "_EL1" : "=r"(ret)); return ret; case 9: __asm__ volatile("mrs %x0," "DBGWCR" "9" "_EL1" : "=r"(ret)); return ret; case 10: __asm__ volatile("mrs %x0," "DBGWCR" "10" "_EL1" : "=r"(ret)); return ret; case 11: __asm__ volatile("mrs %x0," "DBGWCR" "11" "_EL1" : "=r"(ret)); return ret; case 12: __asm__ volatile("mrs %x0," "DBGWCR" "12" "_EL1" : "=r"(ret)); return ret; case 13: __asm__ volatile("mrs %x0," "DBGWCR" "13" "_EL1" : "=r"(ret)); return ret; case 14: __asm__ volatile("mrs %x0," "DBGWCR" "14" "_EL1" : "=r"(ret)); return ret; case 15: __asm__ volatile("mrs %x0," "DBGWCR" "15" "_EL1" : "=r"(ret)); return ret; default: do { if (!(bp_num == 0)) { _assert_fail("bp_num == 0", "/home/b/test/sel4test/kernel/include/arch/arm/arch/machine/debug.h", 26, __func__); } } while(0); __asm__ volatile("mrs %x0," "DBGWCR" "0" "_EL1" : "=r"(ret)); return ret; } }
static inline word_t readWvrCp(uint16_t bp_num) { word_t ret; switch (bp_num) { case 1: __asm__ volatile("mrs %x0," "DBGWVR" "1" "_EL1" : "=r"(ret)); return ret; case 2: __asm__ volatile("mrs %x0," "DBGWVR" "2" "_EL1" : "=r"(ret)); return ret; case 3: __asm__ volatile("mrs %x0," "DBGWVR" "3" "_EL1" : "=r"(ret)); return ret; case 4: __asm__ volatile("mrs %x0," "DBGWVR" "4" "_EL1" : "=r"(ret)); return ret; case 5: __asm__ volatile("mrs %x0," "DBGWVR" "5" "_EL1" : "=r"(ret)); return ret; case 6: __asm__ volatile("mrs %x0," "DBGWVR" "6" "_EL1" : "=r"(ret)); return ret; case 7: __asm__ volatile("mrs %x0," "DBGWVR" "7" "_EL1" : "=r"(ret)); return ret; case 8: __asm__ volatile("mrs %x0," "DBGWVR" "8" "_EL1" : "=r"(ret)); return ret; case 9: __asm__ volatile("mrs %x0," "DBGWVR" "9" "_EL1" : "=r"(ret)); return ret; case 10: __asm__ volatile("mrs %x0," "DBGWVR" "10" "_EL1" : "=r"(ret)); return ret; case 11: __asm__ volatile("mrs %x0," "DBGWVR" "11" "_EL1" : "=r"(ret)); return ret; case 12: __asm__ volatile("mrs %x0," "DBGWVR" "12" "_EL1" : "=r"(ret)); return ret; case 13: __asm__ volatile("mrs %x0," "DBGWVR" "13" "_EL1" : "=r"(ret)); return ret; case 14: __asm__ volatile("mrs %x0," "DBGWVR" "14" "_EL1" : "=r"(ret)); return ret; case 15: __asm__ volatile("mrs %x0," "DBGWVR" "15" "_EL1" : "=r"(ret)); return ret; default: do { if (!(bp_num == 0)) { _assert_fail("bp_num == 0", "/home/b/test/sel4test/kernel/include/arch/arm/arch/machine/debug.h", 27, __func__); } } while(0); __asm__ volatile("mrs %x0," "DBGWVR" "0" "_EL1" : "=r"(ret)); return ret; } }
static inline void writeBcrCp(uint16_t bp_num, word_t val) { switch (bp_num) { case 1: do { word_t _v = val; __asm__ volatile("msr " "DBGBCR" "1" "_EL1" ",%x0" :: "r" (_v)); }while(0); return; case 2: do { word_t _v = val; __asm__ volatile("msr " "DBGBCR" "2" "_EL1" ",%x0" :: "r" (_v)); }while(0); return; case 3: do { word_t _v = val; __asm__ volatile("msr " "DBGBCR" "3" "_EL1" ",%x0" :: "r" (_v)); }while(0); return; case 4: do { word_t _v = val; __asm__ volatile("msr " "DBGBCR" "4" "_EL1" ",%x0" :: "r" (_v)); }while(0); return; case 5: do { word_t _v = val; __asm__ volatile("msr " "DBGBCR" "5" "_EL1" ",%x0" :: "r" (_v)); }while(0); return; case 6: do { word_t _v = val; __asm__ volatile("msr " "DBGBCR" "6" "_EL1" ",%x0" :: "r" (_v)); }while(0); return; case 7: do { word_t _v = val; __asm__ volatile("msr " "DBGBCR" "7" "_EL1" ",%x0" :: "r" (_v)); }while(0); return; case 8: do { word_t _v = val; __asm__ volatile("msr " "DBGBCR" "8" "_EL1" ",%x0" :: "r" (_v)); }while(0); return; case 9: do { word_t _v = val; __asm__ volatile("msr " "DBGBCR" "9" "_EL1" ",%x0" :: "r" (_v)); }while(0); return; case 10: do { word_t _v = val; __asm__ volatile("msr " "DBGBCR" "10" "_EL1" ",%x0" :: "r" (_v)); }while(0); return; case 11: do { word_t _v = val; __asm__ volatile("msr " "DBGBCR" "11" "_EL1" ",%x0" :: "r" (_v)); }while(0); return; case 12: do { word_t _v = val; __asm__ volatile("msr " "DBGBCR" "12" "_EL1" ",%x0" :: "r" (_v)); }while(0); return; case 13: do { word_t _v = val; __asm__ volatile("msr " "DBGBCR" "13" "_EL1" ",%x0" :: "r" (_v)); }while(0); return; case 14: do { word_t _v = val; __asm__ volatile("msr " "DBGBCR" "14" "_EL1" ",%x0" :: "r" (_v)); }while(0); return; case 15: do { word_t _v = val; __asm__ volatile("msr " "DBGBCR" "15" "_EL1" ",%x0" :: "r" (_v)); }while(0); return; default: do { if (!(bp_num == 0)) { _assert_fail("bp_num == 0", "/home/b/test/sel4test/kernel/include/arch/arm/arch/machine/debug.h", 28, __func__); } } while(0); do { word_t _v = val; __asm__ volatile("msr " "DBGBCR" "0" "_EL1" ",%x0" :: "r" (_v)); }while(0); return; } }
static inline void writeBvrCp(uint16_t bp_num, word_t val) { switch (bp_num) { case 1: do { word_t _v = val; __asm__ volatile("msr " "DBGBVR" "1" "_EL1" ",%x0" :: "r" (_v)); }while(0); return; case 2: do { word_t _v = val; __asm__ volatile("msr " "DBGBVR" "2" "_EL1" ",%x0" :: "r" (_v)); }while(0); return; case 3: do { word_t _v = val; __asm__ volatile("msr " "DBGBVR" "3" "_EL1" ",%x0" :: "r" (_v)); }while(0); return; case 4: do { word_t _v = val; __asm__ volatile("msr " "DBGBVR" "4" "_EL1" ",%x0" :: "r" (_v)); }while(0); return; case 5: do { word_t _v = val; __asm__ volatile("msr " "DBGBVR" "5" "_EL1" ",%x0" :: "r" (_v)); }while(0); return; case 6: do { word_t _v = val; __asm__ volatile("msr " "DBGBVR" "6" "_EL1" ",%x0" :: "r" (_v)); }while(0); return; case 7: do { word_t _v = val; __asm__ volatile("msr " "DBGBVR" "7" "_EL1" ",%x0" :: "r" (_v)); }while(0); return; case 8: do { word_t _v = val; __asm__ volatile("msr " "DBGBVR" "8" "_EL1" ",%x0" :: "r" (_v)); }while(0); return; case 9: do { word_t _v = val; __asm__ volatile("msr " "DBGBVR" "9" "_EL1" ",%x0" :: "r" (_v)); }while(0); return; case 10: do { word_t _v = val; __asm__ volatile("msr " "DBGBVR" "10" "_EL1" ",%x0" :: "r" (_v)); }while(0); return; case 11: do { word_t _v = val; __asm__ volatile("msr " "DBGBVR" "11" "_EL1" ",%x0" :: "r" (_v)); }while(0); return; case 12: do { word_t _v = val; __asm__ volatile("msr " "DBGBVR" "12" "_EL1" ",%x0" :: "r" (_v)); }while(0); return; case 13: do { word_t _v = val; __asm__ volatile("msr " "DBGBVR" "13" "_EL1" ",%x0" :: "r" (_v)); }while(0); return; case 14: do { word_t _v = val; __asm__ volatile("msr " "DBGBVR" "14" "_EL1" ",%x0" :: "r" (_v)); }while(0); return; case 15: do { word_t _v = val; __asm__ volatile("msr " "DBGBVR" "15" "_EL1" ",%x0" :: "r" (_v)); }while(0); return; default: do { if (!(bp_num == 0)) { _assert_fail("bp_num == 0", "/home/b/test/sel4test/kernel/include/arch/arm/arch/machine/debug.h", 29, __func__); } } while(0); do { word_t _v = val; __asm__ volatile("msr " "DBGBVR" "0" "_EL1" ",%x0" :: "r" (_v)); }while(0); return; } }
static inline void writeWcrCp(uint16_t bp_num, word_t val) { switch (bp_num) { case 1: do { word_t _v = val; __asm__ volatile("msr " "DBGWCR" "1" "_EL1" ",%x0" :: "r" (_v)); }while(0); return; case 2: do { word_t _v = val; __asm__ volatile("msr " "DBGWCR" "2" "_EL1" ",%x0" :: "r" (_v)); }while(0); return; case 3: do { word_t _v = val; __asm__ volatile("msr " "DBGWCR" "3" "_EL1" ",%x0" :: "r" (_v)); }while(0); return; case 4: do { word_t _v = val; __asm__ volatile("msr " "DBGWCR" "4" "_EL1" ",%x0" :: "r" (_v)); }while(0); return; case 5: do { word_t _v = val; __asm__ volatile("msr " "DBGWCR" "5" "_EL1" ",%x0" :: "r" (_v)); }while(0); return; case 6: do { word_t _v = val; __asm__ volatile("msr " "DBGWCR" "6" "_EL1" ",%x0" :: "r" (_v)); }while(0); return; case 7: do { word_t _v = val; __asm__ volatile("msr " "DBGWCR" "7" "_EL1" ",%x0" :: "r" (_v)); }while(0); return; case 8: do { word_t _v = val; __asm__ volatile("msr " "DBGWCR" "8" "_EL1" ",%x0" :: "r" (_v)); }while(0); return; case 9: do { word_t _v = val; __asm__ volatile("msr " "DBGWCR" "9" "_EL1" ",%x0" :: "r" (_v)); }while(0); return; case 10: do { word_t _v = val; __asm__ volatile("msr " "DBGWCR" "10" "_EL1" ",%x0" :: "r" (_v)); }while(0); return; case 11: do { word_t _v = val; __asm__ volatile("msr " "DBGWCR" "11" "_EL1" ",%x0" :: "r" (_v)); }while(0); return; case 12: do { word_t _v = val; __asm__ volatile("msr " "DBGWCR" "12" "_EL1" ",%x0" :: "r" (_v)); }while(0); return; case 13: do { word_t _v = val; __asm__ volatile("msr " "DBGWCR" "13" "_EL1" ",%x0" :: "r" (_v)); }while(0); return; case 14: do { word_t _v = val; __asm__ volatile("msr " "DBGWCR" "14" "_EL1" ",%x0" :: "r" (_v)); }while(0); return; case 15: do { word_t _v = val; __asm__ volatile("msr " "DBGWCR" "15" "_EL1" ",%x0" :: "r" (_v)); }while(0); return; default: do { if (!(bp_num == 0)) { _assert_fail("bp_num == 0", "/home/b/test/sel4test/kernel/include/arch/arm/arch/machine/debug.h", 30, __func__); } } while(0); do { word_t _v = val; __asm__ volatile("msr " "DBGWCR" "0" "_EL1" ",%x0" :: "r" (_v)); }while(0); return; } }
static inline void writeWvrCp(uint16_t bp_num, word_t val) { switch (bp_num) { case 1: do { word_t _v = val; __asm__ volatile("msr " "DBGWVR" "1" "_EL1" ",%x0" :: "r" (_v)); }while(0); return; case 2: do { word_t _v = val; __asm__ volatile("msr " "DBGWVR" "2" "_EL1" ",%x0" :: "r" (_v)); }while(0); return; case 3: do { word_t _v = val; __asm__ volatile("msr " "DBGWVR" "3" "_EL1" ",%x0" :: "r" (_v)); }while(0); return; case 4: do { word_t _v = val; __asm__ volatile("msr " "DBGWVR" "4" "_EL1" ",%x0" :: "r" (_v)); }while(0); return; case 5: do { word_t _v = val; __asm__ volatile("msr " "DBGWVR" "5" "_EL1" ",%x0" :: "r" (_v)); }while(0); return; case 6: do { word_t _v = val; __asm__ volatile("msr " "DBGWVR" "6" "_EL1" ",%x0" :: "r" (_v)); }while(0); return; case 7: do { word_t _v = val; __asm__ volatile("msr " "DBGWVR" "7" "_EL1" ",%x0" :: "r" (_v)); }while(0); return; case 8: do { word_t _v = val; __asm__ volatile("msr " "DBGWVR" "8" "_EL1" ",%x0" :: "r" (_v)); }while(0); return; case 9: do { word_t _v = val; __asm__ volatile("msr " "DBGWVR" "9" "_EL1" ",%x0" :: "r" (_v)); }while(0); return; case 10: do { word_t _v = val; __asm__ volatile("msr " "DBGWVR" "10" "_EL1" ",%x0" :: "r" (_v)); }while(0); return; case 11: do { word_t _v = val; __asm__ volatile("msr " "DBGWVR" "11" "_EL1" ",%x0" :: "r" (_v)); }while(0); return; case 12: do { word_t _v = val; __asm__ volatile("msr " "DBGWVR" "12" "_EL1" ",%x0" :: "r" (_v)); }while(0); return; case 13: do { word_t _v = val; __asm__ volatile("msr " "DBGWVR" "13" "_EL1" ",%x0" :: "r" (_v)); }while(0); return; case 14: do { word_t _v = val; __asm__ volatile("msr " "DBGWVR" "14" "_EL1" ",%x0" :: "r" (_v)); }while(0); return; case 15: do { word_t _v = val; __asm__ volatile("msr " "DBGWVR" "15" "_EL1" ",%x0" :: "r" (_v)); }while(0); return; default: do { if (!(bp_num == 0)) { _assert_fail("bp_num == 0", "/home/b/test/sel4test/kernel/include/arch/arm/arch/machine/debug.h", 31, __func__); } } while(0); do { word_t _v = val; __asm__ volatile("msr " "DBGWVR" "0" "_EL1" ",%x0" :: "r" (_v)); }while(0); return; } }

void writeBvrContext(tcb_t *t, uint16_t index, word_t val);
void writeBcrContext(tcb_t *t, uint16_t index, word_t val);
word_t readBcrContext(tcb_t *t, uint16_t index);




enum breakpoint_privilege /* BCR[2:1] */ {
    DBGBCR_PRIV_RESERVED = 0u,
    DBGBCR_PRIV_PRIVILEGED = 1u,
    DBGBCR_PRIV_USER = 2u,
    /* Use either when doing context linking, because the linked WVR or BVR that
     * specifies the vaddr, overrides the context-programmed BCR privilege.
     */
    DBGBCR_BCR_PRIV_EITHER = 3u
};

int getAndResetActiveBreakpoint(word_t vaddr, word_t reason);
__attribute__((__section__(".boot.text"))) void disableAllBpsAndWps(void);
uint16_t getBpNumFromType(uint16_t bp_num, word_t type);
# 148 "/home/b/test/sel4test/kernel/include/arch/arm/arch/machine/debug.h"
/** Convert a watchpoint size (0, 1, 2, 4 or 8 bytes) into the arch specific
 * register encoding.
 */
static inline word_t convertSizeToArch(word_t size)
{
    switch (size) {
    case 1:
        return 0x1;
    case 2:
        return 0x3;
    case 8:
        return 0xFF;
    default:
        do { if (!(size == 4)) { _assert_fail("size == 4", "/home/b/test/sel4test/kernel/include/arch/arm/arch/machine/debug.h", 161, __func__); } } while(0);
        return 0xF;
    }
}

/** These next two functions are part of some state flags.
 *
 * A bitfield of all currently enabled breakpoints for a thread is kept in that
 * thread's TCB. These two functions here set and unset the bits in that
 * bitfield.
 */
static inline void setBreakpointUsedFlag(tcb_t *t, uint16_t bp_num)
{
    if (t != ((void *)0)) {
        t->tcbArch.tcbContext.breakpointState.used_breakpoints_bf |= (1ul << (bp_num));
    }
}

static inline void unsetBreakpointUsedFlag(tcb_t *t, uint16_t bp_num)
{
    if (t != ((void *)0)) {
        t->tcbArch.tcbContext.breakpointState.used_breakpoints_bf &= ~(1ul << (bp_num));
    }
}

static uint16_t convertBpNumToArch(uint16_t bp_num)
{
    if (bp_num >= 6) {
        bp_num -= 6;
    }
    return bp_num;
}

static word_t getTypeFromBpNum(uint16_t bp_num)
{
    return (bp_num >= 6)
           ? seL4_DataBreakpoint
           : seL4_InstructionBreakpoint;
}

static inline syscall_error_t Arch_decodeConfigureSingleStepping(tcb_t *t,
                                                                 uint16_t bp_num,
                                                                 word_t n_instr,
                                                                 bool_t is_reply)
{



    syscall_error_t ret = {
        .type = seL4_NoError
    };

    if (is_reply) {
        /* If this is a single-step fault reply, just default to the already-
         * configured bp_num. Of course, this assumes that a register had
         * already previously been configured for single-stepping.
         */
        if (!t->tcbArch.tcbContext.breakpointState.single_step_enabled) {
            do { printf("\033[0m" "\033[1m" "<<" "\033[0m" "\033[32m" "seL4(CPU %" "lu" ")" "\033[0m" "\033[1m" " [%s/%d T%p \"%s\" @%lx]: " "Debug: Single-step reply when single-stepping not " "enabled." ">>" "\033[0m" "\n", 0lu, __func__, 219, ksCurThread, ((debug_tcb_t *)(((cte_t *)((word_t)(ksCurThread)&~((1ul << (12)) - 1ul)))+(tcbCNodeEntries)))->tcbName, (word_t)getRestartPC(ksCurThread)); } while (0)
                                 ;
            ret.type = seL4_IllegalOperation;
            return ret;
        }

        /* The following code relates to checking that the specified breakpoint is suitable to
            for being used for conguring single stepping. AARCH64 does not need to use breakpoints
            to simulate single stepping, so these checks can be omitted. */
# 254 "/home/b/test/sel4test/kernel/include/arch/arm/arch/machine/debug.h"
    }

    return ret;
}

bool_t byte8WatchpointsSupported(void);

static inline syscall_error_t Arch_decodeSetBreakpoint(tcb_t *t,
                                                       uint16_t bp_num, word_t vaddr, word_t type,
                                                       word_t size, word_t rw)
{
    syscall_error_t ret = {
        .type = seL4_NoError
    };

    bp_num = convertBpNumToArch(bp_num);

    if (type == seL4_DataBreakpoint) {
        if (bp_num >= 4) {
            do { printf("\033[0m" "\033[1m" "<<" "\033[0m" "\033[32m" "seL4(CPU %" "lu" ")" "\033[0m" "\033[1m" " [%s/%d T%p \"%s\" @%lx]: " "Debug: invalid data-watchpoint number %u." ">>" "\033[0m" "\n", 0lu, __func__, 273, ksCurThread, ((debug_tcb_t *)(((cte_t *)((word_t)(ksCurThread)&~((1ul << (12)) - 1ul)))+(tcbCNodeEntries)))->tcbName, (word_t)getRestartPC(ksCurThread), bp_num); } while (0);
            ret.type = seL4_RangeError;
            ret.rangeErrorMin = 0;
            ret.rangeErrorMax = 6 - 1;
            return ret;
        }
    } else if (type == seL4_InstructionBreakpoint) {
        if (bp_num >= 6) {
            do { printf("\033[0m" "\033[1m" "<<" "\033[0m" "\033[32m" "seL4(CPU %" "lu" ")" "\033[0m" "\033[1m" " [%s/%d T%p \"%s\" @%lx]: " "Debug: invalid instruction breakpoint number %u." ">>" "\033[0m" "\n", 0lu, __func__, 281, ksCurThread, ((debug_tcb_t *)(((cte_t *)((word_t)(ksCurThread)&~((1ul << (12)) - 1ul)))+(tcbCNodeEntries)))->tcbName, (word_t)getRestartPC(ksCurThread), bp_num); } while (0);
            ret.type = seL4_RangeError;
            ret.rangeErrorMin = 0;
            ret.rangeErrorMax = 4 - 1;
            return ret;
        }
    }

    if (size == 8 && !byte8WatchpointsSupported()) {
        do { printf("\033[0m" "\033[1m" "<<" "\033[0m" "\033[32m" "seL4(CPU %" "lu" ")" "\033[0m" "\033[1m" " [%s/%d T%p \"%s\" @%lx]: " "Debug: 8-byte watchpoints not supported on this CPU." ">>" "\033[0m" "\n", 0lu, __func__, 290, ksCurThread, ((debug_tcb_t *)(((cte_t *)((word_t)(ksCurThread)&~((1ul << (12)) - 1ul)))+(tcbCNodeEntries)))->tcbName, (word_t)getRestartPC(ksCurThread)); } while (0);
        ret.type = seL4_InvalidArgument;
        ret.invalidArgumentNumber = 3;
        return ret;
    }

    if (size == 8 && type != seL4_DataBreakpoint) {
        do { printf("\033[0m" "\033[1m" "<<" "\033[0m" "\033[32m" "seL4(CPU %" "lu" ")" "\033[0m" "\033[1m" " [%s/%d T%p \"%s\" @%lx]: " "Debug: 8-byte sizes can only be used with watchpoints." ">>" "\033[0m" "\n", 0lu, __func__, 297, ksCurThread, ((debug_tcb_t *)(((cte_t *)((word_t)(ksCurThread)&~((1ul << (12)) - 1ul)))+(tcbCNodeEntries)))->tcbName, (word_t)getRestartPC(ksCurThread)); } while (0);
        ret.type = seL4_InvalidArgument;
        ret.invalidArgumentNumber = 3;
        return ret;
    }

    return ret;
}

static inline syscall_error_t Arch_decodeGetBreakpoint(tcb_t *t, uint16_t bp_num)
{
    syscall_error_t ret = {
        .type = seL4_NoError
    };

    if (bp_num >= 6 + 4) {
        do { printf("\033[0m" "\033[1m" "<<" "\033[0m" "\033[32m" "seL4(CPU %" "lu" ")" "\033[0m" "\033[1m" " [%s/%d T%p \"%s\" @%lx]: " "Arch Debug: Invalid API bp_num %u." ">>" "\033[0m" "\n", 0lu, __func__, 313, ksCurThread, ((debug_tcb_t *)(((cte_t *)((word_t)(ksCurThread)&~((1ul << (12)) - 1ul)))+(tcbCNodeEntries)))->tcbName, (word_t)getRestartPC(ksCurThread), bp_num); } while (0);
        ret.type = seL4_NoError;
        return ret;
    }
    return ret;
}

static inline syscall_error_t Arch_decodeUnsetBreakpoint(tcb_t *t, uint16_t bp_num)
{
    syscall_error_t ret = {
        .type = seL4_NoError
    };

    if (bp_num >= 6 + 4) {
        do { printf("\033[0m" "\033[1m" "<<" "\033[0m" "\033[32m" "seL4(CPU %" "lu" ")" "\033[0m" "\033[1m" " [%s/%d T%p \"%s\" @%lx]: " "Arch Debug: Invalid API bp_num %u." ">>" "\033[0m" "\n", 0lu, __func__, 327, ksCurThread, ((debug_tcb_t *)(((cte_t *)((word_t)(ksCurThread)&~((1ul << (12)) - 1ul)))+(tcbCNodeEntries)))->tcbName, (word_t)getRestartPC(ksCurThread), bp_num); } while (0);
        ret.type = seL4_NoError;
        return ret;
    }

    word_t type;

    type = getTypeFromBpNum(bp_num);
    bp_num = convertBpNumToArch(bp_num);

    if (type == seL4_InstructionBreakpoint) {
        if (Arch_breakpointIsSingleStepping(t, bp_num)) {
            do { printf("\033[0m" "\033[1m" "<<" "\033[0m" "\033[32m" "seL4(CPU %" "lu" ")" "\033[0m" "\033[1m" " [%s/%d T%p \"%s\" @%lx]: " "Rejecting call to unsetBreakpoint on breakpoint configured " "for single-stepping (hwid %u)." ">>" "\033[0m" "\n", 0lu, __func__, 339, ksCurThread, ((debug_tcb_t *)(((cte_t *)((word_t)(ksCurThread)&~((1ul << (12)) - 1ul)))+(tcbCNodeEntries)))->tcbName, (word_t)getRestartPC(ksCurThread), bp_num); } while (0)
                                                               ;
            ret.type = seL4_IllegalOperation;
            return ret;
        }
    }

    return ret;
}
# 14 "/home/b/test/sel4test/kernel/include/machine/debug.h" 2



/* Arch specific setup functions */
__attribute__((__section__(".boot.text"))) bool_t Arch_initHardwareBreakpoints(void);
void Arch_breakpointThreadDelete(tcb_t *thread);

/** Sets up (and overwrites) the current configuration of a hardware breakpoint.
 * @param bp_num Hardware breakpoint ID. Usually an integer from 0..N.
 * @param vaddr Address that the breakpoint should be triggered by.
 * @param type Type of operation that should trigger the breakpoint.
 * @param size Operand size that should trigger the breakpoint.
 * @param rwx Access type (read/write) that should trigger the breakpoint.
 * @param uds If NULL, this function call will write directly to the hardware
 *            registers.
 *            If non-NULL, 'uds' is assumed to be a pointer to a debug register
 *            context-saving memory block, and this function will write to that
 *            context-saving memory block instead.
 */
void setBreakpoint(tcb_t *t,
                   uint16_t bp_num,
                   word_t vaddr, word_t type, word_t size, word_t rw);

/** Reads and returns the current configuration of a hardware breakpoint.
 * @param bp_num Hardware breakpoint ID. Usually an integer from 0..N.
 *
 * @return Filled out getBreakpoint_t with the following fields:
 * @param vaddr[out] Address that the breakpoint is set to trigger on.
 * @param type[out] Type of operation that will trigger the breakpoint.
 * @param size[out] operand size that will trigger the breakpoint.
 * @param rw[out] Access type (read/write) that will trigger the breakpoint.
 * @param uds If NULL, this function call will read directly from the hardware
 *            registers.
 *            If non-NULL, 'uds' is assumed to be a pointer to a debug register
 *            context-saving memory block, and this function will read from that
 *            context-saving memory block instead.
 * @param is_enabled Bool stating whether or not the breakpoint is enabled.
 */
typedef struct getBreakpointRet {
    word_t vaddr, type, size, rw;
    bool_t is_enabled;
} getBreakpoint_t;

getBreakpoint_t getBreakpoint(tcb_t *t, uint16_t bp_num);

/** Clears a breakpoint's configuration and disables it.
 * @param bp_num Hardware breakpoint ID. Usually an integer from 0..N.
 * @param uds If NULL, this function call will write directly to the hardware
 *            registers.
 *            If non-NULL, 'uds' is assumed to be a pointer to a debug register
 *            context-saving memory block, and this function will write to that
 *            context-saving memory block instead.
 */
void unsetBreakpoint(tcb_t *t, uint16_t bp_num);

bool_t configureSingleStepping(tcb_t *t,
                               uint16_t bp_num,
                               word_t n_instr,
                               bool_t is_reply);

static inline bool_t singleStepFaultCounterReady(tcb_t *t)
{
    /* For a single-step exception, the user may have specified a certain
     * number of instructions to skip over before the next stop-point, so
     * we need to decrement the counter.
     *
     * We will check the counter's value when deciding whether or not to
     * actually send a fault message to userspace.
     */
    if (t->tcbArch.tcbContext.breakpointState.n_instructions > 0) {
        t->tcbArch.tcbContext.breakpointState.n_instructions--;
    }
    return t->tcbArch.tcbContext.breakpointState.n_instructions == 0;
}
# 14 "/home/b/test/sel4test/kernel/src/api/faults.c" 2





/* consistency with libsel4 */
_Static_assert(lookup_fault_invalid_root + 1 == seL4_InvalidRoot, "InvalidRoot");
_Static_assert(lookup_fault_missing_capability + 1 == seL4_MissingCapability, "MissingCapability");
_Static_assert(lookup_fault_depth_mismatch + 1 == seL4_DepthMismatch, "DepthMismatch");
_Static_assert(lookup_fault_guard_mismatch + 1 == seL4_GuardMismatch, "GuardMismatch");
_Static_assert((word_t) n_syscallMessage == seL4_UnknownSyscall_Syscall, "seL4_UnknownSyscall_Syscall");
_Static_assert((word_t) n_exceptionMessage == seL4_UserException_Number, "seL4_UserException_Number");
_Static_assert((word_t) n_exceptionMessage + 1 == seL4_UserException_Code, "seL4_UserException_Code");

static inline unsigned int
setMRs_lookup_failure(tcb_t *receiver, word_t *receiveIPCBuffer,
                      lookup_fault_t luf, unsigned int offset)
{
    word_t lufType = lookup_fault_get_lufType(luf);
    word_t i;

    i = setMR(receiver, receiveIPCBuffer, offset, lufType + 1);

    /* check constants match libsel4 */
    if (offset == seL4_CapFault_LookupFailureType) {
        do { if (!(offset + 1 == seL4_CapFault_BitsLeft)) { _assert_fail("offset + 1 == seL4_CapFault_BitsLeft", "/home/b/test/sel4test/kernel/src/api/faults.c", 39, __func__); } } while(0);
        do { if (!(offset + 2 == seL4_CapFault_DepthMismatch_BitsFound)) { _assert_fail("offset + 2 == seL4_CapFault_DepthMismatch_BitsFound", "/home/b/test/sel4test/kernel/src/api/faults.c", 40, __func__); } } while(0);
        do { if (!(offset + 2 == seL4_CapFault_GuardMismatch_GuardFound)) { _assert_fail("offset + 2 == seL4_CapFault_GuardMismatch_GuardFound", "/home/b/test/sel4test/kernel/src/api/faults.c", 41, __func__); } } while(0);
        do { if (!(offset + 3 == seL4_CapFault_GuardMismatch_BitsFound)) { _assert_fail("offset + 3 == seL4_CapFault_GuardMismatch_BitsFound", "/home/b/test/sel4test/kernel/src/api/faults.c", 42, __func__); } } while(0);
    } else {
        do { if (!(offset == 1)) { _assert_fail("offset == 1", "/home/b/test/sel4test/kernel/src/api/faults.c", 44, __func__); } } while(0);
    }

    switch (lufType) {
    case lookup_fault_invalid_root:
        return i;

    case lookup_fault_missing_capability:
        return setMR(receiver, receiveIPCBuffer, offset + 1,
                     lookup_fault_missing_capability_get_bitsLeft(luf));

    case lookup_fault_depth_mismatch:
        setMR(receiver, receiveIPCBuffer, offset + 1,
              lookup_fault_depth_mismatch_get_bitsLeft(luf));
        return setMR(receiver, receiveIPCBuffer, offset + 2,
                     lookup_fault_depth_mismatch_get_bitsFound(luf));

    case lookup_fault_guard_mismatch:
        setMR(receiver, receiveIPCBuffer, offset + 1,
              lookup_fault_guard_mismatch_get_bitsLeft(luf));
        setMR(receiver, receiveIPCBuffer, offset + 2,
              lookup_fault_guard_mismatch_get_guardFound(luf));
        return setMR(receiver, receiveIPCBuffer, offset + 3,
                     lookup_fault_guard_mismatch_get_bitsFound(luf));

    default:
        _fail("Invalid lookup failure", "/home/b/test/sel4test/kernel/src/api/faults.c", 70, __func__);
    }
}

static inline void copyMRsFaultReply(tcb_t *sender, tcb_t *receiver, MessageID_t id, word_t length)
{
    word_t i;
    bool_t archInfo;

    archInfo = Arch_getSanitiseRegisterInfo(receiver);

    for (i = 0; i < (((length)<(n_msgRegisters))?(length):(n_msgRegisters)); i++) {
        register_t r = fault_messages[id][i];
        word_t v = getRegister(sender, msgRegisters[i]);
        setRegister(receiver, r, sanitiseRegister(r, v, archInfo));
    }

    if (i < length) {
        word_t *sendBuf = lookupIPCBuffer(false, sender);
        if (sendBuf) {
            for (; i < length; i++) {
                register_t r = fault_messages[id][i];
                word_t v = sendBuf[i + 1];
                setRegister(receiver, r, sanitiseRegister(r, v, archInfo));
            }
        }
    }
}

static inline void copyMRsFault(tcb_t *sender, tcb_t *receiver, MessageID_t id,
                                word_t length, word_t *receiveIPCBuffer)
{
    word_t i;
    for (i = 0; i < (((length)<(n_msgRegisters))?(length):(n_msgRegisters)); i++) {
        setRegister(receiver, msgRegisters[i], getRegister(sender, fault_messages[id][i]));
    }

    if (receiveIPCBuffer) {
        for (; i < length; i++) {
            receiveIPCBuffer[i + 1] = getRegister(sender, fault_messages[id][i]);
        }
    }
}

bool_t handleFaultReply(tcb_t *receiver, tcb_t *sender)
{
    /* These lookups are moved inward from doReplyTransfer */
    seL4_MessageInfo_t tag = messageInfoFromWord(getRegister(sender, msgInfoRegister));
    word_t label = seL4_MessageInfo_get_label(tag);
    word_t length = seL4_MessageInfo_get_length(tag);
    seL4_Fault_t fault = receiver->tcbFault;

    switch (seL4_Fault_get_seL4_FaultType(fault)) {
    case seL4_Fault_CapFault:
        return true;

    case seL4_Fault_UnknownSyscall:
        copyMRsFaultReply(sender, receiver, MessageID_Syscall, (((length)<(n_syscallMessage))?(length):(n_syscallMessage)));
        return (label == 0);

    case seL4_Fault_UserException:
        copyMRsFaultReply(sender, receiver, MessageID_Exception, (((length)<(n_exceptionMessage))?(length):(n_exceptionMessage)));
        return (label == 0);







    case seL4_Fault_DebugException: {
        word_t n_instrs;

        if (seL4_Fault_DebugException_get_exceptionReason(fault) != seL4_SingleStep) {
            /* Only single-step replies are required to set message registers.
             */
            return (label == 0);
        }

        if (length < (1)) {
            /* A single-step reply doesn't mean much if it isn't composed of the bp
             * number and number of instructions to skip. But even if both aren't
             * set, we can still allow the thread to continue because replying
             * should uniformly resume thread execution, based on the general seL4
             * API model.
             *
             * If it was single-step, but no reply registers were set, just
             * default to skipping 1 and continuing.
             *
             * On x86, bp_num actually doesn't matter for single-stepping
             * because single-stepping doesn't use a hardware register -- it
             * uses EFLAGS.TF.
             */
            n_instrs = 1;
        } else {
            /* If the reply had all expected registers set, proceed as normal */
            n_instrs = getRegister(sender, msgRegisters[0]);
        }

        syscall_error_t res;

        res = Arch_decodeConfigureSingleStepping(receiver, 0, n_instrs, true);
        if (res.type != seL4_NoError) {
            return false;
        };

        configureSingleStepping(receiver, 0, n_instrs, true);

        /* Replying will always resume the thread: the only variant behaviour
         * is whether or not the thread will be resumed with stepping still
         * enabled.
         */
        return (label == 0);
    }


    default:
        return Arch_handleFaultReply(receiver, sender, seL4_Fault_get_seL4_FaultType(fault));
    }
}

word_t setMRs_fault(tcb_t *sender, tcb_t *receiver, word_t *receiveIPCBuffer)
{
    switch (seL4_Fault_get_seL4_FaultType(sender->tcbFault)) {
    case seL4_Fault_CapFault:
        setMR(receiver, receiveIPCBuffer, seL4_CapFault_IP, getRestartPC(sender));
        setMR(receiver, receiveIPCBuffer, seL4_CapFault_Addr,
              seL4_Fault_CapFault_get_address(sender->tcbFault));
        setMR(receiver, receiveIPCBuffer, seL4_CapFault_InRecvPhase,
              seL4_Fault_CapFault_get_inReceivePhase(sender->tcbFault));
        return setMRs_lookup_failure(receiver, receiveIPCBuffer,
                                     sender->tcbLookupFailure, seL4_CapFault_LookupFailureType);

    case seL4_Fault_UnknownSyscall: {
        copyMRsFault(sender, receiver, MessageID_Syscall, n_syscallMessage,
                     receiveIPCBuffer);

        return setMR(receiver, receiveIPCBuffer, n_syscallMessage,
                     seL4_Fault_UnknownSyscall_get_syscallNumber(sender->tcbFault));
    }

    case seL4_Fault_UserException: {
        copyMRsFault(sender, receiver, MessageID_Exception,
                     n_exceptionMessage, receiveIPCBuffer);
        setMR(receiver, receiveIPCBuffer, n_exceptionMessage,
              seL4_Fault_UserException_get_number(sender->tcbFault));
        return setMR(receiver, receiveIPCBuffer, n_exceptionMessage + 1u,
                     seL4_Fault_UserException_get_code(sender->tcbFault));
    }
# 235 "/home/b/test/sel4test/kernel/src/api/faults.c"
    case seL4_Fault_DebugException: {
        word_t reason = seL4_Fault_DebugException_get_exceptionReason(sender->tcbFault);

        setMR(receiver, receiveIPCBuffer,
              seL4_DebugException_FaultIP, getRestartPC(sender));
        unsigned int ret = setMR(receiver, receiveIPCBuffer,
                                 seL4_DebugException_ExceptionReason, reason);

        if (reason != seL4_SingleStep && reason != seL4_SoftwareBreakRequest) {
            ret = setMR(receiver, receiveIPCBuffer,
                        seL4_DebugException_TriggerAddress,
                        seL4_Fault_DebugException_get_breakpointAddress(sender->tcbFault));

            /* Breakpoint messages also set a "breakpoint number" register. */
            ret = setMR(receiver, receiveIPCBuffer,
                        seL4_DebugException_BreakpointNumber,
                        seL4_Fault_DebugException_get_breakpointNumber(sender->tcbFault));
        }
        return ret;
    }


    default:
        return Arch_setMRs_fault(sender, receiver, receiveIPCBuffer,
                                 seL4_Fault_get_seL4_FaultType(sender->tcbFault));
    }
}
# 1 "/home/b/test/sel4test/kernel/src/api/syscall.c"
/*
 * Copyright 2014, General Dynamics C4 Systems
 *
 * SPDX-License-Identifier: GPL-2.0-only
 */



# 1 "/home/b/test/sel4test/kernel/include/benchmark/benchmark.h" 1
/*
 * Copyright 2014, General Dynamics C4 Systems
 *
 * SPDX-License-Identifier: GPL-2.0-only
 */

       






# 1 "/home/b/test/sel4test/kernel/libsel4/include/sel4/benchmark_tracepoints_types.h" 1
/*
 * Copyright 2020, Data61, CSIRO (ABN 41 687 119 230)
 *
 * SPDX-License-Identifier: BSD-2-Clause
 */

       
# 15 "/home/b/test/sel4test/kernel/include/benchmark/benchmark.h" 2
# 10 "/home/b/test/sel4test/kernel/src/api/syscall.c" 2


# 1 "/home/b/test/sel4test/kernel/include/benchmark/benchmark_utilisation.h" 1
/*
 * Copyright 2016, General Dynamics C4 Systems
 *
 * SPDX-License-Identifier: GPL-2.0-only
 */

       



# 1 "/home/b/test/sel4test/kernel/libsel4/include/sel4/benchmark_utilisation_types.h" 1
/*
 * Copyright 2020, Data61, CSIRO (ABN 41 687 119 230)
 *
 * SPDX-License-Identifier: BSD-2-Clause
 */

       
# 12 "/home/b/test/sel4test/kernel/include/benchmark/benchmark_utilisation.h" 2
# 13 "/home/b/test/sel4test/kernel/src/api/syscall.c" 2




# 1 "/home/b/test/sel4test/kernel/include/kernel/faulthandler.h" 1
/*
 * Copyright 2014, General Dynamics C4 Systems
 *
 * SPDX-License-Identifier: GPL-2.0-only
 */

       

# 1 "/home/b/test/sel4test/kernel/include/object.h" 1
/*
 * Copyright 2014, General Dynamics C4 Systems
 *
 * SPDX-License-Identifier: GPL-2.0-only
 */

       


# 1 "/home/b/test/sel4test/kernel/include/object/objecttype.h" 1
/*
 * Copyright 2014, General Dynamics C4 Systems
 *
 * SPDX-License-Identifier: GPL-2.0-only
 */

       






# 1 "/home/b/test/sel4test/kernel/include/object/cap.h" 1
/*
 * Copyright 2020, Data61, CSIRO (ABN 41 687 119 230)
 *
 * SPDX-License-Identifier: GPL-2.0-only
 */

       

struct deriveCap_ret {
    exception_t status;
    cap_t cap;
};
typedef struct deriveCap_ret deriveCap_ret_t;

struct finaliseCap_ret {
    cap_t remainder;
    /* potential cap holding information for cleanup that needs to be happen *after* a
     * cap has been deleted. Where deleted here means been removed from the slot in emptySlot */
    cap_t cleanupInfo;
};
typedef struct finaliseCap_ret finaliseCap_ret_t;
# 15 "/home/b/test/sel4test/kernel/include/object/objecttype.h" 2
# 1 "/home/b/test/sel4test/kernel/include/arch/arm/arch/object/objecttype.h" 1
/*
 * Copyright 2014, General Dynamics C4 Systems
 *
 * SPDX-License-Identifier: GPL-2.0-only
 */

       







deriveCap_ret_t Arch_deriveCap(cte_t *slot, cap_t cap);
cap_t __attribute__((__const__)) Arch_updateCapData(bool_t preserve, word_t data, cap_t cap);
cap_t __attribute__((__const__)) Arch_maskCapRights(seL4_CapRights_t cap_rights_mask, cap_t cap);
finaliseCap_ret_t Arch_finaliseCap(cap_t cap, bool_t final);
bool_t __attribute__((__const__)) Arch_sameRegionAs(cap_t cap_a, cap_t cap_b);
bool_t __attribute__((__const__)) Arch_sameObjectAs(cap_t cap_a, cap_t cap_b);
bool_t __attribute__((__const__)) Arch_isFrameType(word_t type);
cap_t Arch_createObject(object_t t, void *regionBase, word_t userSize, bool_t deviceMemory);
exception_t Arch_decodeInvocation(word_t invLabel, word_t length,
                                  cptr_t cptr, cte_t *slot, cap_t cap,
                                  bool_t call, word_t *buffer);
void Arch_prepareThreadDelete(tcb_t *thread);
word_t Arch_getObjectSize(word_t t);

static inline void Arch_postCapDeletion(cap_t cap)
{
}

/**
 * Return true if the given arch cap can be a descendant of an IRQControlCap.
 */
static inline __attribute__((__const__)) bool_t Arch_isIRQControlDescendant(cap_t cap)
{

    return cap_get_capType(cap) == cap_sgi_signal_cap;



}

/**
 * isMDBParentOf for architecture caps. For most arch caps, this just returns
 * true, but if there are badged versions of arch caps, this functions should
 * perform the necessary checks.
 *
 * Called by the generic isMDBParentOf after the revocable bit has been checked
 * and sameRegionAs has been established. This means we can assume both as true
 * inside Arch_isMDBParentOf.
 */
static inline __attribute__((__const__)) bool_t Arch_isMDBParentOf(cap_t cap_a, cap_t cap_b, bool_t firstBadged)
{
    switch (cap_get_capType(cap_a)) {

    case cap_sgi_signal_cap:
        return !firstBadged;
        break;
# 77 "/home/b/test/sel4test/kernel/include/arch/arm/arch/object/objecttype.h"
    default:
        return true;
    }
}
# 16 "/home/b/test/sel4test/kernel/include/object/objecttype.h" 2
# 1 "/home/b/test/sel4test/kernel/include/object/interrupt.h" 1
/*
 * Copyright 2014, General Dynamics C4 Systems
 *
 * SPDX-License-Identifier: GPL-2.0-only
 */

       




# 1 "/home/b/test/sel4test/kernel/include/arch/arm/arch/object/interrupt.h" 1
/*
 * Copyright 2014, General Dynamics C4 Systems
 *
 * SPDX-License-Identifier: GPL-2.0-only
 */

       





# 1 "/home/b/test/sel4test/kernel/include/plat/default/plat/machine.h" 1
/*
 * Copyright 2020, Data61, CSIRO (ABN 41 687 119 230)
 *
 * SPDX-License-Identifier: GPL-2.0-only
 */
# 14 "/home/b/test/sel4test/kernel/include/arch/arm/arch/object/interrupt.h" 2

exception_t Arch_decodeIRQControlInvocation(word_t invLabel, word_t length,
                                            cte_t *srcSlot, word_t *buffer);

exception_t decodeSGISignalInvocation(word_t invLabel, word_t length,
                                      cap_t cap, word_t *buffer);

/* Handle a platform-reserved IRQ. */
static inline void handleReservedIRQ(irq_t irq)
{
# 52 "/home/b/test/sel4test/kernel/include/arch/arm/arch/object/interrupt.h"
    printf("Received unhandled reserved IRQ: 0x%lx\n", (irq));

}


static inline exception_t Arch_checkIRQ(word_t irq_w)
{
    if (irq_w > maxIRQ) {
        current_syscall_error.type = seL4_RangeError;
        current_syscall_error.rangeErrorMin = 0;
        current_syscall_error.rangeErrorMax = maxIRQ;
        do { printf("\033[0m" "\033[1m" "<<" "\033[0m" "\033[32m" "seL4(CPU %" "lu" ")" "\033[0m" "\033[1m" " [%s/%d T%p \"%s\" @%lx]: " "Rejecting request for IRQ %u. IRQ is out of range [0..%u]." ">>" "\033[0m" "\n", 0lu, __func__, 63, ksCurThread, ((debug_tcb_t *)(((cte_t *)((word_t)(ksCurThread)&~((1ul << (12)) - 1ul)))+(tcbCNodeEntries)))->tcbName, (word_t)getRestartPC(ksCurThread), (int)irq_w, maxIRQ); } while (0);
        return EXCEPTION_SYSCALL_ERROR;
    }
    return EXCEPTION_NONE;
}
# 13 "/home/b/test/sel4test/kernel/include/object/interrupt.h" 2
# 1 "/home/b/test/sel4test/kernel/include/plat/default/plat/machine.h" 1
/*
 * Copyright 2020, Data61, CSIRO (ABN 41 687 119 230)
 *
 * SPDX-License-Identifier: GPL-2.0-only
 */
# 14 "/home/b/test/sel4test/kernel/include/object/interrupt.h" 2

exception_t decodeIRQControlInvocation(word_t invLabel, word_t length,
                                       cte_t *srcSlot, word_t *buffer);
exception_t invokeIRQControl(irq_t irq, cte_t *handlerSlot, cte_t *controlSlot);
exception_t decodeIRQHandlerInvocation(word_t invLabel, irq_t irq);
void invokeIRQHandler_AckIRQ(irq_t irq);
void invokeIRQHandler_SetIRQHandler(irq_t irq, cap_t cap, cte_t *slot);
void invokeIRQHandler_ClearIRQHandler(irq_t irq);
void deletingIRQHandler(irq_t irq);
void deletedIRQHandler(irq_t irq);
void handleInterrupt(irq_t irq);
bool_t isIRQActive(irq_t irq);
void setIRQState(irq_state_t irqState, irq_t irq);
# 17 "/home/b/test/sel4test/kernel/include/object/objecttype.h" 2

deriveCap_ret_t deriveCap(cte_t *slot, cap_t cap);
finaliseCap_ret_t finaliseCap(cap_t cap, bool_t final, bool_t exposed);
bool_t __attribute__((__const__)) hasCancelSendRights(cap_t cap);
bool_t __attribute__((__const__)) sameRegionAs(cap_t cap_a, cap_t cap_b);
bool_t __attribute__((__const__)) sameObjectAs(cap_t cap_a, cap_t cap_b);
cap_t __attribute__((__const__)) updateCapData(bool_t preserve, word_t newData, cap_t cap);
cap_t __attribute__((__const__)) maskCapRights(seL4_CapRights_t seL4_CapRights, cap_t cap);
cap_t createObject(object_t t, void *regionBase, word_t, bool_t deviceMemory);
void createNewObjects(object_t t, cte_t *parent,
                      cte_t *destCNode, word_t destOffset, word_t destLength,
                      void *regionBase, word_t userSize, bool_t deviceMemory);
# 41 "/home/b/test/sel4test/kernel/include/object/objecttype.h"
exception_t decodeInvocation(word_t invLabel, word_t length,
                             cptr_t capIndex, cte_t *slot, cap_t cap,
                             bool_t block, bool_t call, word_t *buffer);
exception_t performInvocation_Endpoint(endpoint_t *ep, word_t badge,
                                       bool_t canGrant, bool_t canGrantReply,
                                       bool_t block, bool_t call);
exception_t performInvocation_Notification(notification_t *ntfn,
                                           word_t badge);
exception_t performInvocation_Reply(tcb_t *thread, cte_t *slot, bool_t canGrant);

word_t getObjectSize(word_t t, word_t userObjSize);

static inline void postCapDeletion(cap_t cap)
{
    if (cap_get_capType(cap) == cap_irq_handler_cap) {
        irq_t irq = (cap_irq_handler_cap_get_capIRQ(cap));
        deletedIRQHandler(irq);
    } else if (isArchCap(cap)) {
        Arch_postCapDeletion(cap);
    }
}

word_t __attribute__((__const__)) cap_get_capSizeBits(cap_t cap);
bool_t __attribute__((__const__)) cap_get_capIsPhysical(cap_t cap);
void *__attribute__((__const__)) cap_get_capPtr(cap_t cap);
bool_t __attribute__((__const__)) isCapRevocable(cap_t derivedCap, cap_t srcCap);
# 11 "/home/b/test/sel4test/kernel/include/object.h" 2
# 1 "/home/b/test/sel4test/kernel/include/object/notification.h" 1
/*
 * Copyright 2014, General Dynamics C4 Systems
 *
 * SPDX-License-Identifier: GPL-2.0-only
 */

       




void sendSignal(notification_t *ntfnPtr, word_t badge);
void receiveSignal(tcb_t *thread, cap_t cap, bool_t isBlocking);
void cancelAllSignals(notification_t *ntfnPtr);
void cancelSignal(tcb_t *threadPtr, notification_t *ntfnPtr);
void completeSignal(notification_t *ntfnPtr, tcb_t *tcb);
void unbindMaybeNotification(notification_t *ntfnPtr);
void unbindNotification(tcb_t *tcb);
void bindNotification(tcb_t *tcb, notification_t *ntfnPtr);
# 39 "/home/b/test/sel4test/kernel/include/object/notification.h"
static inline void ntfn_set_active(notification_t *ntfnPtr, word_t badge)
{
    notification_ptr_set_state(ntfnPtr, NtfnState_Active);
    notification_ptr_set_ntfnMsgIdentifier(ntfnPtr, badge);
}
# 12 "/home/b/test/sel4test/kernel/include/object.h" 2

# 1 "/home/b/test/sel4test/kernel/include/object/endpoint.h" 1
/*
 * Copyright 2014, General Dynamics C4 Systems
 *
 * SPDX-License-Identifier: GPL-2.0-only
 */

       




static inline tcb_queue_t __attribute__((__pure__)) ep_ptr_get_queue(endpoint_t *epptr)
{
    tcb_queue_t queue;

    queue.head = (tcb_t *)endpoint_ptr_get_epQueue_head(epptr);
    queue.end = (tcb_t *)endpoint_ptr_get_epQueue_tail(epptr);

    return queue;
}

static inline void ep_ptr_set_queue(endpoint_t *epptr, tcb_queue_t queue)
{
    endpoint_ptr_set_epQueue_head(epptr, (word_t)queue.head);
    endpoint_ptr_set_epQueue_tail(epptr, (word_t)queue.end);
}
# 35 "/home/b/test/sel4test/kernel/include/object/endpoint.h"
void sendIPC(bool_t blocking, bool_t do_call, word_t badge,
             bool_t canGrant, bool_t canGrantReply, tcb_t *thread,
             endpoint_t *epptr);
void receiveIPC(tcb_t *thread, cap_t cap, bool_t isBlocking);

void cancelIPC(tcb_t *tptr);
void cancelAllIPC(endpoint_t *epptr);
void cancelBadgedSends(endpoint_t *epptr, word_t badge);
void replyFromKernel_error(tcb_t *thread);
void replyFromKernel_success_empty(tcb_t *thread);
# 14 "/home/b/test/sel4test/kernel/include/object.h" 2



# 1 "/home/b/test/sel4test/kernel/include/object/untyped.h" 1
/*
 * Copyright 2014, General Dynamics C4 Systems
 *
 * SPDX-License-Identifier: GPL-2.0-only
 */

       
# 16 "/home/b/test/sel4test/kernel/include/object/untyped.h"
/* It is assumed that every untyped is within seL4_MinUntypedBits and seL4_MaxUntypedBits
 * (inclusive). This means that every untyped stored as seL4_MinUntypedBits
 * subtracted from its size before it is stored in capBlockSize, and
 * capFreeIndex counts in chunks of size 2^seL4_MinUntypedBits. The seL4_MaxUntypedBits
 * is the minimal untyped that can be stored when considering both how
 * many bits of capBlockSize there are, and the largest offset that can
 * be stored in capFreeIndex */







exception_t decodeUntypedInvocation(word_t invLabel, word_t length,
                                    cte_t *slot, cap_t cap,
                                    bool_t call, word_t *buffer);
exception_t invokeUntyped_Retype(cte_t *srcSlot, bool_t reset,
                                 void *retypeBase, object_t newType, word_t userSize,
                                 cte_t *destCNode, word_t destOffset, word_t destLength,
                                 bool_t deviceMemory);
# 18 "/home/b/test/sel4test/kernel/include/object.h" 2
# 10 "/home/b/test/sel4test/kernel/include/kernel/faulthandler.h" 2
# 22 "/home/b/test/sel4test/kernel/include/kernel/faulthandler.h"
exception_t sendFaultIPC(tcb_t *tptr);
void handleDoubleFault(tcb_t *tptr, seL4_Fault_t ex1);

void handleFault(tcb_t *tptr);
# 18 "/home/b/test/sel4test/kernel/src/api/syscall.c" 2






# 1 "/home/b/test/sel4test/kernel/include/string.h" 1
/*
 * Copyright 2014, General Dynamics C4 Systems
 *
 * SPDX-License-Identifier: GPL-2.0-only
 */

       



word_t strnlen(const char *s, word_t maxlen);
word_t strlcpy(char *dest, const char *src, word_t size);
word_t strlcat(char *dest, const char *src, word_t size);
# 25 "/home/b/test/sel4test/kernel/src/api/syscall.c" 2
# 1 "/home/b/test/sel4test/kernel/include/kernel/traps.h" 1
/*
 * Copyright 2016, General Dynamics C4 Systems
 *
 * SPDX-License-Identifier: GPL-2.0-only
 */

       



# 1 "/home/b/test/sel4test/kernel/include/arch/arm/arch/kernel/traps.h" 1
/*
 * Copyright 2016, General Dynamics C4 Systems
 *
 * SPDX-License-Identifier: GPL-2.0-only
 */

       





static inline void arch_c_entry_hook(void)
{
    arm_save_thread_id(ksCurThread);
}

static inline void arch_c_exit_hook(void)
{
    arm_load_thread_id(ksCurThread);
}

void __attribute__((externally_visible)) __attribute__((__noreturn__)) restore_user_context(void);

void c_handle_syscall(word_t cptr, word_t msgInfo, syscall_t syscall)
__attribute__((externally_visible)) __attribute__((__section__(".vectors.text")));


void c_handle_fastpath_call(word_t cptr, word_t msgInfo)
__attribute__((externally_visible)) __attribute__((__section__(".vectors.text")));

void c_handle_fastpath_signal(word_t cptr, word_t msgInfo)
__attribute__((externally_visible)) __attribute__((__section__(".vectors.text")));




void c_handle_fastpath_reply_recv(word_t cptr, word_t msgInfo)

__attribute__((externally_visible)) __attribute__((__section__(".vectors.text")));


void c_handle_interrupt(void)
__attribute__((externally_visible)) __attribute__((__section__(".vectors.text")));

void c_handle_undefined_instruction(void)
__attribute__((externally_visible)) __attribute__((__section__(".vectors.text")));

void c_handle_data_fault(void)
__attribute__((externally_visible)) __attribute__((__section__(".vectors.text")));

void c_handle_instruction_fault(void)
__attribute__((externally_visible)) __attribute__((__section__(".vectors.text")));
# 12 "/home/b/test/sel4test/kernel/include/kernel/traps.h" 2
# 1 "/home/b/test/sel4test/kernel/include/smp/lock.h" 1
/*
 * Copyright 2020, Data61, CSIRO (ABN 41 687 119 230)
 *
 * SPDX-License-Identifier: GPL-2.0-only
 */

       






# 1 "/home/b/test/sel4test/kernel/include/smp/ipi.h" 1
/*
 * Copyright 2020, Data61, CSIRO (ABN 41 687 119 230)
 *
 * SPDX-License-Identifier: GPL-2.0-only
 */

       



# 1 "/home/b/test/sel4test/kernel/include/plat/default/plat/machine.h" 1
/*
 * Copyright 2020, Data61, CSIRO (ABN 41 687 119 230)
 *
 * SPDX-License-Identifier: GPL-2.0-only
 */
# 12 "/home/b/test/sel4test/kernel/include/smp/ipi.h" 2
# 1 "/home/b/test/sel4test/kernel/include/arch/arm/arch/smp/ipi.h" 1
/*
 * Copyright 2020, Data61, CSIRO (ABN 41 687 119 230)
 *
 * SPDX-License-Identifier: GPL-2.0-only
 */

       
# 13 "/home/b/test/sel4test/kernel/include/smp/ipi.h" 2
# 15 "/home/b/test/sel4test/kernel/include/smp/lock.h" 2
# 13 "/home/b/test/sel4test/kernel/include/kernel/traps.h" 2

/* This C function should be the first thing called from C after entry from
 * assembly. It provides a single place to do any entry work that is not
 * done in assembly for various reasons */
static inline void c_entry_hook(void)
{
    arch_c_entry_hook();



}

/* This C function should be the last thing called from C before exiting
 * the kernel (be it to assembly or returning to user space). It provides
 * a place to provide any additional instrumentation or functionality
 * in C before leaving the kernel */
static inline void c_exit_hook(void)
{
# 44 "/home/b/test/sel4test/kernel/include/kernel/traps.h"
    arch_c_exit_hook();
}
# 26 "/home/b/test/sel4test/kernel/src/api/syscall.c" 2





# 1 "/home/b/test/sel4test/kernel/include/arch/arm/arch/machine/capdl.h" 1
/*
 * Copyright 2014, General Dynamics C4 Systems
 *
 * SPDX-License-Identifier: GPL-2.0-only
 */

       


# 1 "/home/b/test/sel4test/kernel/include/machine/capdl.h" 1
/*
 * Copyright 2014, General Dynamics C4 Systems
 *
 * SPDX-License-Identifier: GPL-2.0-only
 */

       



/* helpers */
void add_to_seen(cap_t c);
void reset_seen_list(void);
bool_t seen(cap_t c);
bool_t same_cap(cap_t a, cap_t b);
bool_t root_or_idle_tcb(tcb_t *tcb);
word_t get_tcb_sp(tcb_t *tcb);

/* common */
void debug_capDL(void);





void obj_tcb_print_cnodes(cap_t cnode, tcb_t *tcb);
void print_caps(void);
void print_objects(void);
void print_cap(cap_t cap);
void print_object(cap_t cap);

void obj_tcb_print_attrs(tcb_t *tcb);
void obj_sc_print_attrs(cap_t sc);
void obj_cnode_print_attrs(cap_t cnode);
void obj_ut_print_attrs(cte_t *slot, tcb_t *tcb);

void obj_tcb_print_slots(tcb_t *tcb);
void obj_cnode_print_slots(tcb_t *tcb);
void obj_irq_print_slots(cap_t irq_cap);
void obj_irq_print_maps(void);

void cap_ep_print_attrs(cap_t ep);
void cap_ntfn_print_attrs(cap_t ntfn);
void cap_cnode_print_attrs(cap_t cnode);

/* arch specific functions */
void print_ipc_buffer_slot(tcb_t *tcb);
/* TBD: currently the capDL extractor declaring an object for every entry in the vspace.
 * However, frames can be mapped into multiple locations but sould only be declared once.
 */
void obj_vtable_print_slots(tcb_t *tcb);

void print_cap_arch(cap_t cap);
void print_object_arch(cap_t cap);
void obj_tcb_print_vtable(tcb_t *tcb);
# 11 "/home/b/test/sel4test/kernel/include/arch/arm/arch/machine/capdl.h" 2
# 21 "/home/b/test/sel4test/kernel/include/arch/arm/arch/machine/capdl.h"
static inline void obj_asidpool_print_attrs(cap_t asid_cap)
{
    printf("(asid_high: 0x%lx)\n", (long unsigned int)((cap_asid_pool_cap_get_capASIDBase(asid_cap) >> asidLowBits) & ((1ul << (asidHighBits)) - 1ul)));
}
# 32 "/home/b/test/sel4test/kernel/src/api/syscall.c" 2





/* The haskell function 'handleEvent' is split into 'handleXXX' variants
 * for each event causing a kernel entry */

exception_t handleInterruptEntry(void)
{
    irq_t irq;
# 51 "/home/b/test/sel4test/kernel/src/api/syscall.c"
    irq = getActiveIRQ();
    if ((irq) != (irqInvalid)) {
        handleInterrupt(irq);
    } else {

        do { printf("\033[0m" "\033[1m" "<<" "\033[0m" "\033[32m" "seL4(CPU %" "lu" ")" "\033[0m" "\033[1m" " [%s/%d T%p \"%s\" @%lx]: " "Spurious interrupt!" ">>" "\033[0m" "\n", 0lu, __func__, 56, ksCurThread, ((debug_tcb_t *)(((cte_t *)((word_t)(ksCurThread)&~((1ul << (12)) - 1ul)))+(tcbCNodeEntries)))->tcbName, (word_t)getRestartPC(ksCurThread)); } while (0);

        handleSpuriousIRQ();
    }




        schedule();
        activateThread();




    return EXCEPTION_NONE;
}

exception_t handleUnknownSyscall(word_t w)
{

    if (w == SysDebugPutChar) {
        kernel_putchar(getRegister(ksCurThread, capRegister));
        return EXCEPTION_NONE;
    }
    if (w == SysDebugDumpScheduler) {

        debug_dumpScheduler();

        return EXCEPTION_NONE;
    }


    if (w == SysDebugHalt) {
        tcb_t *__attribute__((unused)) tptr = ksCurThread;
        printf("Debug halt syscall from user thread %p \"%s\"\n", tptr, ((debug_tcb_t *)(((cte_t *)((word_t)(tptr)&~((1ul << (12)) - 1ul)))+(tcbCNodeEntries)))->tcbName);
        halt();
    }
    if (w == SysDebugSnapshot) {
        tcb_t *__attribute__((unused)) tptr = ksCurThread;
        printf("Debug snapshot syscall from user thread %p \"%s\"\n",
               tptr, ((debug_tcb_t *)(((cte_t *)((word_t)(tptr)&~((1ul << (12)) - 1ul)))+(tcbCNodeEntries)))->tcbName);
        debug_capDL();
        return EXCEPTION_NONE;
    }
    if (w == SysDebugCapIdentify) {
        word_t cptr = getRegister(ksCurThread, capRegister);
        lookupCapAndSlot_ret_t lu_ret = lookupCapAndSlot(ksCurThread, cptr);
        word_t cap_type = cap_get_capType(lu_ret.cap);
        setRegister(ksCurThread, capRegister, cap_type);
        return EXCEPTION_NONE;
    }

    if (w == SysDebugNameThread) {
        /* This is a syscall meant to aid debugging, so if anything goes wrong
         * then assume the system is completely misconfigured and halt */
        const char *name;
        word_t len;
        word_t cptr = getRegister(ksCurThread, capRegister);
        lookupCapAndSlot_ret_t lu_ret = lookupCapAndSlot(ksCurThread, cptr);
        /* ensure we got a TCB cap */
        word_t cap_type = cap_get_capType(lu_ret.cap);
        if (cap_type != cap_thread_cap) {
            do { printf("\033[0m" "\033[1m" "<<" "\033[0m" "\033[32m" "seL4(CPU %" "lu" ")" "\033[0m" "\033[1m" " [%s/%d T%p \"%s\" @%lx]: " "SysDebugNameThread: cap is not a TCB, halting" ">>" "\033[0m" "\n", 0lu, __func__, 118, ksCurThread, ((debug_tcb_t *)(((cte_t *)((word_t)(ksCurThread)&~((1ul << (12)) - 1ul)))+(tcbCNodeEntries)))->tcbName, (word_t)getRestartPC(ksCurThread)); } while (0);
            halt();
        }
        /* Add 1 to the IPC buffer to skip the message info word */
        name = (const char *)(lookupIPCBuffer(true, ksCurThread) + 1);
        if (!name) {
            do { printf("\033[0m" "\033[1m" "<<" "\033[0m" "\033[32m" "seL4(CPU %" "lu" ")" "\033[0m" "\033[1m" " [%s/%d T%p \"%s\" @%lx]: " "SysDebugNameThread: Failed to lookup IPC buffer, halting" ">>" "\033[0m" "\n", 0lu, __func__, 124, ksCurThread, ((debug_tcb_t *)(((cte_t *)((word_t)(ksCurThread)&~((1ul << (12)) - 1ul)))+(tcbCNodeEntries)))->tcbName, (word_t)getRestartPC(ksCurThread)); } while (0);
            halt();
        }
        /* ensure the name isn't too long */
        len = strnlen(name, seL4_MsgMaxLength * sizeof(word_t));
        if (len == seL4_MsgMaxLength * sizeof(word_t)) {
            do { printf("\033[0m" "\033[1m" "<<" "\033[0m" "\033[32m" "seL4(CPU %" "lu" ")" "\033[0m" "\033[1m" " [%s/%d T%p \"%s\" @%lx]: " "SysDebugNameThread: Name too long, halting" ">>" "\033[0m" "\n", 0lu, __func__, 130, ksCurThread, ((debug_tcb_t *)(((cte_t *)((word_t)(ksCurThread)&~((1ul << (12)) - 1ul)))+(tcbCNodeEntries)))->tcbName, (word_t)getRestartPC(ksCurThread)); } while (0);
            halt();
        }
        setThreadName(((tcb_t *)(cap_thread_cap_get_capTCBPtr(lu_ret.cap))), name);
        return EXCEPTION_NONE;
    }
# 210 "/home/b/test/sel4test/kernel/src/api/syscall.c"
    { { current_fault = seL4_Fault_UnknownSyscall_new(w); handleFault(ksCurThread); } }
# 227 "/home/b/test/sel4test/kernel/src/api/syscall.c"
    schedule();
    activateThread();

    return EXCEPTION_NONE;
}

exception_t handleUserLevelFault(word_t w_a, word_t w_b)
{
    { { current_fault = seL4_Fault_UserException_new(w_a, w_b); handleFault(ksCurThread); } }



    schedule();
    activateThread();

    return EXCEPTION_NONE;
}

exception_t handleVMFaultEvent(vm_fault_type_t vm_faultType)
{
    { { exception_t status = handleVMFault(ksCurThread, vm_faultType); if (status != EXCEPTION_NONE) { handleFault(ksCurThread); } } }
# 256 "/home/b/test/sel4test/kernel/src/api/syscall.c"
    schedule();
    activateThread();

    return EXCEPTION_NONE;
}




static exception_t handleInvocation(bool_t isCall, bool_t isBlocking)

{
    seL4_MessageInfo_t info;
    lookupCapAndSlot_ret_t lu_ret;
    word_t *buffer;
    exception_t status;
    word_t length;
    tcb_t *thread;

    thread = ksCurThread;

    info = messageInfoFromWord(getRegister(thread, msgInfoRegister));

    cptr_t cptr = getRegister(thread, capRegister);


    /* faulting section */
    lu_ret = lookupCapAndSlot(thread, cptr);

    if (__builtin_expect(!!(lu_ret.status != EXCEPTION_NONE), 0)) {
        do { printf("\033[0m" "\033[1m" "<<" "\033[0m" "\033[32m" "seL4(CPU %" "lu" ")" "\033[0m" "\033[1m" " [%s/%d T%p \"%s\" @%lx]: " "Invocation of invalid cap #%lu." ">>" "\033[0m" "\n", 0lu, __func__, 286, ksCurThread, ((debug_tcb_t *)(((cte_t *)((word_t)(ksCurThread)&~((1ul << (12)) - 1ul)))+(tcbCNodeEntries)))->tcbName, (word_t)getRestartPC(ksCurThread), cptr); } while (0);
        current_fault = seL4_Fault_CapFault_new(cptr, false);

        if (isBlocking) {
            handleFault(thread);
        }

        return EXCEPTION_NONE;
    }

    buffer = lookupIPCBuffer(false, thread);

    status = lookupExtraCaps(thread, buffer, info);

    if (__builtin_expect(!!(status != EXCEPTION_NONE), 0)) {
        do { printf("\033[0m" "\033[1m" "<<" "\033[0m" "\033[32m" "seL4(CPU %" "lu" ")" "\033[0m" "\033[1m" " [%s/%d T%p \"%s\" @%lx]: " "Lookup of extra caps failed." ">>" "\033[0m" "\n", 0lu, __func__, 301, ksCurThread, ((debug_tcb_t *)(((cte_t *)((word_t)(ksCurThread)&~((1ul << (12)) - 1ul)))+(tcbCNodeEntries)))->tcbName, (word_t)getRestartPC(ksCurThread)); } while (0);
        if (isBlocking) {
            handleFault(thread);
        }
        return EXCEPTION_NONE;
    }

    /* Syscall error/Preemptible section */
    length = seL4_MessageInfo_get_length(info);
    if (__builtin_expect(!!(length > n_msgRegisters && !buffer), 0)) {
        length = n_msgRegisters;
    }






    status = decodeInvocation(seL4_MessageInfo_get_label(info), length,
                              cptr, lu_ret.slot, lu_ret.cap,
                              isBlocking, isCall, buffer);


    if (__builtin_expect(!!(status == EXCEPTION_PREEMPTED), 0)) {
        return status;
    }

    if (__builtin_expect(!!(status == EXCEPTION_SYSCALL_ERROR), 0)) {
        if (isCall) {
            replyFromKernel_error(thread);
        }
        return EXCEPTION_NONE;
    }

    if (__builtin_expect(!!(thread_state_get_tsType(thread->tcbState) == ThreadState_Restart), 0)
                                                                             ) {
        if (isCall) {
            replyFromKernel_success_empty(thread);
        }
        setThreadState(thread, ThreadState_Running);
    }

    return EXCEPTION_NONE;
}
# 369 "/home/b/test/sel4test/kernel/src/api/syscall.c"
static void handleReply(void)
{
    cte_t *callerSlot;
    cap_t callerCap;

    callerSlot = (((cte_t *)((word_t)(ksCurThread)&~((1ul << (12)) - 1ul)))+(tcbCaller));
    callerCap = callerSlot->cap;

    switch (cap_get_capType(callerCap)) {
    case cap_reply_cap: {
        tcb_t *caller;

        if (cap_reply_cap_get_capReplyMaster(callerCap)) {
            break;
        }
        caller = ((tcb_t *)(cap_reply_cap_get_capTCBPtr(callerCap)));
        /* Haskell error:
         * "handleReply: caller must not be the current thread" */
        do { if (!(caller != ksCurThread)) { _assert_fail("caller != NODE_STATE(ksCurThread)", "/home/b/test/sel4test/kernel/src/api/syscall.c", 387, __func__); } } while(0);
        doReplyTransfer(ksCurThread, caller, callerSlot,
                        cap_reply_cap_get_capReplyCanGrant(callerCap));
        return;
    }

    case cap_null_cap:
        /* Do nothing when no caller is pending */
        return;

    default:
        break;
    }

    _fail("handleReply: invalid caller cap", "/home/b/test/sel4test/kernel/src/api/syscall.c", 401, __func__);
}





static void handleRecv(bool_t isBlocking)

{
    word_t epCPtr;
    lookupCap_ret_t lu_ret;

    epCPtr = getRegister(ksCurThread, capRegister);

    lu_ret = lookupCap(ksCurThread, epCPtr);

    if (__builtin_expect(!!(lu_ret.status != EXCEPTION_NONE), 0)) {
        /* current_lookup_fault has been set by lookupCap */
        current_fault = seL4_Fault_CapFault_new(epCPtr, true);
        handleFault(ksCurThread);
        return;
    }

    switch (cap_get_capType(lu_ret.cap)) {
    case cap_endpoint_cap:
        if (__builtin_expect(!!(!cap_endpoint_cap_get_capCanReceive(lu_ret.cap)), 0)) {
            current_lookup_fault = lookup_fault_missing_capability_new(0);
            current_fault = seL4_Fault_CapFault_new(epCPtr, true);
            handleFault(ksCurThread);
            break;
        }
# 447 "/home/b/test/sel4test/kernel/src/api/syscall.c"
        deleteCallerCap(ksCurThread);
        receiveIPC(ksCurThread, lu_ret.cap, isBlocking);

        break;

    case cap_notification_cap: {
        notification_t *ntfnPtr;
        tcb_t *boundTCB;
        ntfnPtr = ((notification_t *)(cap_notification_cap_get_capNtfnPtr(lu_ret.cap)));
        boundTCB = (tcb_t *)notification_ptr_get_ntfnBoundTCB(ntfnPtr);
        if (__builtin_expect(!!(!cap_notification_cap_get_capNtfnCanReceive(lu_ret.cap) || (boundTCB && boundTCB != ksCurThread)), 0)
                                                                          ) {
            current_lookup_fault = lookup_fault_missing_capability_new(0);
            current_fault = seL4_Fault_CapFault_new(epCPtr, true);
            handleFault(ksCurThread);
            break;
        }

        receiveSignal(ksCurThread, lu_ret.cap, isBlocking);
        break;
    }
    default:
        current_lookup_fault = lookup_fault_missing_capability_new(0);
        current_fault = seL4_Fault_CapFault_new(epCPtr, true);
        handleFault(ksCurThread);
        break;
    }
}
# 502 "/home/b/test/sel4test/kernel/src/api/syscall.c"
static void handleYield(void)
{
# 512 "/home/b/test/sel4test/kernel/src/api/syscall.c"
    tcbSchedDequeue(ksCurThread);
    tcbSchedAppend(ksCurThread);
    rescheduleRequired();

}

exception_t handleSyscall(syscall_t syscall)
{
    exception_t ret;
    irq_t irq;
    { { switch (syscall) { case SysSend: ret = handleInvocation(false, true); if (__builtin_expect(!!(ret != EXCEPTION_NONE), 0)) { ; irq = getActiveIRQ(); if ((irq) != (irqInvalid)) { handleInterrupt(irq); } } break; case SysNBSend: ret = handleInvocation(false, false); if (__builtin_expect(!!(ret != EXCEPTION_NONE), 0)) { ; irq = getActiveIRQ(); if ((irq) != (irqInvalid)) { handleInterrupt(irq); } } break; case SysCall: ret = handleInvocation(true, true); if (__builtin_expect(!!(ret != EXCEPTION_NONE), 0)) { ; irq = getActiveIRQ(); if ((irq) != (irqInvalid)) { handleInterrupt(irq); } } break; case SysRecv: handleRecv(true); break; case SysReply: handleReply(); break; case SysReplyRecv: handleReply(); handleRecv(true); break; case SysNBRecv: handleRecv(false); break; case SysYield: handleYield(); break; default: _fail("Invalid syscall", "/home/b/test/sel4test/kernel/src/api/syscall.c", 626, __func__); } } }
# 631 "/home/b/test/sel4test/kernel/src/api/syscall.c"
    schedule();
    activateThread();

    return EXCEPTION_NONE;
}
# 1 "/home/b/test/sel4test/kernel/src/arch/arm/64/c_traps.c"
/*
 * Copyright 2020, Data61, CSIRO (ABN 41 687 119 230)
 *
 * SPDX-License-Identifier: GPL-2.0-only
 */



# 1 "/home/b/test/sel4test/kernel/include/arch/arm/arch/fastpath/fastpath.h" 1
/*
 * Copyright 2014, General Dynamics C4 Systems
 *
 * SPDX-License-Identifier: GPL-2.0-only
 */

       


# 1 "/home/b/test/sel4test/kernel/include/arch/arm/arch/64/mode/fastpath/fastpath.h" 1
/*
 * Copyright 2020, Data61, CSIRO (ABN 41 687 119 230)
 *
 * SPDX-License-Identifier: GPL-2.0-only
 */

       






# 1 "/home/b/test/sel4test/kernel/include/arch/arm/armv/armv8-a/64/armv/context_switch.h" 1
/*
 * Copyright 2020, Data61, CSIRO (ABN 41 687 119 230)
 *
 * SPDX-License-Identifier: GPL-2.0-only
 */

       





static inline void armv_contextSwitch_HWASID(vspace_root_t *vspace, asid_t asid)
{
    setCurrentUserVSpaceRoot(ttbr_new(asid, addrFromPPtr(vspace)));
}

/*
 * In AARCH64, hardware and virtual asids are the same and are written
 * when updating the translation table base register.
 */
static inline void armv_contextSwitch(vspace_root_t *vspace, asid_t asid)
{



    setCurrentUserVSpaceRoot(ttbr_new(asid, addrFromPPtr(vspace)));
}
# 15 "/home/b/test/sel4test/kernel/include/arch/arm/arch/64/mode/fastpath/fastpath.h" 2
# 1 "/home/b/test/sel4test/kernel/include/arch/arm/arch/64/mode/model/statedata.h" 1
/*
 * Copyright 2020, Data61, CSIRO (ABN 41 687 119 230)
 *
 * SPDX-License-Identifier: GPL-2.0-only
 */

       
# 23 "/home/b/test/sel4test/kernel/include/arch/arm/arch/64/mode/model/statedata.h"
/* The top level asid mapping table */
extern asid_pool_t *armKSASIDTable[(1ul << (asidHighBits))] __attribute__((externally_visible));

/* This is the temporary userspace page table in kernel. It is required before running
 * user thread to avoid speculative page table walking with the wrong page table. */
extern vspace_root_t armKSGlobalUserVSpace[(1ul << (9))] __attribute__((externally_visible));
extern pte_t armKSGlobalKernelPGD[(1ul << (9))] __attribute__((externally_visible));

extern pte_t armKSGlobalKernelPUD[(1ul << (9))] __attribute__((externally_visible));
extern pte_t armKSGlobalKernelPDs[(1ul << (9))][(1ul << (9))] __attribute__((externally_visible));
extern pte_t armKSGlobalKernelPT[(1ul << (9))] __attribute__((externally_visible));
# 16 "/home/b/test/sel4test/kernel/include/arch/arm/arch/64/mode/fastpath/fastpath.h" 2

# 1 "/home/b/test/sel4test/kernel/include/machine/fpu.h" 1
/*
 * Copyright 2020, Data61, CSIRO (ABN 41 687 119 230)
 *
 * SPDX-License-Identifier: GPL-2.0-only
 */

       




# 1 "/home/b/test/sel4test/kernel/include/arch/arm/arch/machine/fpu.h" 1
/*
 * Copyright 2020, Data61, CSIRO (ABN 41 687 119 230)
 *
 * SPDX-License-Identifier: GPL-2.0-only
 */

       
# 1 "/home/b/test/sel4test/kernel/include/arch/arm/arch/64/mode/machine/fpu.h" 1
/*
 * Copyright 2020, Data61, CSIRO (ABN 41 687 119 230)
 *
 * SPDX-License-Identifier: GPL-2.0-only
 */

       




/* Store state in the FPU registers into memory. */
static inline void saveFpuState(tcb_t *thread)
{
    user_fpu_state_t *dest = &thread->tcbArch.tcbContext.fpuState;
    word_t temp;

    __asm__ volatile(
        /* SIMD and floating-point register file */
        ".arch_extension fp\n"
        "stp     q0, q1, [%1, #16 * 0]      \n"
        "stp     q2, q3, [%1, #16 * 2]      \n"
        "stp     q4, q5, [%1, #16 * 4]      \n"
        "stp     q6, q7, [%1, #16 * 6]      \n"
        "stp     q8, q9, [%1, #16 * 8]      \n"
        "stp     q10, q11, [%1, #16 * 10]   \n"
        "stp     q12, q13, [%1, #16 * 12]   \n"
        "stp     q14, q15, [%1, #16 * 14]   \n"
        "stp     q16, q17, [%1, #16 * 16]   \n"
        "stp     q18, q19, [%1, #16 * 18]   \n"
        "stp     q20, q21, [%1, #16 * 20]   \n"
        "stp     q22, q23, [%1, #16 * 22]   \n"
        "stp     q24, q25, [%1, #16 * 24]   \n"
        "stp     q26, q27, [%1, #16 * 26]   \n"
        "stp     q28, q29, [%1, #16 * 28]   \n"
        "stp     q30, q31, [%1, #16 * 30]   \n"

        /* FP control and status registers */
        "mrs     %0, fpsr                   \n"
        "str     %w0, [%1, #16 * 32]        \n"
        "mrs     %0, fpcr                   \n"
        "str     %w0, [%1, #16 * 32 + 4]    \n"
        ".arch_extension nofp\n"
        : "=&r"(temp)
        : "r"(dest)
        : "memory"
    );
}

/* Load FPU state from memory into the FPU registers. */
static inline void loadFpuState(const tcb_t *thread)
{
    const user_fpu_state_t *src = &thread->tcbArch.tcbContext.fpuState;
    word_t temp;

    __asm__ volatile(
        /* SIMD and floating-point register file */
        ".arch_extension fp\n"
        "ldp     q0, q1, [%1, #16 * 0]      \n"
        "ldp     q2, q3, [%1, #16 * 2]      \n"
        "ldp     q4, q5, [%1, #16 * 4]      \n"
        "ldp     q6, q7, [%1, #16 * 6]      \n"
        "ldp     q8, q9, [%1, #16 * 8]      \n"
        "ldp     q10, q11, [%1, #16 * 10]   \n"
        "ldp     q12, q13, [%1, #16 * 12]   \n"
        "ldp     q14, q15, [%1, #16 * 14]   \n"
        "ldp     q16, q17, [%1, #16 * 16]   \n"
        "ldp     q18, q19, [%1, #16 * 18]   \n"
        "ldp     q20, q21, [%1, #16 * 20]   \n"
        "ldp     q22, q23, [%1, #16 * 22]   \n"
        "ldp     q24, q25, [%1, #16 * 24]   \n"
        "ldp     q26, q27, [%1, #16 * 26]   \n"
        "ldp     q28, q29, [%1, #16 * 28]   \n"
        "ldp     q30, q31, [%1, #16 * 30]  \n"

        /* FP control and status registers */
        "ldr     %w0, [%1, #16 * 32]        \n"
        "msr     fpsr, %0                   \n"
        "ldr     %w0, [%1, #16 * 32 + 4]    \n"
        "msr     fpcr, %0                   \n"
        ".arch_extension nofp\n"
        : "=&r"(temp)
        : "r"(src)
        : "memory"
    );
}

/* Disable trapping FPU instructions to EL2 */
static inline void disableTrapFpu(void)
{
    word_t cptr;
    __asm__ volatile("mrs %x0," "cptr_el2" : "=r"(cptr));
    cptr &= ~((1ul << (10)) | (1ul << (31)));
    do { word_t _v = cptr; __asm__ volatile("msr " "cptr_el2" ",%x0" :: "r" (_v)); }while(0);
    isb();
}

/* Enable FPU access in EL0 and EL1 */
static inline void enableFpuEL01(void)
{
    word_t cpacr;
    __asm__ volatile("mrs %x0," "cpacr_el1" : "=r"(cpacr));
    cpacr |= (3 << 20 /* FP registers access*/);
    do { word_t _v = cpacr; __asm__ volatile("msr " "cpacr_el1" ",%x0" :: "r" (_v)); }while(0);
    isb();
}

/* Enable the FPU to be used without faulting.
 * Required even if the kernel attempts to use the FPU. */
/** MODIFIES: phantom_machine_state */
/** DONT_TRANSLATE */
static inline void enableFpu(void)
{
    if (wrap_config_set(0)) {
        disableTrapFpu();
    } else {
        enableFpuEL01();
    }
}


/* Trap any FPU related instructions to EL2 */
static inline void enableTrapFpu(void)
{
    word_t cptr;
    __asm__ volatile("mrs %x0," "cptr_el2" : "=r"(cptr));
    cptr |= ((1ul << (10)) | (1ul << (31)));
    do { word_t _v = cptr; __asm__ volatile("msr " "cptr_el2" ",%x0" :: "r" (_v)); }while(0);
    isb();
}

/* Disable FPU access in EL0 */
static inline void disableFpuEL0(void)
{
    word_t cpacr;
    __asm__ volatile("mrs %x0," "cpacr_el1" : "=r"(cpacr));
    cpacr &= ~(3 << 20 /* FP registers access*/);
    cpacr |= (1 << 20 /* FP registers access*/);
    do { word_t _v = cpacr; __asm__ volatile("msr " "cpacr_el1" ",%x0" :: "r" (_v)); }while(0);
    isb();
}

/* Disable the FPU so that usage of it causes a fault */
static inline void disableFpu(void)
{
    if (wrap_config_set(0)) {
        enableTrapFpu();
    } else {
        disableFpuEL0();
    }
}
# 9 "/home/b/test/sel4test/kernel/include/arch/arm/arch/machine/fpu.h" 2

bool_t fpsimd_HWCapTest(void);
bool_t fpsimd_init(void);
# 13 "/home/b/test/sel4test/kernel/include/machine/fpu.h" 2



/* Perform any actions required for the deletion of the given thread. */
void fpuRelease(tcb_t *thread);

void switchLocalFpuOwner(tcb_t *new_owner);

/* Switch the current owner of the FPU state on the core specified by 'cpu'. */
void switchFpuOwner(tcb_t *new_owner, word_t cpu);

/* Returns whether or not the passed thread is using the current active fpu state */
static inline bool_t nativeThreadUsingFPU(tcb_t *thread)
{
    return thread == ksCurFPUOwner;
}

static inline void __attribute__((always_inline)) lazyFPURestore(tcb_t *thread)
{
    if (thread->tcbFlags & seL4_TCBFlag_fpuDisabled) {
        disableFpu();
    } else if (nativeThreadUsingFPU(thread)) {
        enableFpu();
    } else {
        switchLocalFpuOwner(thread);
    }
}
# 18 "/home/b/test/sel4test/kernel/include/arch/arm/arch/64/mode/fastpath/fastpath.h" 2


/* When building the fastpath the assembler in traps.S makes these
 * assumptions. Because compile_asserts are hard to do in assembler,
 * we place them here */
_Static_assert(SysCall == -1, "SysCall_Minus1");
_Static_assert(SysReplyRecv == -2, "SysReplyRecv_Minus2");

/* Use macros to not break verification */



static inline void __attribute__((always_inline))
switchToThread_fp(tcb_t *thread, vspace_root_t *vroot, pde_t stored_hw_asid)
{
    asid_t asid;

    if (wrap_config_set(0)) {
        do {} while(0);
    }
    asid = (asid_t)(stored_hw_asid.words[0] & 0xffff);
    armv_contextSwitch_HWASID(vroot, asid);






    lazyFPURestore(thread);


    ksCurThread = thread;
}
# 89 "/home/b/test/sel4test/kernel/include/arch/arm/arch/64/mode/fastpath/fastpath.h"
static inline void mdb_node_ptr_mset_mdbNext_mdbRevocable_mdbFirstBadged(
    mdb_node_t *node_ptr, word_t mdbNext,
    word_t mdbRevocable, word_t mdbFirstBadged)
{
    node_ptr->words[1] = mdbNext | (mdbRevocable << 1) | mdbFirstBadged;
}

static inline void mdb_node_ptr_set_mdbPrev_np(mdb_node_t *node_ptr, word_t mdbPrev)
{
    node_ptr->words[0] = mdbPrev;
}

static inline bool_t isValidVTableRoot_fp(cap_t vspace_root_cap)
{
    return cap_capType_equals(vspace_root_cap, cap_vspace_cap)
           && cap_vspace_cap_get_capVSIsMapped(vspace_root_cap);
}

/* This is an accelerated check that msgLength, which appears
   in the bottom of the msgInfo word, is <= 4 and that msgExtraCaps
   which appears above it is zero. We are assuming that n_msgRegisters == 4
   for this check to be useful. By masking out the bottom 3 bits, we are
   really checking that n + 3 <= MASK(3), i.e. n + 3 <= 7 or n <= 4. */
_Static_assert(n_msgRegisters == 4, "n_msgRegisters_eq_4");
static inline int
fastpath_mi_check(word_t msgInfo)
{
    return (msgInfo & ((1ul << (seL4_MsgLengthBits + seL4_MsgExtraCapBits)) - 1ul)) > 4;
}

static inline void fastpath_copy_mrs(word_t length, tcb_t *src, tcb_t *dest)
{
    word_t i;
    register_t reg;

    /* assuming that length < n_msgRegisters */
    for (i = 0; i < length; i ++) {
        /* assuming that the message registers simply increment */
        reg = msgRegisters[0] + i;
        setRegister(dest, reg, getRegister(src, reg));
    }
}


static inline int fastpath_reply_cap_check(cap_t cap)
{
    return cap_capType_equals(cap, cap_reply_cap);
}


/** DONT_TRANSLATE */
static inline void __attribute__((__noreturn__)) __attribute__((always_inline)) fastpath_restore(word_t badge, word_t msgInfo, tcb_t *cur_thread)
{
    do {} while (0);

    c_exit_hook();


    restore_user_debug_context(cur_thread);


    register word_t badge_reg __asm__("x0") = badge;
    register word_t msgInfo_reg __asm__("x1") = msgInfo;
    register word_t cur_thread_reg __asm__("x2") = (word_t)cur_thread->tcbArch.tcbContext.registers;

    __asm__ volatile(
        "mov     sp, x2                     \n"

        /* Restore thread's SPSR, LR, and SP */
        "ldp     x21, x22, [sp, %[SP_EL0]]  \n"
        "ldr     x23, [sp, %[SPSR_EL1]]     \n"
        "msr     sp_el0, x21                \n"




        "msr     elr_el1, x22               \n"
        "msr     spsr_el1, x23              \n"


        /* Restore remaining registers */
        "ldp     x2,  x3,  [sp, #16 * 1]    \n"
        "ldp     x4,  x5,  [sp, #16 * 2]    \n"
        "ldp     x6,  x7,  [sp, #16 * 3]    \n"
        "ldp     x8,  x9,  [sp, #16 * 4]    \n"
        "ldp     x10, x11, [sp, #16 * 5]    \n"
        "ldp     x12, x13, [sp, #16 * 6]    \n"
        "ldp     x14, x15, [sp, #16 * 7]    \n"
        "ldp     x16, x17, [sp, #16 * 8]    \n"
        "ldp     x18, x19, [sp, #16 * 9]    \n"
        "ldp     x20, x21, [sp, #16 * 10]   \n"
        "ldp     x22, x23, [sp, #16 * 11]   \n"
        "ldp     x24, x25, [sp, #16 * 12]   \n"
        "ldp     x26, x27, [sp, #16 * 13]   \n"
        "ldp     x28, x29, [sp, #16 * 14]   \n"
        "ldr     x30, [sp, %[LR]]           \n"
        "eret                                 "
        :
        : "r"(badge_reg), "r"(msgInfo_reg), "r"(cur_thread_reg),
        [SP_EL0] "i"((31 * 8)), [SPSR_EL1] "i"((33 * 8)), [LR] "i"((30 * 8))
        : "memory"
    );

    __builtin_unreachable();
}
# 11 "/home/b/test/sel4test/kernel/include/arch/arm/arch/fastpath/fastpath.h" 2



void slowpath(syscall_t syscall)
__attribute__((__noreturn__));







static inline
void fastpath_call(word_t cptr, word_t r_msgInfo)
__attribute__((__noreturn__));
# 37 "/home/b/test/sel4test/kernel/include/arch/arm/arch/fastpath/fastpath.h"
static inline



void fastpath_reply_recv(word_t cptr, word_t r_msgInfo)

__attribute__((__noreturn__));
# 10 "/home/b/test/sel4test/kernel/src/arch/arm/64/c_traps.c" 2
# 18 "/home/b/test/sel4test/kernel/src/arch/arm/64/c_traps.c"
/** DONT_TRANSLATE */
void __attribute__((externally_visible)) __attribute__((__noreturn__)) restore_user_context(void)
{
    c_exit_hook();


    restore_user_debug_context(ksCurThread);


    do {} while (0);

    __asm__ volatile(
        "mov     sp, %0                     \n"

        /* Restore thread's SPSR, LR, and SP */
        "ldp     x21, x22, [sp, %[SP_EL0]] \n"
        "ldr     x23, [sp, %[SPSR_EL1]]    \n"
        "msr     sp_el0, x21                \n"




        "msr     elr_el1, x22               \n"
        "msr     spsr_el1, x23              \n"

        /* Restore remaining registers */
        "ldp     x0,  x1,  [sp, #16 * 0]    \n"
        "ldp     x2,  x3,  [sp, #16 * 1]    \n"
        "ldp     x4,  x5,  [sp, #16 * 2]    \n"
        "ldp     x6,  x7,  [sp, #16 * 3]    \n"
        "ldp     x8,  x9,  [sp, #16 * 4]    \n"
        "ldp     x10, x11, [sp, #16 * 5]    \n"
        "ldp     x12, x13, [sp, #16 * 6]    \n"
        "ldp     x14, x15, [sp, #16 * 7]    \n"
        "ldp     x16, x17, [sp, #16 * 8]    \n"
        "ldp     x18, x19, [sp, #16 * 9]    \n"
        "ldp     x20, x21, [sp, #16 * 10]   \n"
        "ldp     x22, x23, [sp, #16 * 11]   \n"
        "ldp     x24, x25, [sp, #16 * 12]   \n"
        "ldp     x26, x27, [sp, #16 * 13]   \n"
        "ldp     x28, x29, [sp, #16 * 14]   \n"
        "ldr     x30, [sp, %[LR]]          \n"
        "eret"
        :
        : "r"(ksCurThread->tcbArch.tcbContext.registers),
        [SP_EL0] "i"((31 * 8)), [SPSR_EL1] "i"((33 * 8)), [LR] "i"((30 * 8))
        : "memory"
    );
    __builtin_unreachable();
}
# 1 "/home/b/test/sel4test/kernel/src/arch/arm/64/idle.c"
/*
 * Copyright 2020, Data61, CSIRO (ABN 41 687 119 230)
 *
 * SPDX-License-Identifier: GPL-2.0-only
 */





void idle_thread(void)
{
    while (1) {
        wfi();
    }
}

/** DONT_TRANSLATE */
void __attribute__((__noreturn__)) __attribute__((noinline)) __attribute__((externally_visible)) halt(void)
{
    /* halt is actually, idle thread without the interrupts */
    do { word_t _v = ((1 << 9) | (1 << 8) | (1 << 7) | (1 << 6)); __asm__ volatile("msr " "daif" ",%x0" :: "r" (_v)); }while(0);


    printf("halting...");

    debug_printKernelEntryReason();


    idle_thread();
    __builtin_unreachable();
}
# 1 "/home/b/test/sel4test/kernel/src/arch/arm/64/kernel/thread.c"
/*
 * Copyright 2020, Data61, CSIRO (ABN 41 687 119 230)
 *
 * SPDX-License-Identifier: GPL-2.0-only
 */
# 14 "/home/b/test/sel4test/kernel/src/arch/arm/64/kernel/thread.c"
void Arch_switchToThread(tcb_t *tcb)
{
    if (wrap_config_set(0)) {
        do {} while(0);
    }
    setVMRoot(tcb);
}

__attribute__((__section__(".boot.text"))) void Arch_configureIdleThread(tcb_t *tcb)
{
    setRegister(tcb, SPSR_EL1, ((1 << 6) | 5 | 0));
    setRegister(tcb, ELR_EL1, (word_t)&idle_thread);
}

void Arch_switchToIdleThread(void)
{
    if (wrap_config_set(0)) {
        do {} while(0);
    }
    setCurrentUserVSpaceRoot(ttbr_new(0, addrFromKPPtr(armKSGlobalUserVSpace)));
}

void Arch_activateIdleThread(tcb_t *tcb)
{
    /* Don't need to do anything */
}
# 1 "/home/b/test/sel4test/kernel/src/arch/arm/64/kernel/vspace.c"
/*
 * Copyright 2020, Data61, CSIRO (ABN 41 687 119 230)
 *
 * SPDX-License-Identifier: GPL-2.0-only
 */






# 1 "/home/b/test/sel4test/kernel/include/kernel/boot.h" 1
/*
 * Copyright 2014, General Dynamics C4 Systems
 *
 * SPDX-License-Identifier: GPL-2.0-only
 */

       



# 1 "/home/b/test/sel4test/kernel/include/bootinfo.h" 1
/*
 * Copyright 2014, General Dynamics C4 Systems
 *
 * SPDX-License-Identifier: GPL-2.0-only
 */

       



# 1 "/home/b/test/sel4test/kernel/libsel4/include/sel4/bootinfo_types.h" 1
/*
 * Copyright 2020, Data61, CSIRO (ABN 41 687 119 230)
 *
 * SPDX-License-Identifier: BSD-2-Clause
 */

       





/* caps with fixed slot positions in the root CNode */
enum seL4_RootCNodeCapSlots {
    seL4_CapNull = 0, /* null cap */
    seL4_CapInitThreadTCB = 1, /* initial thread's TCB cap */
    seL4_CapInitThreadCNode = 2, /* initial thread's root CNode cap */
    seL4_CapInitThreadVSpace = 3, /* initial thread's VSpace cap */
    seL4_CapIRQControl = 4, /* global IRQ controller cap */
    seL4_CapASIDControl = 5, /* global ASID controller cap */
    seL4_CapInitThreadASIDPool = 6, /* initial thread's ASID pool cap */
    seL4_CapIOPortControl = 7, /* global IO port control cap (null cap if not supported) */
    seL4_CapIOSpace = 8, /* global IO space cap (null cap if no IOMMU support) */
    seL4_CapBootInfoFrame = 9, /* bootinfo frame cap */
    seL4_CapInitThreadIPCBuffer = 10, /* initial thread's IPC buffer frame cap */
    seL4_CapDomain = 11, /* global domain controller cap */
    seL4_CapSMMUSIDControl = 12, /* global SMMU SID controller cap, null cap if not supported */
    seL4_CapSMMUCBControl = 13, /* global SMMU CB controller cap, null cap if not supported */
    seL4_CapInitThreadSC = 14, /* initial thread's scheduling context cap, null cap if not supported */
    seL4_CapSMC = 15, /* global SMC cap, null cap if not supported */
    seL4_NumInitialCaps = 16
};

/* Legacy code will have assumptions on the vspace root being a Page Directory
 * type, so for now we define one to the other */


/* types */
typedef seL4_Word seL4_SlotPos;

typedef struct seL4_SlotRegion {
    seL4_SlotPos start; /* first CNode slot position OF region */
    seL4_SlotPos end; /* first CNode slot position AFTER region */
} seL4_SlotRegion;

typedef struct seL4_UntypedDesc {
    seL4_Word paddr; /* physical address of untyped cap  */
    seL4_Uint8 sizeBits;/* size (2^n) bytes of each untyped */
    seL4_Uint8 isDevice;/* whether the untyped is a device  */
    seL4_Uint8 padding[sizeof(seL4_Word) - 2 * sizeof(seL4_Uint8)];
} seL4_UntypedDesc;

typedef int __assert_failed_invalid_seL4_UntypedDesc[(sizeof(seL4_UntypedDesc) == 2 * sizeof(seL4_Word)) ? 1 : -1] __attribute__((unused));

                                                      ;

typedef struct seL4_BootInfo {
    seL4_Word extraLen; /* length of any additional bootinfo information */
    seL4_NodeId nodeID; /* ID [0..numNodes-1] of the seL4 node (0 if uniprocessor) */
    seL4_Word numNodes; /* number of seL4 nodes (1 if uniprocessor) */
    seL4_Word numIOPTLevels; /* number of IOMMU PT levels (0 if no IOMMU support) */
    seL4_IPCBuffer *ipcBuffer; /* pointer to initial thread's IPC buffer */
    seL4_SlotRegion empty; /* empty slots (null caps) */
    seL4_SlotRegion sharedFrames; /* shared-frame caps (shared between seL4 nodes) */
    seL4_SlotRegion userImageFrames; /* userland-image frame caps */
    seL4_SlotRegion userImagePaging; /* userland-image paging structure caps */
    seL4_SlotRegion ioSpaceCaps; /* IOSpace caps for ARM SMMU */
    seL4_SlotRegion extraBIPages; /* caps for any pages used to back the additional bootinfo information */
    seL4_Word initThreadCNodeSizeBits; /* initial thread's root CNode size (2^n slots) */
    seL4_Domain initThreadDomain; /* Initial thread's domain ID */



    seL4_SlotRegion untyped; /* untyped-object caps (untyped caps) */
    seL4_UntypedDesc untypedList[230]; /* information about each untyped */
    /* the untypedList should be the last entry in this struct, in order
     * to make this struct easier to represent in other languages */
} seL4_BootInfo;

/* The boot info frame must be large enough to hold the seL4_BootInfo data
 * structure. Due to internal restrictions, the size must be of the form 2^n and
 * the minimum is one page.
 */



typedef int __assert_failed_invalid_seL4_BootInfoFrameSize[(sizeof(seL4_BootInfo) <= (1ul<<(12))) ? 1 : -1] __attribute__((unused));



/* If seL4_BootInfo.extraLen > 0, this indicate the presence of additional boot
 * information chunks starting at the offset seL4_BootInfoFrameSize. Userland
 * code often contains the hard-coded assumption that the offset is 4 KiByte,
 * because the boot info frame usually is one page, which is 4 KiByte on x86,
 * Arm and RISC-V.
 * The additional boot info chunks are arch/platform specific, they may or may
 * not exist in any given execution. Each chunk has a header that contains an ID
 * to describe the chunk. All IDs share a global namespace to ensure uniqueness.
 */

typedef enum {
    SEL4_BOOTINFO_HEADER_PADDING = 0,
    SEL4_BOOTINFO_HEADER_X86_VBE = 1,
    SEL4_BOOTINFO_HEADER_X86_MBMMAP = 2,
    SEL4_BOOTINFO_HEADER_X86_ACPI_RSDP = 3,
    SEL4_BOOTINFO_HEADER_X86_FRAMEBUFFER = 4,
    SEL4_BOOTINFO_HEADER_X86_TSC_FREQ = 5, /* frequency is in MHz */
    SEL4_BOOTINFO_HEADER_FDT = 6, /* device tree */
    /* Add more IDs here, the two elements below must always be at the end. */
    SEL4_BOOTINFO_HEADER_NUM,
    _enum_pad_seL4_BootInfoID = ((1ULL << ((sizeof(long)*8) - 1)) - 1)
} seL4_BootInfoID;

/* Common header for all additional bootinfo chunks to describe the chunk. */
typedef struct seL4_BootInfoHeader {
    seL4_Word id; /* identifier of the following blob */
    seL4_Word len; /* length of the chunk, including this header */
} seL4_BootInfoHeader;

typedef int __assert_failed_invalid_seL4_BootInfoHeader[(sizeof(seL4_BootInfoHeader) == 2 * sizeof(seL4_Word)) ? 1 : -1] __attribute__((unused));

                                                         ;
# 12 "/home/b/test/sel4test/kernel/include/bootinfo.h" 2

/* declare object-specific macros to hide the casting */
# 12 "/home/b/test/sel4test/kernel/include/kernel/boot.h" 2
# 1 "/home/b/test/sel4test/kernel/include/arch/arm/arch/bootinfo.h" 1
/*
 * Copyright 2020, Data61, CSIRO (ABN 41 687 119 230)
 *
 * SPDX-License-Identifier: GPL-2.0-only
 */

       





/* The max number of free memory regions is:
 * +1 for each available physical memory region (elements in avail_p_regs)
 * +1 for each MODE_RESERVED region, there might be none
 * +1 to allow the kernel to release its own boot data region
 * +1 for a possible gap between ELF images and rootserver objects
 */


/* The regions reserved by the boot code are:
 * +1 for kernel
 * +1 for device tree binary
 * +1 for user image.
 * +1 for each the MODE_RESERVED region, there might be none
 */



/* The maximum number of reserved regions is:
 * +1 for each free memory region (MAX_NUM_FREEMEM_REG)
 * +1 for each kernel frame (NUM_KERNEL_DEVICE_FRAMES, there might be none)
 * +1 for each region reserved by the boot code (NUM_RESERVED_REGIONS)
 */
# 13 "/home/b/test/sel4test/kernel/include/kernel/boot.h" 2

/*
 * Resolve naming differences between the abstract specifications
 * of the bootstrapping phase and the runtime phase of the kernel.
 */
typedef cte_t slot_t;
typedef cte_t *slot_ptr_t;



/* (node-local) state accessed only during bootstrapping */

typedef struct ndks_boot {
    p_region_t reserved[(((sizeof(avail_p_regs) / sizeof((avail_p_regs)[0])) + 0 + 1 + 1) + (sizeof(kernel_device_frames) / sizeof((kernel_device_frames)[0])) + (3 + 0))];
    word_t resv_count;
    region_t freemem[((sizeof(avail_p_regs) / sizeof((avail_p_regs)[0])) + 0 + 1 + 1)];
    seL4_BootInfo *bi_frame;
    seL4_SlotPos slot_pos_cur;
} ndks_boot_t;

extern ndks_boot_t ndks_boot;

/* function prototypes */

static inline bool_t is_reg_empty(region_t reg)
{
    return reg.start == reg.end;
}

p_region_t get_p_reg_kernel_img_boot(void);
p_region_t get_p_reg_kernel_img(void);
bool_t init_freemem(word_t n_available, const p_region_t *available,
                    word_t n_reserved, const region_t *reserved,
                    v_region_t it_v_reg, word_t extra_bi_size_bits);
bool_t reserve_region(p_region_t reg);
void write_slot(slot_ptr_t slot_ptr, cap_t cap);
cap_t create_root_cnode(void);
bool_t provide_cap(cap_t root_cnode_cap, cap_t cap);
cap_t create_it_asid_pool(cap_t root_cnode_cap);
void write_it_pd_pts(cap_t root_cnode_cap, cap_t it_pd_cap);
void create_idle_thread(void);
bool_t create_untypeds(cap_t root_cnode_cap);
void bi_finalise(void);
void create_domain_cap(cap_t root_cnode_cap);

cap_t create_ipcbuf_frame_cap(cap_t root_cnode_cap, cap_t pd_cap, vptr_t vptr);
word_t calculate_extra_bi_size_bits(word_t extra_size);
void populate_bi_frame(node_id_t node_id, word_t num_nodes, vptr_t ipcbuf_vptr,
                       word_t extra_bi_size);
void create_bi_frame_cap(cap_t root_cnode_cap, cap_t pd_cap, vptr_t vptr);





typedef struct create_frames_of_region_ret {
    seL4_SlotRegion region;
    bool_t success;
} create_frames_of_region_ret_t;

create_frames_of_region_ret_t
create_frames_of_region(
    cap_t root_cnode_cap,
    cap_t pd_cap,
    region_t reg,
    bool_t do_map,
    sword_t pv_offset
);

cap_t
create_it_pd_pts(
    cap_t root_cnode_cap,
    v_region_t ui_v_reg,
    vptr_t ipcbuf_vptr,
    vptr_t bi_frame_vptr
);

tcb_t *
create_initial_thread(
    cap_t root_cnode_cap,
    cap_t it_pd_cap,
    vptr_t ui_v_entry,
    vptr_t bi_frame_vptr,
    vptr_t ipcbuf_vptr,
    cap_t ipcbuf_cap
);

void init_core_state(tcb_t *scheduler_action);

/* state tracking the memory allocated for root server objects */
typedef struct {
    pptr_t cnode;
    pptr_t vspace;
    pptr_t asid_pool;
    pptr_t ipc_buf;
    pptr_t boot_info;
    pptr_t extra_bi;
    pptr_t tcb;



    region_t paging;
} rootserver_mem_t;

extern rootserver_mem_t rootserver;

/* get the number of paging structures required to cover it_v_reg, with
 * the paging structure covering `bits` of the address range - for a 4k page
 * `bits` would be 12 */
static inline __attribute__((__section__(".boot.text"))) word_t get_n_paging(v_region_t v_reg, word_t bits)
{
    vptr_t start = (((v_reg.start) >> (bits)) << (bits));
    vptr_t end = (((((v_reg.end) - 1ul) >> (bits)) + 1ul) << (bits));
    return (end - start) / (1ul << (bits));
}

/* allocate a page table sized structure from rootserver.paging */
static inline __attribute__((__section__(".boot.text"))) pptr_t it_alloc_paging(void)
{
    pptr_t allocated = rootserver.paging.start;
    rootserver.paging.start += (1ul << (12));
    do { if (!(rootserver.paging.start <= rootserver.paging.end)) { _assert_fail("rootserver.paging.start <= rootserver.paging.end", "/home/b/test/sel4test/kernel/include/kernel/boot.h", 134, __func__); } } while(0);
    return allocated;
}

/* return the amount of paging structures required to cover v_reg */
word_t arch_get_n_paging(v_region_t it_veg);
# 13 "/home/b/test/sel4test/kernel/src/arch/arm/64/kernel/vspace.c" 2
# 21 "/home/b/test/sel4test/kernel/src/arch/arm/64/kernel/vspace.c"
# 1 "/home/b/test/sel4test/cbuild/kernel/gen_headers/arch/api/invocation.h" 1

/*
 * Copyright 2020, Data61, CSIRO (ABN 41 687 119 230)
 ** SPDX-License-Identifier: GPL-2.0-only
 */

/* This header was generated by kernel/tools/invocation_header_gen.py.
 *
 * To add an invocation call number, edit libsel4/include/interfaces/sel4.xml.
 *
 */
       
# 1 "/home/b/test/sel4test/cbuild/kernel/gen_headers/arch/api/sel4_invocation.h" 1

/*
 * Copyright 2020, Data61, CSIRO (ABN 41 687 119 230)
 ** SPDX-License-Identifier: GPL-2.0-only
 */

/* This header was generated by kernel/tools/invocation_header_gen.py.
 *
 * To add an invocation call number, edit libsel4/include/interfaces/sel4.xml.
 *
 */
       
# 1 "/home/b/test/sel4test/cbuild/kernel/gen_headers/api/invocation.h" 1

/*
 * Copyright 2020, Data61, CSIRO (ABN 41 687 119 230)
 ** SPDX-License-Identifier: GPL-2.0-only
 */

/* This header was generated by kernel/tools/invocation_header_gen.py.
 *
 * To add an invocation call number, edit libsel4/include/interfaces/sel4.xml.
 *
 */
       

enum invocation_label {
    InvalidInvocation,
    UntypedRetype,
    TCBReadRegisters,
    TCBWriteRegisters,
    TCBCopyRegisters,

    TCBConfigure,




    TCBSetPriority,
    TCBSetMCPriority,

    TCBSetSchedParams,







    TCBSetIPCBuffer,

    TCBSetSpace,




    TCBSuspend,
    TCBResume,
    TCBBindNotification,
    TCBUnbindNotification,




    TCBSetBreakpoint,


    TCBGetBreakpoint,


    TCBUnsetBreakpoint,


    TCBConfigureSingleStepping,

    TCBSetTLSBase,
    TCBSetFlags,
    CNodeRevoke,
    CNodeDelete,
    CNodeCancelBadgedSends,
    CNodeCopy,
    CNodeMint,
    CNodeMove,
    CNodeMutate,
    CNodeRotate,

    CNodeSaveCaller,

    IRQIssueIRQHandler,
    IRQAckIRQ,
    IRQSetIRQHandler,
    IRQClearIRQHandler,
    DomainSetSet,
# 99 "/home/b/test/sel4test/cbuild/kernel/gen_headers/api/invocation.h"
    nInvocationLabels
};
# 14 "/home/b/test/sel4test/cbuild/kernel/gen_headers/arch/api/sel4_invocation.h" 2

enum sel4_arch_invocation_label {
    ARMVSpaceClean_Data = nInvocationLabels,
    ARMVSpaceInvalidate_Data,
    ARMVSpaceCleanInvalidate_Data,
    ARMVSpaceUnify_Instruction,
    ARMSMCCall,
    nSeL4ArchInvocationLabels
};
# 14 "/home/b/test/sel4test/cbuild/kernel/gen_headers/arch/api/invocation.h" 2

enum arch_invocation_label {
    ARMPageTableMap = nSeL4ArchInvocationLabels,
    ARMPageTableUnmap,






    ARMPageMap,
    ARMPageUnmap,



    ARMPageClean_Data,
    ARMPageInvalidate_Data,
    ARMPageCleanInvalidate_Data,
    ARMPageUnify_Instruction,
    ARMPageGetAddress,
    ARMASIDControlMakePool,
    ARMASIDPoolAssign,
# 51 "/home/b/test/sel4test/cbuild/kernel/gen_headers/arch/api/invocation.h"
    ARMIRQIssueIRQHandlerTrigger,




    ARMIRQIssueSGISignal,
# 94 "/home/b/test/sel4test/cbuild/kernel/gen_headers/arch/api/invocation.h"
    nArchInvocationLabels
};
# 22 "/home/b/test/sel4test/kernel/src/arch/arm/64/kernel/vspace.c" 2




# 1 "/home/b/test/sel4test/kernel/include/arch/arm/arch/object/iospace.h" 1
/*
 * Copyright 2016, General Dynamics C4 Systems
 *
 * SPDX-License-Identifier: GPL-2.0-only
 */

       
# 26 "/home/b/test/sel4test/kernel/include/arch/arm/arch/object/iospace.h"
/* define dummy functions */
static inline seL4_SlotRegion create_iospace_caps(cap_t root_cnode_cap)
{
    return (seL4_SlotRegion){ .start = 0, .end = 0 };
}

static inline exception_t decodeARMIOPTInvocation(word_t invLabel, uint32_t length, cte_t *slot, cap_t cap,
                                                  word_t *buffer)
{
    return EXCEPTION_NONE;
}

static inline exception_t decodeARMIOMapInvocation(word_t invLabel, uint32_t length, cte_t *slot, cap_t cap,
                                                   word_t *buffer)
{
    return EXCEPTION_NONE;
}

static inline exception_t performPageInvocationUnmapIO(cap_t cap, cte_t *slot)
{
    return EXCEPTION_NONE;
}

static inline exception_t decodeARMIOSpaceInvocation(word_t invLabel, cap_t cap)
{
    return EXCEPTION_NONE;
}

static inline void unmapIOPage(cap_t cap)
{
}

static inline void deleteIOPageTable(cap_t cap)
{
}

static inline void clearIOPageDirectory(cap_t cap)
{
}
# 27 "/home/b/test/sel4test/kernel/src/arch/arm/64/kernel/vspace.c" 2

# 1 "/home/b/test/sel4test/kernel/include/arch/arm/arch/machine/tlb.h" 1
/*
 * Copyright 2020, Data61, CSIRO (ABN 41 687 119 230)
 *
 * SPDX-License-Identifier: GPL-2.0-only
 */

       


# 1 "/home/b/test/sel4test/kernel/include/arch/arm/arch/smp/ipi_inline.h" 1
/*
 * Copyright 2020, Data61, CSIRO (ABN 41 687 119 230)
 *
 * SPDX-License-Identifier: GPL-2.0-only
 */

       
# 11 "/home/b/test/sel4test/kernel/include/arch/arm/arch/machine/tlb.h" 2




static inline void invalidateTranslationSingleLocal(vptr_t vptr)
{



    invalidateLocalTLB_VAASID(vptr);

}

static inline void invalidateTranslationASIDLocal(hw_asid_t hw_asid)
{



    invalidateLocalTLB_ASID(hw_asid);

}

static inline void invalidateTranslationAllLocal(void)
{
    invalidateLocalTLB();
}

static inline void invalidateTranslationSingle(vptr_t vptr)
{
    invalidateTranslationSingleLocal(vptr);
    ;
}

static inline void invalidateTranslationASID(hw_asid_t hw_asid)
{
    invalidateTranslationASIDLocal(hw_asid);
    ;
}

static inline void invalidateTranslationAll(void)
{
    invalidateTranslationAllLocal();
    ;
}
# 29 "/home/b/test/sel4test/kernel/src/arch/arm/64/kernel/vspace.c" 2

/*
 * Memory types are defined in Memory Attribute Indirection Register.
 *  - nGnRnE Device non-Gathering, non-Reordering, No Early write acknowledgement
 *  - nGnRE Unused Device non-Gathering, non-Reordering, Early write acknowledgement
 *  - GRE Unused Device Gathering, Reordering, Early write acknowledgement
 *  - NORMAL_NC Normal Memory, Inner/Outer non-cacheable
 *  - NORMAL Normal Memory, Inner/Outer Write-back non-transient, Write-allocate, Read-allocate
 *  - NORMAL_WT Normal Memory, Inner/Outer Write-through non-transient, No-Write-allocate, Read-allocate
 * Note: These should match with contents of MAIR_EL1 register!
 */
enum mair_types {
    DEVICE_nGnRnE = 0,
    DEVICE_nGnRE = 1,
    DEVICE_GRE = 2,
    NORMAL_NC = 3,
    NORMAL = 4,
    NORMAL_WT = 5
};

/* Stage-2 translation memory attributes */
enum mair_s2_types {
    S2_DEVICE_nGnRnE = 0,
    S2_DEVICE_nGnRE = 1,
    S2_DEVICE_nGRE = 2,
    S2_DEVICE_GRE = 3,

    S2_NORMAL_INNER_NC_OUTER_NC = 5,
    S2_NORMAL_INNER_WTC_OUTER_NC = 6,
    S2_NORMAL_INNER_WBC_OUTER_NC = 7,

    S2_NORMAL_INNER_NC_OUTER_WTC = 9,
    S2_NORMAL_INNER_WTC_OUTER_WTC = 10,
    S2_NORMAL_INNER_WBC_OUTER_WTC = 11,

    S2_NORMAL_INNER_NC_OUTER_WBC = 13,
    S2_NORMAL_INNER_WTC_OUTER_WBC = 14,
    S2_NORMAL_INNER_WBC_OUTER_WBC = 15,

    S2_NORMAL = S2_NORMAL_INNER_WBC_OUTER_WBC
};

/* Leif from Linaro said the big.LITTLE clusters should be treated as
 * inner shareable, and we believe so, although the Example B2-1 given in
 * ARM ARM DDI 0487B.b (ID092517) says otherwise.
 */



struct lookupPTSlot_ret {
    pte_t *ptSlot;
    word_t ptBitsLeft;
};
typedef struct lookupPTSlot_ret lookupPTSlot_ret_t;

struct findVSpaceForASID_ret {
    exception_t status;
    vspace_root_t *vspace_root;
};
typedef struct findVSpaceForASID_ret findVSpaceForASID_ret_t;

/* Stage-1 access permissions:
 * AP[2:1]  higher EL        EL0
 *   00       rw            None
 *   01       rw            rw
 *   10       r             None
 *   11       r             r
 *
 * Stage-2 access permissions:
 * S2AP    Access from Nonsecure EL1 or Non-secure EL0
 *  00                      None
 *  01                      r
 *  10                      w
 *  11                      rw
 *
 *  For VMs or native seL4 applications, if hypervisor support
 *  is enabled, we use the S2AP. The kernel itself running in
 *  EL2 still uses the Stage-1 AP format.
 */

static word_t __attribute__((__const__)) APFromVMRights(vm_rights_t vm_rights)
{
    switch (vm_rights) {
    case VMKernelOnly:
        if (wrap_config_set(0)) {
            return 0;
        } else {
            return 0;
        }

    case VMReadWrite:
        if (wrap_config_set(0)) {
            return 3;
        } else {
            return 1;
        }

    case VMReadOnly:
        if (wrap_config_set(0)) {
            return 1;
        } else {
            return 3;
        }

    default:
        _fail("Invalid VM rights", "/home/b/test/sel4test/kernel/src/arch/arm/64/kernel/vspace.c", 134, __func__);
    }
}


static inline __attribute__((__const__)) word_t pte_get_AP(pte_t pte)
{
    do { if (!(pte_is_page_type(pte))) { _assert_fail("pte_is_page_type(pte)", "/home/b/test/sel4test/kernel/src/arch/arm/64/kernel/vspace.c", 141, __func__); } } while(0);
    switch (pte_get_pte_type(pte)) {
    case pte_pte_4k_page:
        return pte_pte_4k_page_get_AP(pte);

    case pte_pte_page:
        return pte_pte_page_get_AP(pte);

    default:
        return 0;
    }
}

static vm_rights_t __attribute__((__const__)) vmRightsFromPTE(pte_t pte)
{
    word_t access_perms = pte_get_AP(pte);
    switch (access_perms) {
    case 0:
    case 2:
        return VMKernelOnly;
    case 1:
        return VMReadWrite;
    case 3:
        return VMReadOnly;
    default:
        _fail("Invalid AP bit", "/home/b/test/sel4test/kernel/src/arch/arm/64/kernel/vspace.c", 166, __func__);
    }
}


vm_rights_t __attribute__((__const__)) maskVMRights(vm_rights_t vm_rights, seL4_CapRights_t cap_rights_mask)
{
    if (vm_rights == VMReadOnly &&
        seL4_CapRights_get_capAllowRead(cap_rights_mask)) {
        return VMReadOnly;
    }
    if (vm_rights == VMReadWrite &&
        seL4_CapRights_get_capAllowRead(cap_rights_mask)) {
        if (!seL4_CapRights_get_capAllowWrite(cap_rights_mask)) {
            return VMReadOnly;
        } else {
            return VMReadWrite;
        }
    }
    if (vm_rights == VMReadWrite &&
        !seL4_CapRights_get_capAllowRead(cap_rights_mask) &&
        seL4_CapRights_get_capAllowWrite(cap_rights_mask)) {
        do { printf("\033[0m" "\033[1m" "<<" "\033[0m" "\033[32m" "seL4(CPU %" "lu" ")" "\033[0m" "\033[1m" " [%s/%d T%p \"%s\" @%lx]: " "Attempted to make unsupported write only mapping" ">>" "\033[0m" "\n", 0lu, __func__, 188, ksCurThread, ((debug_tcb_t *)(((cte_t *)((word_t)(ksCurThread)&~((1ul << (12)) - 1ul)))+(tcbCNodeEntries)))->tcbName, (word_t)getRestartPC(ksCurThread)); } while (0);
    }
    return VMKernelOnly;
}

/* ==================== BOOT CODE STARTS HERE ==================== */

/* The 54th bit is defined as UXN (unprivileged execute-never) for stage 1
 * of any translation regime for which stage 1 translation can support
 * two VA ranges. This field applies only to execution at EL0. A value
 * of 0 indicates that this control permits execution.
 *
 * The 54th bit is defined as XN (execute-never) for stage 1 of any translation
 * regime for which the stage 1 translation can support only a singe VA range or
 * stage 2 translation when ARMVv8.2-TTS2UXN is not implemented.
 * This field applies to execution at any exception level to which the stage of
 * translation applies. A value of 0 indicates that this control permits execution.
 *
 * When the kernel is running in EL2, the stage-1 translation only supports one
 * VA range so that the 54th bit is XN. Setting the bit to 0 allows execution.
 *
 */
__attribute__((__section__(".boot.text"))) void map_kernel_frame(paddr_t paddr, pptr_t vaddr, vm_rights_t vm_rights, vm_attributes_t attributes)
{
    do { if (!(vaddr >= 0xffffffffc0000000ul)) { _assert_fail("vaddr >= PPTR_TOP", "/home/b/test/sel4test/kernel/src/arch/arm/64/kernel/vspace.c", 212, __func__); } } while(0);




    word_t uxn = 1; /* unprivileged execute never */

    word_t attr_index;
    word_t shareable;
    if (vm_attributes_get_armPageCacheable(attributes)) {
        attr_index = NORMAL;
        shareable = 0;
    } else {
        attr_index = DEVICE_nGnRnE;
        shareable = 0;
    }
    armKSGlobalKernelPT[(((vaddr) >> (((9) * (((4) - 1) - ((3)))) + 12)) & ((1ul << (9)) - 1ul))] = pte_pte_4k_page_new(uxn, paddr,
                                                                                            0, /* global */
                                                                                            1, /* access flag */
                                                                                            shareable,
                                                                                            APFromVMRights(vm_rights),
                                                                                            attr_index);
}

__attribute__((__section__(".boot.text"))) void map_kernel_window(void)
{

    paddr_t paddr;
    pptr_t vaddr;
    word_t idx;





    /* verify that the kernel window as at the last entry of the PGD */
    do { if (!((((0xffffff8000000000ul) >> (((9) * (((4) - 1) - ((0)))) + 12)) & ((1ul << (9)) - 1ul)) == (1ul << (9)) - 1)) { _assert_fail("GET_KPT_INDEX(PPTR_BASE, KLVL_FRM_ARM_PT_LVL(0)) == BIT(PT_INDEX_BITS) - 1", "/home/b/test/sel4test/kernel/src/arch/arm/64/kernel/vspace.c", 248, __func__); } } while(0);

    do { if (!((!((0xffffff8000000000ul) & ((1ul << (21)) - 1ul))))) { _assert_fail("IS_ALIGNED(PPTR_BASE, seL4_LargePageBits)", "/home/b/test/sel4test/kernel/src/arch/arm/64/kernel/vspace.c", 250, __func__); } } while(0);
    /* verify that the kernel device window is 1gb aligned and 1gb in size */
    do { if (!((((0xffffffffc0000000ul) >> (((9) * (((4) - 1) - ((1)))) + 12)) & ((1ul << (9)) - 1ul)) == (1ul << (9)) - 1)) { _assert_fail("GET_KPT_INDEX(PPTR_TOP, KLVL_FRM_ARM_PT_LVL(1)) == BIT(PT_INDEX_BITS) - 1", "/home/b/test/sel4test/kernel/src/arch/arm/64/kernel/vspace.c", 252, __func__); } } while(0);
    do { if (!((!((0xffffffffc0000000ul) & ((1ul << (30)) - 1ul))))) { _assert_fail("IS_ALIGNED(PPTR_TOP, seL4_HugePageBits)", "/home/b/test/sel4test/kernel/src/arch/arm/64/kernel/vspace.c", 253, __func__); } } while(0);

    /* place the PUD into the PGD */
    armKSGlobalKernelPGD[(((0xffffff8000000000ul) >> (((9) * (((4) - 1) - ((0)))) + 12)) & ((1ul << (9)) - 1ul))] = pte_pte_table_new(
                                                                                 addrFromKPPtr(armKSGlobalKernelPUD));

    /* place all PDs except the last one in PUD */
    for (idx = (((0xffffff8000000000ul) >> (((9) * (((4) - 1) - ((1)))) + 12)) & ((1ul << (9)) - 1ul)); idx < (((0xffffffffc0000000ul) >> (((9) * (((4) - 1) - ((1)))) + 12)) & ((1ul << (9)) - 1ul));
         idx++) {
        armKSGlobalKernelPUD[idx] = pte_pte_table_new(
                                        addrFromKPPtr(&armKSGlobalKernelPDs[idx][0])
                                    );
    }

    /* map the kernel window using large pages */
    vaddr = 0xffffff8000000000ul;
    for (paddr = 0x0ul; paddr < (0xffffffffc0000000ul - (0xffffff8000000000ul - 0x0ul)); paddr += (1ul << (21))) {
        armKSGlobalKernelPDs[(((vaddr) >> (((9) * (((4) - 1) - ((1)))) + 12)) & ((1ul << (9)) - 1ul))][(((vaddr) >> (((9) * (((4) - 1) - ((2)))) + 12)) & ((1ul << (9)) - 1ul))
                                                                                                                ] = pte_pte_page_new(



                                                                                                                        1, // UXN

                                                                                                                        paddr,
                                                                                                                        0, /* global */
                                                                                                                        1, /* access flag */
                                                                                                                        0, /* Inner-shareable if SMP enabled, otherwise unshared */
                                                                                                                        0, /* VMKernelOnly */
                                                                                                                        NORMAL
                                                                                                                    );
        vaddr += (1ul << (21));
    }

    /* put the PD into the PUD for device window */
    armKSGlobalKernelPUD[(((0xffffffffc0000000ul) >> (((9) * (((4) - 1) - ((1)))) + 12)) & ((1ul << (9)) - 1ul))] = pte_pte_table_new(
                                                                                addrFromKPPtr(&armKSGlobalKernelPDs[(1ul << (9)) - 1][0])
                                                                            );

    /* put the PT into the PD for device window */
    armKSGlobalKernelPDs[(1ul << (9)) - 1][(1ul << (9)) - 1] = pte_pte_table_new(
                                                                               addrFromKPPtr(armKSGlobalKernelPT)
                                                                           );

    map_kernel_devices();
}

/* When the hypervisor support is enabled, the stage-2 translation table format
 * is used for applications.
 * The global bit is always 0.
 * The memory attributes use the S2 translation values.
 */
static __attribute__((__section__(".boot.text"))) void map_it_frame_cap(cap_t vspace_cap, cap_t frame_cap, bool_t executable)
{
    vspace_root_t *vspaceRoot = ((vspace_root_t *)(((pptr_t)cap_get_capPtr(vspace_cap))));
    pte_t *pud;
    pte_t *pd;
    pte_t *pt;

    vptr_t vptr = cap_frame_cap_get_capFMappedAddress(frame_cap);
    void *pptr = (void *)cap_frame_cap_get_capFBasePtr(frame_cap);

    do { if (!(cap_frame_cap_get_capFMappedASID(frame_cap) != 0)) { _assert_fail("cap_frame_cap_get_capFMappedASID(frame_cap) != 0", "/home/b/test/sel4test/kernel/src/arch/arm/64/kernel/vspace.c", 315, __func__); } } while(0);




    vspaceRoot += (((vptr) >> (((9) * (((4) - 1) - ((0)))) + 12)) & ((1ul << (((0) == 0 ? 9 : 9))) - 1ul));
    do { if (!(pte_pte_table_ptr_get_present(vspaceRoot))) { _assert_fail("pte_pte_table_ptr_get_present(vspaceRoot)", "/home/b/test/sel4test/kernel/src/arch/arm/64/kernel/vspace.c", 321, __func__); } } while(0);
    pud = ptrFromPAddr(pte_pte_table_ptr_get_pt_base_address(vspaceRoot));

    pud += (((vptr) >> (((9) * (((4) - 1) - ((1)))) + 12)) & ((1ul << (((1) == 0 ? 9 : 9))) - 1ul));
    do { if (!(pte_pte_table_ptr_get_present(pud))) { _assert_fail("pte_pte_table_ptr_get_present(pud)", "/home/b/test/sel4test/kernel/src/arch/arm/64/kernel/vspace.c", 325, __func__); } } while(0);
    pd = ptrFromPAddr(pte_pte_table_ptr_get_pt_base_address(pud));
    pd += (((vptr) >> (((9) * (((4) - 1) - ((2)))) + 12)) & ((1ul << (((2) == 0 ? 9 : 9))) - 1ul));
    do { if (!(pte_pte_table_ptr_get_present(pd))) { _assert_fail("pte_pte_table_ptr_get_present(pd)", "/home/b/test/sel4test/kernel/src/arch/arm/64/kernel/vspace.c", 328, __func__); } } while(0);
    pt = ptrFromPAddr(pte_pte_table_ptr_get_pt_base_address(pd));
    *(pt + (((vptr) >> (((9) * (((4) - 1) - ((3)))) + 12)) & ((1ul << (((3) == 0 ? 9 : 9))) - 1ul))) = pte_pte_4k_page_new(
                                                              !executable, /* unprivileged execute never */
                                                              addrFromPPtr(pptr), /* page_base_address    */



                                                              1, /* not global */

                                                              1, /* access flag */
                                                              0, /* Inner-shareable if SMP enabled, otherwise unshared */
                                                              APFromVMRights(VMReadWrite),



                                                              NORMAL

                                                          );
}

static __attribute__((__section__(".boot.text"))) cap_t create_it_frame_cap(pptr_t pptr, vptr_t vptr, asid_t asid, bool_t use_large)
{
    vm_page_size_t frame_size;
    if (use_large) {
        frame_size = ARMLargePage;
    } else {
        frame_size = ARMSmallPage;
    }
    return
        cap_frame_cap_new(
            asid, /* capFMappedASID */
            pptr, /* capFBasePtr */
            frame_size, /* capFSize */
            vptr, /* capFMappedAddress */
            wordFromVMRights(VMReadWrite), /* capFVMRights */
            false /* capFIsDevice */
        );
}

static __attribute__((__section__(".boot.text"))) void map_it_pt_cap(cap_t vspace_cap, cap_t pt_cap)
{
    vspace_root_t *vspaceRoot = ((vspace_root_t *)(((pptr_t)cap_get_capPtr(vspace_cap))));
    pte_t *pud;
    pte_t *pd;
    pte_t *pt = ((pte_t *)(cap_page_table_cap_get_capPTBasePtr(pt_cap)));
    vptr_t vptr = cap_page_table_cap_get_capPTMappedAddress(pt_cap);

    do { if (!(cap_page_table_cap_get_capPTIsMapped(pt_cap))) { _assert_fail("cap_page_table_cap_get_capPTIsMapped(pt_cap)", "/home/b/test/sel4test/kernel/src/arch/arm/64/kernel/vspace.c", 376, __func__); } } while(0);




    vspaceRoot += (((vptr) >> (((9) * (((4) - 1) - ((0)))) + 12)) & ((1ul << (((0) == 0 ? 9 : 9))) - 1ul));
    do { if (!(pte_pte_table_ptr_get_present(vspaceRoot))) { _assert_fail("pte_pte_table_ptr_get_present(vspaceRoot)", "/home/b/test/sel4test/kernel/src/arch/arm/64/kernel/vspace.c", 382, __func__); } } while(0);
    pud = ptrFromPAddr(pte_pte_table_ptr_get_pt_base_address(vspaceRoot));

    pud += (((vptr) >> (((9) * (((4) - 1) - ((1)))) + 12)) & ((1ul << (((1) == 0 ? 9 : 9))) - 1ul));
    do { if (!(pte_pte_table_ptr_get_present(pud))) { _assert_fail("pte_pte_table_ptr_get_present(pud)", "/home/b/test/sel4test/kernel/src/arch/arm/64/kernel/vspace.c", 386, __func__); } } while(0);
    pd = ptrFromPAddr(pte_pte_table_ptr_get_pt_base_address(pud));
    *(pd + (((vptr) >> (((9) * (((4) - 1) - ((2)))) + 12)) & ((1ul << (((2) == 0 ? 9 : 9))) - 1ul))) = pte_pte_table_new(
                                                              addrFromPPtr(pt)
                                                          );
}

static __attribute__((__section__(".boot.text"))) cap_t create_it_pt_cap(cap_t vspace_cap, pptr_t pptr, vptr_t vptr, asid_t asid)
{
    cap_t cap;
    cap = cap_page_table_cap_new(
              asid, /* capPTMappedASID */
              pptr, /* capPTBasePtr */
              1, /* capPTIsMapped */
              vptr /* capPTMappedAddress */
          );
    map_it_pt_cap(vspace_cap, cap);
    return cap;
}

static __attribute__((__section__(".boot.text"))) void map_it_pd_cap(cap_t vspace_cap, cap_t pd_cap)
{
    vspace_root_t *vspaceRoot = ((vspace_root_t *)(((pptr_t)cap_get_capPtr(vspace_cap))));
    pte_t *pud;
    pte_t *pd = ((pte_t *)(cap_page_table_cap_get_capPTBasePtr(pd_cap)));
    vptr_t vptr = cap_page_table_cap_get_capPTMappedAddress(pd_cap);

    do { if (!(cap_page_table_cap_get_capPTIsMapped(pd_cap))) { _assert_fail("cap_page_table_cap_get_capPTIsMapped(pd_cap)", "/home/b/test/sel4test/kernel/src/arch/arm/64/kernel/vspace.c", 413, __func__); } } while(0);




    vspaceRoot += (((vptr) >> (((9) * (((4) - 1) - ((0)))) + 12)) & ((1ul << (((0) == 0 ? 9 : 9))) - 1ul));
    do { if (!(pte_pte_table_ptr_get_present(vspaceRoot))) { _assert_fail("pte_pte_table_ptr_get_present(vspaceRoot)", "/home/b/test/sel4test/kernel/src/arch/arm/64/kernel/vspace.c", 419, __func__); } } while(0);
    pud = ptrFromPAddr(pte_pte_table_ptr_get_pt_base_address(vspaceRoot));

    *(pud + (((vptr) >> (((9) * (((4) - 1) - ((1)))) + 12)) & ((1ul << (((1) == 0 ? 9 : 9))) - 1ul))) = pte_pte_table_new(
                                                               addrFromPPtr(pd)
                                                           );
}

static __attribute__((__section__(".boot.text"))) cap_t create_it_pd_cap(cap_t vspace_cap, pptr_t pptr, vptr_t vptr, asid_t asid)
{
    cap_t cap;
    cap = cap_page_table_cap_new(
              asid, /* capPTMappedASID */
              pptr, /* capPTBasePtr */
              1, /* capPTIsMapped */
              vptr /* capPTMappedAddress */
          );
    map_it_pd_cap(vspace_cap, cap);
    return cap;
}


static __attribute__((__section__(".boot.text"))) void map_it_pud_cap(cap_t vspace_cap, cap_t pud_cap)
{
    pte_t *pgd = ((pte_t *)(((pptr_t)cap_get_capPtr(vspace_cap))));
    pte_t *pud = ((pte_t *)(cap_page_table_cap_get_capPTBasePtr(pud_cap)));
    vptr_t vptr = cap_page_table_cap_get_capPTMappedAddress(pud_cap);

    do { if (!(cap_page_table_cap_get_capPTIsMapped(pud_cap))) { _assert_fail("cap_page_table_cap_get_capPTIsMapped(pud_cap)", "/home/b/test/sel4test/kernel/src/arch/arm/64/kernel/vspace.c", 447, __func__); } } while(0);

    *(pgd + (((vptr) >> (((9) * (((4) - 1) - ((0)))) + 12)) & ((1ul << (((0) == 0 ? 9 : 9))) - 1ul))) = pte_pte_table_new(
                                                               addrFromPPtr(pud));
}

static __attribute__((__section__(".boot.text"))) cap_t create_it_pud_cap(cap_t vspace_cap, pptr_t pptr, vptr_t vptr, asid_t asid)
{
    cap_t cap;
    cap = cap_page_table_cap_new(
              asid, /* capPTMappedASID */
              pptr, /* capPTBasePtr */
              1, /* capPTIsMapped */
              vptr /* capPTMappedAddress */
          );
    map_it_pud_cap(vspace_cap, cap);
    return cap;
}

__attribute__((__section__(".boot.text"))) word_t arch_get_n_paging(v_region_t it_v_reg)
{
    return

        get_n_paging(it_v_reg, (((9) * (((4) - 1) - (((0))))) + 12)) +

        get_n_paging(it_v_reg, (((9) * (((4) - 1) - (((1))))) + 12)) +
        get_n_paging(it_v_reg, (((9) * (((4) - 1) - (((2))))) + 12));
}

__attribute__((__section__(".boot.text"))) cap_t create_it_address_space(cap_t root_cnode_cap, v_region_t it_v_reg)
{
    cap_t vspace_cap;
    vptr_t vptr;
    seL4_SlotPos slot_pos_before;
    seL4_SlotPos slot_pos_after;

    /* create the PGD */
    vspace_cap = cap_vspace_cap_new(
                     1 /* initial thread's ASID */, /* capVSMappedASID */
                     rootserver.vspace, /* capVSBasePtr    */
                     1 /* capVSIsMapped   */



                 );
    slot_pos_before = ndks_boot.slot_pos_cur;
    write_slot((((slot_ptr_t)(((pptr_t)cap_get_capPtr(root_cnode_cap)))) + (seL4_CapInitThreadVSpace)), vspace_cap);


    /* Create any PUDs needed for the user land image */
    for (vptr = (((it_v_reg.start) >> ((((9) * (((4) - 1) - (((0))))) + 12))) << ((((9) * (((4) - 1) - (((0))))) + 12)));
         vptr < it_v_reg.end;
         vptr += (1ul << ((((9) * (((4) - 1) - (((0))))) + 12)))) {
        if (!provide_cap(root_cnode_cap, create_it_pud_cap(vspace_cap, it_alloc_paging(), vptr, 1 /* initial thread's ASID */))) {
            return cap_null_cap_new();
        }
    }

    /* Create any PDs needed for the user land image */
    for (vptr = (((it_v_reg.start) >> ((((9) * (((4) - 1) - (((1))))) + 12))) << ((((9) * (((4) - 1) - (((1))))) + 12)));
         vptr < it_v_reg.end;
         vptr += (1ul << ((((9) * (((4) - 1) - (((1))))) + 12)))) {
        if (!provide_cap(root_cnode_cap, create_it_pd_cap(vspace_cap, it_alloc_paging(), vptr, 1 /* initial thread's ASID */))) {
            return cap_null_cap_new();
        }
    }

    /* Create any PTs needed for the user land image */
    for (vptr = (((it_v_reg.start) >> ((((9) * (((4) - 1) - (((2))))) + 12))) << ((((9) * (((4) - 1) - (((2))))) + 12)));
         vptr < it_v_reg.end;
         vptr += (1ul << ((((9) * (((4) - 1) - (((2))))) + 12)))) {
        if (!provide_cap(root_cnode_cap, create_it_pt_cap(vspace_cap, it_alloc_paging(), vptr, 1 /* initial thread's ASID */))) {
            return cap_null_cap_new();
        }
    }

    slot_pos_after = ndks_boot.slot_pos_cur;
    ndks_boot.bi_frame->userImagePaging = (seL4_SlotRegion) {
        slot_pos_before, slot_pos_after
    };
    return vspace_cap;
}

__attribute__((__section__(".boot.text"))) cap_t create_unmapped_it_frame_cap(pptr_t pptr, bool_t use_large)
{
    return create_it_frame_cap(pptr, 0, asidInvalid, use_large);
}

__attribute__((__section__(".boot.text"))) cap_t create_mapped_it_frame_cap(cap_t pd_cap, pptr_t pptr, vptr_t vptr, asid_t asid, bool_t use_large,
                                           bool_t executable)
{
    cap_t cap = create_it_frame_cap(pptr, vptr, asid, use_large);
    map_it_frame_cap(pd_cap, cap, executable);
    return cap;
}

__attribute__((__section__(".boot.text"))) void activate_kernel_vspace(void)
{
    cleanInvalidateL1Caches();
    setCurrentKernelVSpaceRoot(ttbr_new(0, addrFromKPPtr(armKSGlobalKernelPGD)));

    /* Prevent elf-loader address translation to fill up TLB */
    setCurrentUserVSpaceRoot(ttbr_new(0, addrFromKPPtr(armKSGlobalUserVSpace)));

    invalidateLocalTLB();
    lockTLBEntry(((0xffffff8000000000ul - 0x0ul) + physBase()));
}

__attribute__((__section__(".boot.text"))) void write_it_asid_pool(cap_t it_ap_cap, cap_t it_vspace_cap)
{
    asid_pool_t *ap = ((asid_pool_t*)((pptr_t)cap_get_capPtr(it_ap_cap)));
    asid_map_t asid_map = asid_map_asid_map_vspace_new(




                              /* vspace_root: reference to vspace root page table object */
                              (word_t)cap_vspace_cap_get_capVSBasePtr(it_vspace_cap)




                          );
    ap->array[1 /* initial thread's ASID */] = asid_map;
    armKSASIDTable[1 /* initial thread's ASID */ >> asidLowBits] = ap;
}

/* ==================== BOOT CODE FINISHES HERE ==================== */

asid_map_t findMapForASID(asid_t asid)
{
    asid_pool_t *poolPtr;

    poolPtr = armKSASIDTable[asid >> asidLowBits];
    if (!poolPtr) {
        return asid_map_asid_map_none_new();
    }

    return poolPtr->array[asid & ((1ul << (asidLowBits)) - 1ul)];
}

static findVSpaceForASID_ret_t findVSpaceForASID(asid_t asid)
{
    findVSpaceForASID_ret_t ret;
    asid_map_t asid_map;

    asid_map = findMapForASID(asid);
    if (asid_map_get_type(asid_map) != asid_map_asid_map_vspace) {
        current_lookup_fault = lookup_fault_invalid_root_new();

        ret.vspace_root = ((void *)0);
        ret.status = EXCEPTION_LOOKUP_FAULT;
        return ret;
    }

    ret.vspace_root = (vspace_root_t *)asid_map_asid_map_vspace_get_vspace_root(asid_map);
    ret.status = EXCEPTION_NONE;
    return ret;
}

word_t *__attribute__((__pure__)) lookupIPCBuffer(bool_t isReceiver, tcb_t *thread)
{
    word_t w_bufferPtr;
    cap_t bufferCap;
    vm_rights_t vm_rights;

    w_bufferPtr = thread->tcbIPCBuffer;
    bufferCap = (((cte_t *)((word_t)(thread)&~((1ul << (12)) - 1ul)))+(tcbBuffer))->cap;

    if (__builtin_expect(!!(cap_get_capType(bufferCap) != cap_frame_cap), 0)) {
        return ((void *)0);
    }
    if (__builtin_expect(!!(cap_frame_cap_get_capFIsDevice(bufferCap)), 0)) {
        return ((void *)0);
    }

    vm_rights = cap_frame_cap_get_capFVMRights(bufferCap);
    if (__builtin_expect(!!(vm_rights == VMReadWrite || (!isReceiver && vm_rights == VMReadOnly)), 1)
                                                        ) {
        word_t basePtr;
        unsigned int pageBits;

        basePtr = cap_frame_cap_get_capFBasePtr(bufferCap);
        pageBits = pageBitsForSize(cap_frame_cap_get_capFSize(bufferCap));
        return (word_t *)(basePtr + (w_bufferPtr & ((1ul << (pageBits)) - 1ul)));
    } else {
        return ((void *)0);
    }
}

exception_t checkValidIPCBuffer(vptr_t vptr, cap_t cap)
{
    if (cap_get_capType(cap) != cap_frame_cap) {
        do { printf("\033[0m" "\033[1m" "<<" "\033[0m" "\033[32m" "seL4(CPU %" "lu" ")" "\033[0m" "\033[1m" " [%s/%d T%p \"%s\" @%lx]: " "IPC Buffer is an invalid cap." ">>" "\033[0m" "\n", 0lu, __func__, 640, ksCurThread, ((debug_tcb_t *)(((cte_t *)((word_t)(ksCurThread)&~((1ul << (12)) - 1ul)))+(tcbCNodeEntries)))->tcbName, (word_t)getRestartPC(ksCurThread)); } while (0);
        current_syscall_error.type = seL4_IllegalOperation;
        return EXCEPTION_SYSCALL_ERROR;
    }

    if (__builtin_expect(!!(cap_frame_cap_get_capFIsDevice(cap)), 0)) {
        do { printf("\033[0m" "\033[1m" "<<" "\033[0m" "\033[32m" "seL4(CPU %" "lu" ")" "\033[0m" "\033[1m" " [%s/%d T%p \"%s\" @%lx]: " "Specifying a device frame as an IPC buffer is not permitted." ">>" "\033[0m" "\n", 0lu, __func__, 646, ksCurThread, ((debug_tcb_t *)(((cte_t *)((word_t)(ksCurThread)&~((1ul << (12)) - 1ul)))+(tcbCNodeEntries)))->tcbName, (word_t)getRestartPC(ksCurThread)); } while (0);
        current_syscall_error.type = seL4_IllegalOperation;
        return EXCEPTION_SYSCALL_ERROR;
    }

    if (!(!((vptr) & ((1ul << (10)) - 1ul)))) {
        do { printf("\033[0m" "\033[1m" "<<" "\033[0m" "\033[32m" "seL4(CPU %" "lu" ")" "\033[0m" "\033[1m" " [%s/%d T%p \"%s\" @%lx]: " "IPC Buffer vaddr 0x%x is not aligned." ">>" "\033[0m" "\n", 0lu, __func__, 652, ksCurThread, ((debug_tcb_t *)(((cte_t *)((word_t)(ksCurThread)&~((1ul << (12)) - 1ul)))+(tcbCNodeEntries)))->tcbName, (word_t)getRestartPC(ksCurThread), (int)vptr); } while (0);
        current_syscall_error.type = seL4_AlignmentError;
        return EXCEPTION_SYSCALL_ERROR;
    }

    return EXCEPTION_NONE;
}

static lookupPTSlot_ret_t lookupPTSlot(vspace_root_t *vspace, vptr_t vptr)
{
    lookupPTSlot_ret_t ret;

    word_t level = 4 - 1;
    pte_t *pt = vspace;

    /* this is how many bits we potentially have left to decode. Initially we have the
     * full address space to decode, and every time we walk this will be reduced. The
     * final value of this after the walk is the size of the frame that can be inserted,
     * or already exists, in ret.ptSlot. The following formulation is an invariant of
     * the loop: */
    ret.ptBitsLeft = 9 * level + 12;
    ret.ptSlot = pt + ((vptr >> ret.ptBitsLeft) & ((1ul << (9)) - 1ul));

    while (pte_pte_table_ptr_get_present(ret.ptSlot) && __builtin_expect(!!(level > 0), 1)) {
        level--;
        ret.ptBitsLeft -= 9;
        pt = ptrFromPAddr(pte_pte_table_ptr_get_pt_base_address(ret.ptSlot));
        ret.ptSlot = pt + ((vptr >> ret.ptBitsLeft) & ((1ul << (9)) - 1ul));
    }

    return ret;
}

/* Note that if the hypervisor support is enabled, the user page tables use
 * stage-2 translation format. Otherwise, they follow the stage-1 translation format.
 */
static pte_t makeUserPagePTE(paddr_t paddr, vm_rights_t vm_rights, vm_attributes_t attributes, vm_page_size_t page_size)
{
    bool_t nonexecutable = vm_attributes_get_armExecuteNever(attributes);
    word_t cacheable = vm_attributes_get_armPageCacheable(attributes);





    word_t nG = 1; /* not global */
    word_t attridx = cacheable ? NORMAL : DEVICE_nGnRnE;


    /* Inner-shareable if SMP enabled, otherwise unshared (ignored for devices) */
    word_t shareable = cacheable ? 0 : 0;

    if (page_size == ARMSmallPage) {
        return pte_pte_4k_page_new(nonexecutable, paddr, nG, 1 /* access flag */,
                                   shareable, APFromVMRights(vm_rights), attridx);
    } else {
        return pte_pte_page_new(nonexecutable, paddr, nG, 1 /* access flag */,
                                shareable, APFromVMRights(vm_rights), attridx);
    }
}

exception_t handleVMFault(tcb_t *thread, vm_fault_type_t vm_faultType)
{
    switch (vm_faultType) {
    case ARMDataAbort: {
        word_t addr, fault;

        addr = getFAR();
        fault = getESR();







        current_fault = seL4_Fault_VMFault_new(addr, fault, false);
        return EXCEPTION_FAULT;
    }

    case ARMPrefetchAbort: {
        word_t pc, fault;

        pc = getRestartPC(thread);
        fault = getESR();






        current_fault = seL4_Fault_VMFault_new(pc, fault, true);
        return EXCEPTION_FAULT;
    }

    default:
        _fail("Invalid VM fault type", "/home/b/test/sel4test/kernel/src/arch/arm/64/kernel/vspace.c", 748, __func__);
    }
}

bool_t __attribute__((__const__)) isVTableRoot(cap_t cap)
{
    return cap_get_capType(cap) == cap_vspace_cap;
}

bool_t __attribute__((__const__)) isValidNativeRoot(cap_t cap)
{
    return isVTableRoot(cap) &&
           cap_vspace_cap_get_capVSIsMapped(cap);
}

bool_t __attribute__((__const__)) isValidVTableRoot(cap_t cap)
{
    return isValidNativeRoot(cap);
}

void setVMRoot(tcb_t *tcb)
{
    cap_t threadRoot;
    asid_t asid;
    vspace_root_t *vspaceRoot;
    findVSpaceForASID_ret_t find_ret;

    threadRoot = (((cte_t *)((word_t)(tcb)&~((1ul << (12)) - 1ul)))+(tcbVTable))->cap;

    if (!isValidNativeRoot(threadRoot)) {
        setCurrentUserVSpaceRoot(ttbr_new(0, addrFromKPPtr(armKSGlobalUserVSpace)));
        return;
    }

    vspaceRoot = ((vspace_root_t *)(cap_vspace_cap_get_capVSBasePtr(threadRoot)));
    asid = cap_vspace_cap_get_capVSMappedASID(threadRoot);
    find_ret = findVSpaceForASID(asid);
    if (__builtin_expect(!!(find_ret.status != EXCEPTION_NONE || find_ret.vspace_root != vspaceRoot), 0)) {
        setCurrentUserVSpaceRoot(ttbr_new(0, addrFromKPPtr(armKSGlobalUserVSpace)));
        return;
    }

    armv_contextSwitch(vspaceRoot, asid);
}

static bool_t setVMRootForFlush(vspace_root_t *vspace, asid_t asid)
{
    cap_t threadRoot;

    threadRoot = (((cte_t *)((word_t)(ksCurThread)&~((1ul << (12)) - 1ul)))+(tcbVTable))->cap;

    if (cap_get_capType(threadRoot) == cap_vspace_cap &&
        cap_vspace_cap_get_capVSIsMapped(threadRoot) &&
        ((vspace_root_t *)(cap_vspace_cap_get_capVSBasePtr(threadRoot))) == vspace) {
        return false;
    }

    armv_contextSwitch(vspace, asid);
    return true;
}
# 961 "/home/b/test/sel4test/kernel/src/arch/arm/64/kernel/vspace.c"
static inline void invalidateTLBByASID(asid_t asid)
{
# 978 "/home/b/test/sel4test/kernel/src/arch/arm/64/kernel/vspace.c"
    invalidateTranslationASID(asid);

}

static inline void invalidateTLBByASIDVA(asid_t asid, vptr_t vaddr)
{
# 1000 "/home/b/test/sel4test/kernel/src/arch/arm/64/kernel/vspace.c"
    invalidateTranslationSingle((asid << 48) | vaddr >> 12);

}


void unmapPageTable(asid_t asid, vptr_t vptr, pte_t *target_pt)
{
    findVSpaceForASID_ret_t find_ret = findVSpaceForASID(asid);
    if (__builtin_expect(!!(find_ret.status != EXCEPTION_NONE), 0)) {
        /* nothing to do */
        return;
    }
    pte_t *ptSlot = ((void *)0);
    pte_t *pt = (pte_t *)find_ret.vspace_root;

    for (word_t i = 0; i < 4 - 1 && pt != target_pt; i++) {
        ptSlot = pt + (((vptr) >> (((9) * (((4) - 1) - (i))) + 12)) & ((1ul << ((i == 0 ? 9 : 9))) - 1ul));
        if (__builtin_expect(!!(!pte_pte_table_ptr_get_present(ptSlot)), 0)) {
            /* couldn't find it */
            return;
        }
        pt = ptrFromPAddr(pte_pte_table_ptr_get_pt_base_address(ptSlot));
    }

    if (pt != target_pt) {
        /* didn't find it */
        return;
    }
    /* If we found a pt then ptSlot won't be null */
    do { if (!(ptSlot != ((void *)0))) { _assert_fail("ptSlot != NULL", "/home/b/test/sel4test/kernel/src/arch/arm/64/kernel/vspace.c", 1029, __func__); } } while(0);
    *ptSlot = pte_pte_invalid_new();
    cleanByVA_PoU((vptr_t)ptSlot, addrFromPPtr(ptSlot));
    invalidateTLBByASID(asid);
}

void unmapPage(vm_page_size_t page_size, asid_t asid, vptr_t vptr, pptr_t pptr)
{
    findVSpaceForASID_ret_t find_ret;
    lookupPTSlot_ret_t lu_ret;
    pte_t pte;

    find_ret = findVSpaceForASID(asid);
    if (find_ret.status != EXCEPTION_NONE) {
        return;
    }

    lu_ret = lookupPTSlot(find_ret.vspace_root, vptr);
    if (__builtin_expect(!!(lu_ret.ptBitsLeft != pageBitsForSize(page_size)), 0)) {
        /* Do nothing if the wrong size object was returned */
        return;
    }

    pte = *(lu_ret.ptSlot);
    if (!pte_is_page_type(pte)) {
        /* Do nothing if no page is present */
        return;
    }

    if (pte_get_page_base_address(pte) != addrFromPPtr((void *)pptr)) {
        /* Do nothing if the mapped page is not the same physical frame */
        return;
    }

    *(lu_ret.ptSlot) = pte_pte_invalid_new();
    cleanByVA_PoU((vptr_t)lu_ret.ptSlot, addrFromPPtr(lu_ret.ptSlot));
    do { if (!(asid < (1ul << (16)))) { _assert_fail("asid < BIT(16)", "/home/b/test/sel4test/kernel/src/arch/arm/64/kernel/vspace.c", 1065, __func__); } } while(0);
    invalidateTLBByASIDVA(asid, vptr);
}

void deleteASID(asid_t asid, vspace_root_t *vspace)
{
    asid_pool_t *poolPtr;

    poolPtr = armKSASIDTable[asid >> asidLowBits];

    if (poolPtr != ((void *)0)) {
        asid_map_t asid_map = poolPtr->array[asid & ((1ul << (asidLowBits)) - 1ul)];
        if (asid_map_get_type(asid_map) == asid_map_asid_map_vspace &&
            (vspace_root_t *)asid_map_asid_map_vspace_get_vspace_root(asid_map) == vspace) {
            invalidateTLBByASID(asid);



            poolPtr->array[asid & ((1ul << (asidLowBits)) - 1ul)] = asid_map_asid_map_none_new();
            setVMRoot(ksCurThread);
        }
    }
}

void deleteASIDPool(asid_t asid_base, asid_pool_t *pool)
{
    word_t offset;

    do { if (!((asid_base & ((1ul << (asidLowBits)) - 1ul)) == 0)) { _assert_fail("(asid_base & MASK(asidLowBits)) == 0", "/home/b/test/sel4test/kernel/src/arch/arm/64/kernel/vspace.c", 1093, __func__); } } while(0);

    if (armKSASIDTable[asid_base >> asidLowBits] == pool) {
        for (offset = 0; offset < (1ul << (asidLowBits)); offset++) {
            asid_map_t asid_map = pool->array[offset];
            if (asid_map_get_type(asid_map) == asid_map_asid_map_vspace) {
                invalidateTLBByASID(asid_base + offset);



            }
        }
        armKSASIDTable[asid_base >> asidLowBits] = ((void *)0);
        setVMRoot(ksCurThread);
    }
}

static void doFlush(word_t invLabel, vptr_t start, vptr_t end, paddr_t pstart)
{
    switch (invLabel) {
    case ARMVSpaceClean_Data:
    case ARMPageClean_Data:
        cleanCacheRange_RAM(start, end, pstart);
        break;

    case ARMVSpaceInvalidate_Data:
    case ARMPageInvalidate_Data:
        invalidateCacheRange_RAM(start, end, pstart);
        break;

    case ARMVSpaceCleanInvalidate_Data:
    case ARMPageCleanInvalidate_Data:
        cleanInvalidateCacheRange_RAM(start, end, pstart);
        break;

    case ARMVSpaceUnify_Instruction:
    case ARMPageUnify_Instruction:
        /* First clean data lines to point of unification... */
        cleanCacheRange_PoU(start, end, pstart);
        /* Ensure it's been written. */
        dsb();
        /* ...then invalidate the corresponding instruction lines
           to point of unification... */
        invalidateCacheRange_I(start, end, pstart);
        /* ... and ensure new instructions come from fresh cache lines. */
        isb();
        break;
    default:
        _fail("Invalid operation, shouldn't get here.\n", "/home/b/test/sel4test/kernel/src/arch/arm/64/kernel/vspace.c", 1141, __func__);
    }
}

/* ================= INVOCATION HANDLING STARTS HERE ================== */

static exception_t performVSpaceFlush(word_t invLabel, vspace_root_t *vspaceRoot, asid_t asid,
                                      vptr_t start, vptr_t end, paddr_t pstart)
{

    if (wrap_config_set(0)) {
        word_t size = end - start;
        start = (vptr_t)ptrFromPAddr(pstart);
        end = start + size;
        if (start < end) {
            doFlush(invLabel, start, end, pstart);
        }
    } else {
        bool_t root_switched;

        /* Flush if given a non zero range */
        if (start < end) {
            root_switched = setVMRootForFlush(vspaceRoot, asid);
            doFlush(invLabel, start, end, pstart);
            if (root_switched) {
                setVMRoot(ksCurThread);
            }
        }
    }
    return EXCEPTION_NONE;
}


static exception_t performPageTableInvocationMap(cap_t cap, cte_t *ctSlot, pte_t pte, pte_t *ptSlot)
{
    ctSlot->cap = cap;
    *ptSlot = pte;
    cleanByVA_PoU((vptr_t)ptSlot, addrFromPPtr(ptSlot));

    return EXCEPTION_NONE;
}

static exception_t performPageTableInvocationUnmap(cap_t cap, cte_t *ctSlot)
{
    if (cap_page_table_cap_get_capPTIsMapped(cap)) {
        pte_t *pt = ((pte_t *)(cap_page_table_cap_get_capPTBasePtr(cap)));
        unmapPageTable(cap_page_table_cap_get_capPTMappedASID(cap),
                       cap_page_table_cap_get_capPTMappedAddress(cap), pt);
        clearMemory_PT((void *)pt, cap_get_capSizeBits(cap));
    }

    cap_page_table_cap_ptr_set_capPTIsMapped(&(ctSlot->cap), 0);
    return EXCEPTION_NONE;
}

static exception_t performPageInvocationMap(asid_t asid, cap_t cap, cte_t *ctSlot,
                                            pte_t pte, pte_t *ptSlot)
{
    bool_t tlbflush_required = pte_ptr_get_valid(ptSlot);

    ctSlot->cap = cap;
    *ptSlot = pte;

    cleanByVA_PoU((vptr_t)ptSlot, addrFromPPtr(ptSlot));
    if (__builtin_expect(!!(tlbflush_required), 0)) {
        do { if (!(asid < (1ul << (16)))) { _assert_fail("asid < BIT(16)", "/home/b/test/sel4test/kernel/src/arch/arm/64/kernel/vspace.c", 1206, __func__); } } while(0);
        invalidateTLBByASIDVA(asid, cap_frame_cap_get_capFMappedAddress(cap));
    }

    return EXCEPTION_NONE;
}

static exception_t performPageInvocationUnmap(cap_t cap, cte_t *ctSlot)
{
    if (cap_frame_cap_get_capFMappedASID(cap) != 0) {

        unmapPage(cap_frame_cap_get_capFSize(cap),
                  cap_frame_cap_get_capFMappedASID(cap),
                  cap_frame_cap_get_capFMappedAddress(cap),
                  cap_frame_cap_get_capFBasePtr(cap));
    }

    cap_t slotCap = ctSlot->cap;
    slotCap = cap_frame_cap_set_capFMappedAddress(slotCap, 0);
    slotCap = cap_frame_cap_set_capFMappedASID(slotCap, asidInvalid);
    ctSlot->cap = slotCap;


    return EXCEPTION_NONE;
}

static exception_t performPageFlush(word_t invLabel, vspace_root_t *vspaceRoot, asid_t asid,
                                    vptr_t start, vptr_t end, paddr_t pstart)
{
    if (wrap_config_set(0)) {
        /* We flush the cache with kernel virtual addresses since
         * the user virtual addresses are not valid in EL2.
         * Switching VMRoot is not required.
         */
        word_t size = end - start;
        start = (vptr_t)ptrFromPAddr(pstart);
        end = start + size;

        if (start < end) {
            doFlush(invLabel, start, end, pstart);
        }
    } else {
        bool_t root_switched;

        if (start < end) {
            root_switched = setVMRootForFlush(vspaceRoot, asid);
            doFlush(invLabel, start, end, pstart);
            if (root_switched) {
                setVMRoot(ksCurThread);
            }
        }
    }
    return EXCEPTION_NONE;
}

static exception_t performPageGetAddress(pptr_t base_ptr, bool_t call)
{
    paddr_t base = addrFromPPtr((void *)base_ptr);

    tcb_t *thread;
    thread = ksCurThread;
    if (call) {
        word_t *ipcBuffer = lookupIPCBuffer(true, thread);
        setRegister(thread, badgeRegister, 0);
        unsigned int length = setMR(thread, ipcBuffer, 0, base);
        setRegister(thread, msgInfoRegister, wordFromMessageInfo(
                        seL4_MessageInfo_new(0, 0, 0, length)));
    }
    setThreadState(ksCurThread, ThreadState_Running);
    return EXCEPTION_NONE;
}

static exception_t performASIDControlInvocation(void *frame, cte_t *slot,
                                                cte_t *parent, asid_t asid_base)
{
    /** AUXUPD: "(True, typ_region_bytes (ptr_val \<acute>frame) 12)" */
    /** GHOSTUPD: "(True, gs_clear_region (ptr_val \<acute>frame) 12)" */
    cap_untyped_cap_ptr_set_capFreeIndex(&(parent->cap),
                                         ((1ul << ((cap_untyped_cap_get_capBlockSize(parent->cap)) - 4))));

    memzero(frame, (1ul << (12)));
    /** AUXUPD: "(True, ptr_retyps 1 (Ptr (ptr_val \<acute>frame) :: asid_pool_C ptr))" */

    cteInsert(
        cap_asid_pool_cap_new(
            asid_base, /* capASIDBase  */
            ((word_t)(frame)) /* capASIDPool  */
        ), parent, slot);

    do { if (!((asid_base & ((1ul << (asidLowBits)) - 1ul)) == 0)) { _assert_fail("(asid_base & MASK(asidLowBits)) == 0", "/home/b/test/sel4test/kernel/src/arch/arm/64/kernel/vspace.c", 1295, __func__); } } while(0);
    armKSASIDTable[asid_base >> asidLowBits] = (asid_pool_t *)frame;

    return EXCEPTION_NONE;
}

static exception_t decodeARMVSpaceRootInvocation(word_t invLabel, word_t length,
                                                 cte_t *cte, cap_t cap, word_t *buffer)
{
    vptr_t start, end;
    paddr_t pstart;
    asid_t asid;
    vspace_root_t *vspaceRoot;
    lookupPTSlot_ret_t resolve_ret;
    findVSpaceForASID_ret_t find_ret;
    pte_t pte;

    switch (invLabel) {
    case ARMVSpaceClean_Data:
    case ARMVSpaceInvalidate_Data:
    case ARMVSpaceCleanInvalidate_Data:
    case ARMVSpaceUnify_Instruction:

        if (length < 2) {
            do { printf("\033[0m" "\033[1m" "<<" "\033[0m" "\033[32m" "seL4(CPU %" "lu" ")" "\033[0m" "\033[1m" " [%s/%d T%p \"%s\" @%lx]: " "VSpaceRoot Flush: Truncated message." ">>" "\033[0m" "\n", 0lu, __func__, 1319, ksCurThread, ((debug_tcb_t *)(((cte_t *)((word_t)(ksCurThread)&~((1ul << (12)) - 1ul)))+(tcbCNodeEntries)))->tcbName, (word_t)getRestartPC(ksCurThread)); } while (0);
            current_syscall_error.type = seL4_TruncatedMessage;
            return EXCEPTION_SYSCALL_ERROR;
        }

        start = getSyscallArg(0, buffer);
        end = getSyscallArg(1, buffer);

        /* Check sanity of arguments */
        if (end <= start) {
            do { printf("\033[0m" "\033[1m" "<<" "\033[0m" "\033[32m" "seL4(CPU %" "lu" ")" "\033[0m" "\033[1m" " [%s/%d T%p \"%s\" @%lx]: " "VSpaceRoot Flush: Invalid range." ">>" "\033[0m" "\n", 0lu, __func__, 1329, ksCurThread, ((debug_tcb_t *)(((cte_t *)((word_t)(ksCurThread)&~((1ul << (12)) - 1ul)))+(tcbCNodeEntries)))->tcbName, (word_t)getRestartPC(ksCurThread)); } while (0);
            current_syscall_error.type = seL4_InvalidArgument;
            current_syscall_error.invalidArgumentNumber = 1;
            return EXCEPTION_SYSCALL_ERROR;
        }

        /* Don't let applications flush kernel regions. */
        if (end > 0x00007fffffffffff) {
            do { printf("\033[0m" "\033[1m" "<<" "\033[0m" "\033[32m" "seL4(CPU %" "lu" ")" "\033[0m" "\033[1m" " [%s/%d T%p \"%s\" @%lx]: " "VSpaceRoot Flush: Exceed the user addressable region." ">>" "\033[0m" "\n", 0lu, __func__, 1337, ksCurThread, ((debug_tcb_t *)(((cte_t *)((word_t)(ksCurThread)&~((1ul << (12)) - 1ul)))+(tcbCNodeEntries)))->tcbName, (word_t)getRestartPC(ksCurThread)); } while (0);
            current_syscall_error.type = seL4_IllegalOperation;
            return EXCEPTION_SYSCALL_ERROR;
        }

        if (__builtin_expect(!!(!isValidNativeRoot(cap)), 0)) {
            current_syscall_error.type = seL4_InvalidCapability;
            current_syscall_error.invalidCapNumber = 0;
            return EXCEPTION_SYSCALL_ERROR;
        }

        /* Make sure that the supplied pgd is ok */
        vspaceRoot = ((vspace_root_t *)(cap_vspace_cap_get_capVSBasePtr(cap)));
        asid = cap_vspace_cap_get_capVSMappedASID(cap);

        find_ret = findVSpaceForASID(asid);
        if (__builtin_expect(!!(find_ret.status != EXCEPTION_NONE), 0)) {
            do { printf("\033[0m" "\033[1m" "<<" "\033[0m" "\033[32m" "seL4(CPU %" "lu" ")" "\033[0m" "\033[1m" " [%s/%d T%p \"%s\" @%lx]: " "VSpaceRoot Flush: No VSpace for ASID" ">>" "\033[0m" "\n", 0lu, __func__, 1354, ksCurThread, ((debug_tcb_t *)(((cte_t *)((word_t)(ksCurThread)&~((1ul << (12)) - 1ul)))+(tcbCNodeEntries)))->tcbName, (word_t)getRestartPC(ksCurThread)); } while (0);
            current_syscall_error.type = seL4_FailedLookup;
            current_syscall_error.failedLookupWasSource = false;
            return EXCEPTION_SYSCALL_ERROR;
        }

        if (__builtin_expect(!!(find_ret.vspace_root != vspaceRoot), 0)) {
            do { printf("\033[0m" "\033[1m" "<<" "\033[0m" "\033[32m" "seL4(CPU %" "lu" ")" "\033[0m" "\033[1m" " [%s/%d T%p \"%s\" @%lx]: " "VSpaceRoot Flush: Invalid VSpace Cap" ">>" "\033[0m" "\n", 0lu, __func__, 1361, ksCurThread, ((debug_tcb_t *)(((cte_t *)((word_t)(ksCurThread)&~((1ul << (12)) - 1ul)))+(tcbCNodeEntries)))->tcbName, (word_t)getRestartPC(ksCurThread)); } while (0);
            current_syscall_error.type = seL4_InvalidCapability;
            current_syscall_error.invalidCapNumber = 0;
            return EXCEPTION_SYSCALL_ERROR;
        }

        /* Look up the frame containing 'start'. */
        resolve_ret = lookupPTSlot(vspaceRoot, start);
        pte = *resolve_ret.ptSlot;

        /* Check that the returned slot is a page. */
        if (!pte_is_page_type(pte)) {

            /* Fail silently, as there can't be any stale cached data (for the
             * given address space), and getting a syscall error because the
             * relevant page is non-resident would be 'astonishing'. */
            setThreadState(ksCurThread, ThreadState_Restart);
            return EXCEPTION_NONE;
        }

        /* Refuse to cross a page boundary. */
        if ((((start) >> (resolve_ret.ptBitsLeft)) << (resolve_ret.ptBitsLeft)) != (((end - 1) >> (resolve_ret.ptBitsLeft)) << (resolve_ret.ptBitsLeft))) {
            current_syscall_error.type = seL4_RangeError;
            current_syscall_error.rangeErrorMin = start;
            current_syscall_error.rangeErrorMax = (((start) >> (resolve_ret.ptBitsLeft)) << (resolve_ret.ptBitsLeft)) +
                                                  ((1ul << (resolve_ret.ptBitsLeft)) - 1ul);
            do { printf("\033[0m" "\033[1m" "<<" "\033[0m" "\033[32m" "seL4(CPU %" "lu" ")" "\033[0m" "\033[1m" " [%s/%d T%p \"%s\" @%lx]: " "VSpaceRoot Flush: cannot cross page boundary, valid range is [0x%lx..0x%lx)" ">>" "\033[0m" "\n", 0lu, __func__, 1387, ksCurThread, ((debug_tcb_t *)(((cte_t *)((word_t)(ksCurThread)&~((1ul << (12)) - 1ul)))+(tcbCNodeEntries)))->tcbName, (word_t)getRestartPC(ksCurThread), current_syscall_error.rangeErrorMin, current_syscall_error.rangeErrorMax); } while (0)
                                                                                               ;
            return EXCEPTION_SYSCALL_ERROR;
        }


        /* When in EL1, the mapping must be write-able for ARMVSpaceInvalidate_Data */
        if (invLabel == ARMVSpaceInvalidate_Data && vmRightsFromPTE(pte) != VMReadWrite) {
            do { printf("\033[0m" "\033[1m" "<<" "\033[0m" "\033[32m" "seL4(CPU %" "lu" ")" "\033[0m" "\033[1m" " [%s/%d T%p \"%s\" @%lx]: " "ARMVSpaceInvalidate_Data: Cannot call on mapping without write rights." ">>" "\033[0m" "\n", 0lu, __func__, 1395, ksCurThread, ((debug_tcb_t *)(((cte_t *)((word_t)(ksCurThread)&~((1ul << (12)) - 1ul)))+(tcbCNodeEntries)))->tcbName, (word_t)getRestartPC(ksCurThread)); } while (0);
            current_syscall_error.type = seL4_IllegalOperation;
            return EXCEPTION_SYSCALL_ERROR;
        }


        /* Calculate the physical start address. */
        paddr_t frame_base = pte_get_page_base_address(pte);

        pstart = frame_base + (start & ((1ul << (resolve_ret.ptBitsLeft)) - 1ul));

        setThreadState(ksCurThread, ThreadState_Restart);
        return performVSpaceFlush(invLabel, vspaceRoot, asid, start, end - 1, pstart);

    default:
        current_syscall_error.type = seL4_IllegalOperation;
        return EXCEPTION_SYSCALL_ERROR;
    }
}


static exception_t decodeARMPageTableInvocation(word_t invLabel, word_t length,
                                                cte_t *cte, cap_t cap, word_t *buffer)
{
    cap_t vspaceRootCap;
    vspace_root_t *vspaceRoot;
    pte_t pte;
    asid_t asid;
    vptr_t vaddr;
    lookupPTSlot_ret_t ptSlot;
    findVSpaceForASID_ret_t find_ret;

    if (invLabel == ARMPageTableUnmap) {
        if (__builtin_expect(!!(!isFinalCapability(cte)), 0)) {
            current_syscall_error.type = seL4_RevokeFirst;
            return EXCEPTION_SYSCALL_ERROR;
        }

        setThreadState(ksCurThread, ThreadState_Restart);
        return performPageTableInvocationUnmap(cap, cte);
    }

    if (__builtin_expect(!!(invLabel != ARMPageTableMap), 0)) {
        current_syscall_error.type = seL4_IllegalOperation;
        return EXCEPTION_SYSCALL_ERROR;
    }

    if (__builtin_expect(!!(length < 2 || current_extra_caps.excaprefs[0] == ((void *)0)), 0)) {
        current_syscall_error.type = seL4_TruncatedMessage;
        return EXCEPTION_SYSCALL_ERROR;
    }

    if (__builtin_expect(!!(cap_page_table_cap_get_capPTIsMapped(cap)), 0)) {
        current_syscall_error.type = seL4_InvalidCapability;
        current_syscall_error.invalidCapNumber = 0;
        return EXCEPTION_SYSCALL_ERROR;
    }

    vaddr = getSyscallArg(0, buffer);
    vspaceRootCap = current_extra_caps.excaprefs[0]->cap;

    if (__builtin_expect(!!(!isValidNativeRoot(vspaceRootCap)), 0)) {
        current_syscall_error.type = seL4_InvalidCapability;
        current_syscall_error.invalidCapNumber = 1;
        return EXCEPTION_SYSCALL_ERROR;
    }

    vspaceRoot = ((vspace_root_t *)(cap_vspace_cap_get_capVSBasePtr(vspaceRootCap)));
    asid = cap_vspace_cap_get_capVSMappedASID(vspaceRootCap);

    if (__builtin_expect(!!(vaddr > 0x00007fffffffffff), 0)) {
        current_syscall_error.type = seL4_InvalidArgument;
        current_syscall_error.invalidArgumentNumber = 0;
        return EXCEPTION_SYSCALL_ERROR;
    }

    find_ret = findVSpaceForASID(asid);
    if (__builtin_expect(!!(find_ret.status != EXCEPTION_NONE), 0)) {
        current_syscall_error.type = seL4_FailedLookup;
        current_syscall_error.failedLookupWasSource = false;
        return EXCEPTION_SYSCALL_ERROR;
    }

    if (__builtin_expect(!!(find_ret.vspace_root != vspaceRoot), 0)) {
        current_syscall_error.type = seL4_InvalidCapability;
        current_syscall_error.invalidCapNumber = 1;
        return EXCEPTION_SYSCALL_ERROR;
    }

    ptSlot = lookupPTSlot(vspaceRoot, vaddr);

    if (__builtin_expect(!!(ptSlot.ptBitsLeft == 12 || pte_ptr_get_valid(ptSlot.ptSlot)), 0)) {
        current_syscall_error.type = seL4_DeleteFirst;
        return EXCEPTION_SYSCALL_ERROR;
    }

    pte = pte_pte_table_new(addrFromPPtr(((pte_t *)(cap_page_table_cap_get_capPTBasePtr(cap)))));

    cap = cap_page_table_cap_set_capPTIsMapped(cap, 1);
    cap = cap_page_table_cap_set_capPTMappedASID(cap, asid);
    cap = cap_page_table_cap_set_capPTMappedAddress(cap, (vaddr & ~((1ul << (ptSlot.ptBitsLeft)) - 1ul)));

    setThreadState(ksCurThread, ThreadState_Restart);
    return performPageTableInvocationMap(cap, cte, pte, ptSlot.ptSlot);
}

static inline bool_t __attribute__((__const__)) checkVPAlignment(vm_page_size_t sz, word_t w)
{
    return (w & ((1ul << (pageBitsForSize(sz))) - 1ul)) == 0;
}

static exception_t decodeARMFrameInvocation(word_t invLabel, word_t length,
                                            cte_t *cte, cap_t cap, bool_t call, word_t *buffer)
{
    switch (invLabel) {
    case ARMPageMap: {
        vptr_t vaddr;
        paddr_t base;
        cap_t vspaceRootCap;
        vspace_root_t *vspaceRoot;
        asid_t asid, frame_asid;
        vm_rights_t vmRights;
        vm_page_size_t frameSize;
        vm_attributes_t attributes;
        findVSpaceForASID_ret_t find_ret;

        if (__builtin_expect(!!(length < 3 || current_extra_caps.excaprefs[0] == ((void *)0)), 0)) {
            current_syscall_error.type = seL4_TruncatedMessage;
            return EXCEPTION_SYSCALL_ERROR;
        }

        vaddr = getSyscallArg(0, buffer);
        attributes = vmAttributesFromWord(getSyscallArg(2, buffer));
        vspaceRootCap = current_extra_caps.excaprefs[0]->cap;

        frameSize = cap_frame_cap_get_capFSize(cap);
        vmRights = maskVMRights(cap_frame_cap_get_capFVMRights(cap),
                                rightsFromWord(getSyscallArg(1, buffer)));

        if (__builtin_expect(!!(!isValidNativeRoot(vspaceRootCap)), 0)) {
            current_syscall_error.type = seL4_InvalidCapability;
            current_syscall_error.invalidCapNumber = 1;
            return EXCEPTION_SYSCALL_ERROR;
        }

        vspaceRoot = ((vspace_root_t *)(cap_vspace_cap_get_capVSBasePtr(vspaceRootCap)));
        asid = cap_vspace_cap_get_capVSMappedASID(vspaceRootCap);

        find_ret = findVSpaceForASID(asid);
        if (__builtin_expect(!!(find_ret.status != EXCEPTION_NONE), 0)) {
            current_syscall_error.type = seL4_FailedLookup;
            current_syscall_error.failedLookupWasSource = false;
            return EXCEPTION_SYSCALL_ERROR;
        }

        if (__builtin_expect(!!(find_ret.vspace_root != vspaceRoot), 0)) {
            current_syscall_error.type = seL4_InvalidCapability;
            current_syscall_error.invalidCapNumber = 1;
            return EXCEPTION_SYSCALL_ERROR;
        }

        if (__builtin_expect(!!(!checkVPAlignment(frameSize, vaddr)), 0)) {
            current_syscall_error.type = seL4_AlignmentError;
            return EXCEPTION_SYSCALL_ERROR;
        }

        /* In the case of remap, the cap should have a valid asid */
        frame_asid = cap_frame_cap_get_capFMappedASID(cap);

        if (frame_asid != asidInvalid) {
            if (frame_asid != asid) {
                do { printf("\033[0m" "\033[1m" "<<" "\033[0m" "\033[32m" "seL4(CPU %" "lu" ")" "\033[0m" "\033[1m" " [%s/%d T%p \"%s\" @%lx]: " "ARMPageMap: Attempting to remap a frame that does not belong to the passed address space" ">>" "\033[0m" "\n", 0lu, __func__, 1566, ksCurThread, ((debug_tcb_t *)(((cte_t *)((word_t)(ksCurThread)&~((1ul << (12)) - 1ul)))+(tcbCNodeEntries)))->tcbName, (word_t)getRestartPC(ksCurThread)); } while (0);
                current_syscall_error.type = seL4_InvalidCapability;
                current_syscall_error.invalidCapNumber = 1;
                return EXCEPTION_SYSCALL_ERROR;

            } else if (cap_frame_cap_get_capFMappedAddress(cap) != vaddr) {
                do { printf("\033[0m" "\033[1m" "<<" "\033[0m" "\033[32m" "seL4(CPU %" "lu" ")" "\033[0m" "\033[1m" " [%s/%d T%p \"%s\" @%lx]: " "ARMPageMap: Attempting to map frame into multiple addresses" ">>" "\033[0m" "\n", 0lu, __func__, 1572, ksCurThread, ((debug_tcb_t *)(((cte_t *)((word_t)(ksCurThread)&~((1ul << (12)) - 1ul)))+(tcbCNodeEntries)))->tcbName, (word_t)getRestartPC(ksCurThread)); } while (0);
                current_syscall_error.type = seL4_InvalidArgument;
                current_syscall_error.invalidArgumentNumber = 0;
                return EXCEPTION_SYSCALL_ERROR;
            }
        } else {
            if (__builtin_expect(!!(vaddr + (1ul << (pageBitsForSize(frameSize))) - 1 > 0x00007fffffffffff), 0)) {
                current_syscall_error.type = seL4_InvalidArgument;
                current_syscall_error.invalidArgumentNumber = 0;
                return EXCEPTION_SYSCALL_ERROR;
            }
        }

        cap = cap_frame_cap_set_capFMappedASID(cap, asid);
        cap = cap_frame_cap_set_capFMappedAddress(cap, vaddr);

        base = addrFromPPtr((void *)cap_frame_cap_get_capFBasePtr(cap));

        lookupPTSlot_ret_t lu_ret = lookupPTSlot(vspaceRoot, vaddr);
        if (__builtin_expect(!!(lu_ret.ptBitsLeft != pageBitsForSize(frameSize)), 0)) {
            current_lookup_fault = lookup_fault_missing_capability_new(lu_ret.ptBitsLeft);
            current_syscall_error.type = seL4_FailedLookup;
            current_syscall_error.failedLookupWasSource = false;
            return EXCEPTION_SYSCALL_ERROR;
        }

        setThreadState(ksCurThread, ThreadState_Restart);
        return performPageInvocationMap(asid, cap, cte,
                                        makeUserPagePTE(base, vmRights, attributes, frameSize), lu_ret.ptSlot);
    }

    case ARMPageUnmap:
        setThreadState(ksCurThread, ThreadState_Restart);
        return performPageInvocationUnmap(cap, cte);

    case ARMPageClean_Data:
    case ARMPageInvalidate_Data:
    case ARMPageCleanInvalidate_Data:
    case ARMPageUnify_Instruction: {
        vptr_t start, end;
        vptr_t vaddr;
        asid_t asid;
        word_t page_size;
        findVSpaceForASID_ret_t find_ret;

        if (length < 2) {
            do { printf("\033[0m" "\033[1m" "<<" "\033[0m" "\033[32m" "seL4(CPU %" "lu" ")" "\033[0m" "\033[1m" " [%s/%d T%p \"%s\" @%lx]: " "Page Flush: Truncated message." ">>" "\033[0m" "\n", 0lu, __func__, 1618, ksCurThread, ((debug_tcb_t *)(((cte_t *)((word_t)(ksCurThread)&~((1ul << (12)) - 1ul)))+(tcbCNodeEntries)))->tcbName, (word_t)getRestartPC(ksCurThread)); } while (0);
            current_syscall_error.type = seL4_TruncatedMessage;
            return EXCEPTION_SYSCALL_ERROR;
        }

        if (__builtin_expect(!!(cap_frame_cap_get_capFMappedASID(cap) == 0), 0)) {
            do { printf("\033[0m" "\033[1m" "<<" "\033[0m" "\033[32m" "seL4(CPU %" "lu" ")" "\033[0m" "\033[1m" " [%s/%d T%p \"%s\" @%lx]: " "Page Flush: Frame is not mapped." ">>" "\033[0m" "\n", 0lu, __func__, 1624, ksCurThread, ((debug_tcb_t *)(((cte_t *)((word_t)(ksCurThread)&~((1ul << (12)) - 1ul)))+(tcbCNodeEntries)))->tcbName, (word_t)getRestartPC(ksCurThread)); } while (0);
            current_syscall_error.type = seL4_IllegalOperation;
            return EXCEPTION_SYSCALL_ERROR;
        }

        vaddr = cap_frame_cap_get_capFMappedAddress(cap);
        asid = cap_frame_cap_get_capFMappedASID(cap);

        find_ret = findVSpaceForASID(asid);
        if (__builtin_expect(!!(find_ret.status != EXCEPTION_NONE), 0)) {
            do { printf("\033[0m" "\033[1m" "<<" "\033[0m" "\033[32m" "seL4(CPU %" "lu" ")" "\033[0m" "\033[1m" " [%s/%d T%p \"%s\" @%lx]: " "Page Flush: No PGD for ASID" ">>" "\033[0m" "\n", 0lu, __func__, 1634, ksCurThread, ((debug_tcb_t *)(((cte_t *)((word_t)(ksCurThread)&~((1ul << (12)) - 1ul)))+(tcbCNodeEntries)))->tcbName, (word_t)getRestartPC(ksCurThread)); } while (0);
            current_syscall_error.type = seL4_FailedLookup;
            current_syscall_error.failedLookupWasSource = false;
            return EXCEPTION_SYSCALL_ERROR;
        }

        start = getSyscallArg(0, buffer);
        end = getSyscallArg(1, buffer);

        /* check that the range is sane */
        if (end <= start) {
            do { printf("\033[0m" "\033[1m" "<<" "\033[0m" "\033[32m" "seL4(CPU %" "lu" ")" "\033[0m" "\033[1m" " [%s/%d T%p \"%s\" @%lx]: " "PageFlush: Invalid range" ">>" "\033[0m" "\n", 0lu, __func__, 1645, ksCurThread, ((debug_tcb_t *)(((cte_t *)((word_t)(ksCurThread)&~((1ul << (12)) - 1ul)))+(tcbCNodeEntries)))->tcbName, (word_t)getRestartPC(ksCurThread)); } while (0);
            current_syscall_error.type = seL4_InvalidArgument;
            current_syscall_error.invalidArgumentNumber = 1;
            return EXCEPTION_SYSCALL_ERROR;
        }

        /* start and end are currently relative inside this page */
        page_size = (1ul << (pageBitsForSize(cap_frame_cap_get_capFSize(cap))));
        if (start >= page_size || end > page_size) {
            do { printf("\033[0m" "\033[1m" "<<" "\033[0m" "\033[32m" "seL4(CPU %" "lu" ")" "\033[0m" "\033[1m" " [%s/%d T%p \"%s\" @%lx]: " "Page Flush: Requested range not inside page" ">>" "\033[0m" "\n", 0lu, __func__, 1654, ksCurThread, ((debug_tcb_t *)(((cte_t *)((word_t)(ksCurThread)&~((1ul << (12)) - 1ul)))+(tcbCNodeEntries)))->tcbName, (word_t)getRestartPC(ksCurThread)); } while (0);
            current_syscall_error.type = seL4_InvalidArgument;
            current_syscall_error.invalidArgumentNumber = 0;
            return EXCEPTION_SYSCALL_ERROR;
        }

        word_t pstart = addrFromPPtr((void *)cap_frame_cap_get_capFBasePtr(cap)) + start;
# 1669 "/home/b/test/sel4test/kernel/src/arch/arm/64/kernel/vspace.c"
        /* When in EL1, we are using the user page table for flushing and need to make sure
           the mapping info in the cap is not stale. */
        lookupPTSlot_ret_t lu_ret = lookupPTSlot(find_ret.vspace_root, vaddr);
        pte_t pte = *lu_ret.ptSlot;
        void *base_ptr = (void *) cap_frame_cap_get_capFBasePtr(cap);
        if (__builtin_expect(!!(lu_ret.ptBitsLeft != pageBitsForSize(cap_frame_cap_get_capFSize(cap)) || !pte_is_page_type(pte) || pte_get_page_base_address(pte) != addrFromPPtr(base_ptr)), 0)

                                                                               ) {
            do { printf("\033[0m" "\033[1m" "<<" "\033[0m" "\033[32m" "seL4(CPU %" "lu" ")" "\033[0m" "\033[1m" " [%s/%d T%p \"%s\" @%lx]: " "Page Flush: Attempting to use cap with stale mapping information." ">>" "\033[0m" "\n", 0lu, __func__, 1677, ksCurThread, ((debug_tcb_t *)(((cte_t *)((word_t)(ksCurThread)&~((1ul << (12)) - 1ul)))+(tcbCNodeEntries)))->tcbName, (word_t)getRestartPC(ksCurThread)); } while (0);
            current_syscall_error.type = seL4_InvalidCapability;
            current_syscall_error.invalidCapNumber = 0;
            return EXCEPTION_SYSCALL_ERROR;
        }

        /* When in EL1, the mapping must be writeable for DC IVAC */
        if (invLabel == ARMPageInvalidate_Data && vmRightsFromPTE(pte) != VMReadWrite) {
            do { printf("\033[0m" "\033[1m" "<<" "\033[0m" "\033[32m" "seL4(CPU %" "lu" ")" "\033[0m" "\033[1m" " [%s/%d T%p \"%s\" @%lx]: " "ARMPageInvalidate_Data: Cannot call on mapping without write rights." ">>" "\033[0m" "\n", 0lu, __func__, 1685, ksCurThread, ((debug_tcb_t *)(((cte_t *)((word_t)(ksCurThread)&~((1ul << (12)) - 1ul)))+(tcbCNodeEntries)))->tcbName, (word_t)getRestartPC(ksCurThread)); } while (0);
            current_syscall_error.type = seL4_IllegalOperation;
            return EXCEPTION_SYSCALL_ERROR;
        }

        setThreadState(ksCurThread, ThreadState_Restart);
        return performPageFlush(invLabel, find_ret.vspace_root, asid, vaddr + start, vaddr + end - 1,
                                pstart);
    }

    case ARMPageGetAddress:
        setThreadState(ksCurThread, ThreadState_Restart);
        return performPageGetAddress(cap_frame_cap_get_capFBasePtr(cap), call);

    default:
        current_syscall_error.type = seL4_IllegalOperation;
        return EXCEPTION_SYSCALL_ERROR;
    }
}

exception_t decodeARMMMUInvocation(word_t invLabel, word_t length, cptr_t cptr,
                                   cte_t *cte, cap_t cap, bool_t call, word_t *buffer)
{
    switch (cap_get_capType(cap)) {
    case cap_vspace_cap:
        return decodeARMVSpaceRootInvocation(invLabel, length, cte, cap, buffer);
    case cap_page_table_cap:
        return decodeARMPageTableInvocation(invLabel, length, cte, cap, buffer);

    case cap_frame_cap:
        return decodeARMFrameInvocation(invLabel, length, cte, cap, call, buffer);

    case cap_asid_control_cap: {
        word_t i;
        asid_t asid_base;
        word_t index, depth;
        cap_t untyped, root;
        cte_t *parentSlot, *destSlot;
        lookupSlot_ret_t lu_ret;
        void *frame;
        exception_t status;

        if (__builtin_expect(!!(invLabel != ARMASIDControlMakePool), 0)) {
            current_syscall_error.type = seL4_IllegalOperation;

            return EXCEPTION_SYSCALL_ERROR;
        }

        if (__builtin_expect(!!(length < 2 || current_extra_caps.excaprefs[0] == ((void *)0) || current_extra_caps.excaprefs[1] == ((void *)0)), 0)

                                                             ) {
            current_syscall_error.type = seL4_TruncatedMessage;

            return EXCEPTION_SYSCALL_ERROR;
        }

        index = getSyscallArg(0, buffer);
        depth = getSyscallArg(1, buffer);
        parentSlot = current_extra_caps.excaprefs[0];
        untyped = parentSlot->cap;
        root = current_extra_caps.excaprefs[1]->cap;

        /* Find first free pool */
        for (i = 0; i < (1ul << (asidHighBits)) && armKSASIDTable[i]; i++);

        if (__builtin_expect(!!(i == (1ul << (asidHighBits))), 0)) {
            do { printf("\033[0m" "\033[1m" "<<" "\033[0m" "\033[32m" "seL4(CPU %" "lu" ")" "\033[0m" "\033[1m" " [%s/%d T%p \"%s\" @%lx]: " "ASIDControlMakePool: No unallocated pools found." ">>" "\033[0m" "\n", 0lu, __func__, 1751, ksCurThread, ((debug_tcb_t *)(((cte_t *)((word_t)(ksCurThread)&~((1ul << (12)) - 1ul)))+(tcbCNodeEntries)))->tcbName, (word_t)getRestartPC(ksCurThread)); } while (0);
            current_syscall_error.type = seL4_DeleteFirst;

            return EXCEPTION_SYSCALL_ERROR;
        }

        asid_base = i << asidLowBits;

        if (__builtin_expect(!!(cap_get_capType(untyped) != cap_untyped_cap || cap_untyped_cap_get_capBlockSize(untyped) != 12 || cap_untyped_cap_get_capIsDevice(untyped)), 0)

                                                              ) {
            current_syscall_error.type = seL4_InvalidCapability;
            current_syscall_error.invalidCapNumber = 1;

            return EXCEPTION_SYSCALL_ERROR;
        }

        status = ensureNoChildren(parentSlot);
        if (__builtin_expect(!!(status != EXCEPTION_NONE), 0)) {
            return status;
        }

        frame = ((word_t *)(cap_untyped_cap_get_capPtr(untyped)));

        lu_ret = lookupTargetSlot(root, index, depth);
        if (__builtin_expect(!!(lu_ret.status != EXCEPTION_NONE), 0)) {
            return lu_ret.status;
        }
        destSlot = lu_ret.slot;

        status = ensureEmptySlot(destSlot);
        if (__builtin_expect(!!(status != EXCEPTION_NONE), 0)) {
            return status;
        }

        setThreadState(ksCurThread, ThreadState_Restart);
        return performASIDControlInvocation(frame, destSlot, parentSlot, asid_base);
    }

    case cap_asid_pool_cap: {
        cap_t vspaceCap;
        cte_t *vspaceCapSlot;
        asid_pool_t *pool;
        word_t i;
        asid_t asid;

        if (__builtin_expect(!!(invLabel != ARMASIDPoolAssign), 0)) {
            current_syscall_error.type = seL4_IllegalOperation;

            return EXCEPTION_SYSCALL_ERROR;
        }

        if (__builtin_expect(!!(current_extra_caps.excaprefs[0] == ((void *)0)), 0)) {
            current_syscall_error.type = seL4_TruncatedMessage;

            return EXCEPTION_SYSCALL_ERROR;
        }

        vspaceCapSlot = current_extra_caps.excaprefs[0];
        vspaceCap = vspaceCapSlot->cap;

        if (__builtin_expect(!!(!isVTableRoot(vspaceCap) || cap_vspace_cap_get_capVSIsMapped(vspaceCap)), 0)) {
            current_syscall_error.type = seL4_InvalidCapability;
            current_syscall_error.invalidCapNumber = 1;

            return EXCEPTION_SYSCALL_ERROR;
        }

        pool = armKSASIDTable[cap_asid_pool_cap_get_capASIDBase(cap) >> asidLowBits];

        if (__builtin_expect(!!(!pool), 0)) {
            current_syscall_error.type = seL4_FailedLookup;
            current_syscall_error.failedLookupWasSource = false;
            current_lookup_fault = lookup_fault_invalid_root_new();

            return EXCEPTION_SYSCALL_ERROR;
        }

        if (__builtin_expect(!!(pool != ((asid_pool_t*)cap_asid_pool_cap_get_capASIDPool(cap))), 0)) {
            current_syscall_error.type = seL4_InvalidCapability;
            current_syscall_error.invalidCapNumber = 0;

            return EXCEPTION_SYSCALL_ERROR;
        }

        /* Find first free ASID */
        asid = cap_asid_pool_cap_get_capASIDBase(cap);
        for (i = 0; i < (1 << asidLowBits) && (asid + i == 0
                                               || (asid_map_get_type(pool->array[i]) != asid_map_asid_map_none)); i++);

        if (__builtin_expect(!!(i == 1 << asidLowBits), 0)) {
            current_syscall_error.type = seL4_DeleteFirst;

            return EXCEPTION_SYSCALL_ERROR;
        }

        asid += i;

        setThreadState(ksCurThread, ThreadState_Restart);
        return performASIDPoolInvocation(asid, pool, vspaceCapSlot);
    }

    default:
        _fail("Invalid ARM arch cap type", "/home/b/test/sel4test/kernel/src/arch/arm/64/kernel/vspace.c", 1854, __func__);
    }
}


void kernelPrefetchAbort(word_t pc) __attribute__((externally_visible));
void kernelDataAbort(word_t pc) __attribute__((externally_visible));

void kernelPrefetchAbort(word_t pc)
{
    printf("\n\nKERNEL PREFETCH ABORT!\n");
    printf("Faulting instruction: 0x%""lx""\n", pc);
    printf("ESR (IFSR): 0x%""lx""\n", getESR());
    halt();
}

void kernelDataAbort(word_t pc)
{
    printf("\n\nKERNEL DATA ABORT!\n");
    printf("Faulting instruction: 0x%""lx""\n", pc);
    printf("FAR: 0x%""lx"" ESR (DFSR): 0x%""lx""\n",
           getFAR(), getESR());
    halt();
}



typedef struct readWordFromVSpace_ret {
    exception_t status;
    word_t value;
} readWordFromVSpace_ret_t;

static readWordFromVSpace_ret_t readWordFromVSpace(vspace_root_t *pd, word_t vaddr)
{
    readWordFromVSpace_ret_t ret;
    word_t offset;
    pptr_t kernel_vaddr;
    word_t *value;

    lookupPTSlot_ret_t lookup_ret = lookupPTSlot(pd, vaddr);

    /* Check that the returned slot is a page. */
    if (!pte_ptr_get_valid(lookup_ret.ptSlot) ||
        (pte_pte_table_ptr_get_present(lookup_ret.ptSlot) && lookup_ret.ptBitsLeft > 12)) {
        ret.status = EXCEPTION_LOOKUP_FAULT;
        return ret;
    }

    offset = vaddr & ((1ul << (lookup_ret.ptBitsLeft)) - 1ul);
    kernel_vaddr = (word_t)ptrFromPAddr(pte_page_ptr_get_page_base_address(lookup_ret.ptSlot));
    value = (word_t *)(kernel_vaddr + offset);

    ret.status = EXCEPTION_NONE;
    ret.value = *value;
    return ret;
}

void Arch_userStackTrace(tcb_t *tptr)
{
    cap_t threadRoot;
    vspace_root_t *vspaceRoot;
    word_t sp;

    threadRoot = (((cte_t *)((word_t)(tptr)&~((1ul << (12)) - 1ul)))+(tcbVTable))->cap;

    /* lookup the vspace root */
    if (cap_get_capType(threadRoot) != cap_vspace_cap) {
        printf("Invalid vspace\n");
        return;
    }

    vspaceRoot = ((vspace_root_t *)(cap_vspace_cap_get_capVSBasePtr(threadRoot)));
    sp = getRegister(tptr, SP_EL0);

    /* check for alignment so we don't have to worry about accessing
     * words that might be on two different pages */
    if (!(!((sp) & ((1ul << (3)) - 1ul)))) {
        printf("SP not aligned\n");
        return;
    }

    for (unsigned int i = 0; i < 16; i++) {
        word_t address = sp + (i * sizeof(word_t));
        readWordFromVSpace_ret_t result = readWordFromVSpace(vspaceRoot,
                                                             address);
        if (result.status == EXCEPTION_NONE) {
            printf("0x%""lx"": 0x%""lx""\n",
                   address, result.value);
        } else {
            printf("0x%""lx"": INVALID\n", address);
        }
    }
}
# 1 "/home/b/test/sel4test/kernel/src/arch/arm/64/machine/capdl.c"
/*
 * Copyright 2020, Data61, CSIRO (ABN 41 687 119 230)
 *
 * SPDX-License-Identifier: GPL-2.0-only
 */
# 15 "/home/b/test/sel4test/kernel/src/arch/arm/64/machine/capdl.c"
word_t get_tcb_sp(tcb_t *tcb)
{
    return tcb->tcbArch.tcbContext.registers[SP_EL0];
}



static void obj_frame_print_attrs(vm_page_size_t frameSize, paddr_t frameBase);
static void cap_frame_print_attrs_pt(pte_t *ptSlot);
static void cap_frame_print_attrs_impl(word_t SH, word_t AP, word_t NXN);
static void cap_frame_print_attrs_vptr(word_t vptr, cap_t vspace);

static void _cap_frame_print_attrs_vptr(word_t vptr, vspace_root_t *vspaceRoot);

static void arm64_obj_pt_print_slots(pte_t *pdSlot);
static void arm64_obj_pd_print_slots(pte_t *pudSlot);
static void arm64_obj_pud_print_slots(void *pgdSlot_or_vspace);

static void arm64_cap_pt_print_slots(pte_t *pdSlot, vptr_t vptr);
static void arm64_cap_pd_print_slots(pte_t *pudSlot, vptr_t vptr);
static void arm64_cap_pud_print_slots(void *pgdSlot_or_vspace, vptr_t vptr);

/* Stage-1 access permissions:
 * AP[2:1]  higher EL        EL0
 *   00       rw            None
 *   01       rw            rw
 *   10       r             None
 *   11       r             r
 *
 * Stage-2 access permissions:
 * S2AP    Access from Nonsecure EL1 or Non-secure EL0
 *  00                      None
 *  01                      r
 *  10                      w
 *  11                      rw
 *
 *  For VMs or native seL4 applications, if hypervisor support
 *  is enabled, we use the S2AP. The kernel itself running in
 *  EL2 still uses the Stage-1 AP format.
 */
/* use when only have access to pte of frames */
static void cap_frame_print_attrs_pt(pte_t *ptSlot)
{
    cap_frame_print_attrs_impl(pte_pte_page_ptr_get_SH(ptSlot),
                               pte_pte_page_ptr_get_AP(ptSlot),
                               pte_pte_page_ptr_get_UXN(ptSlot));
}

static void cap_frame_print_attrs_impl(word_t SH, word_t AP, word_t NXN)
{
    printf("(");

    /* rights */
    switch (AP) {
# 82 "/home/b/test/sel4test/kernel/src/arch/arm/64/machine/capdl.c"
    case 0b00:
        break;
    case 0b01:
        printf("RW");
    case 0b10:
        break;
    case 0b11:
        printf("R");

    default:
        break;
    }

    if (!NXN) {
        printf("X");
    }

    /* Only has effect if SMP enabled */
    if (SH != 0) {
        printf(", uncached");
    }

    printf(")\n");
}

/* use when only have access to vptr of frames */
static void _cap_frame_print_attrs_vptr(word_t vptr, vspace_root_t *vspace)
{
    lookupPTSlot_ret_t ret = lookupPTSlot(vspace, vptr);

    /* Check that the returned slot is a page. */
    if (!pte_ptr_get_valid(ret.ptSlot) ||
        (pte_pte_table_ptr_get_present(ret.ptSlot) && ret.ptBitsLeft > 12)) {
        do { if (!(0)) { _assert_fail("0", "/home/b/test/sel4test/kernel/src/arch/arm/64/machine/capdl.c", 115, __func__); } } while(0);
    }

    word_t table_index;
    switch (ret.ptBitsLeft) {

    case ARMHugePage:
        table_index = (((vptr) >> (((9) * (((4) - 1) - ((1)))) + 12)) & ((1ul << (((1) == 0 ? 9 : 9))) - 1ul));
        break;
    case ARMLargePage:
        table_index = (((vptr) >> (((9) * (((4) - 1) - ((2)))) + 12)) & ((1ul << (((2) == 0 ? 9 : 9))) - 1ul));
        break;
    case ARMSmallPage:
        table_index = (((vptr) >> (((9) * (((4) - 1) - ((3)))) + 12)) & ((1ul << (((3) == 0 ? 9 : 9))) - 1ul));
        break;
    default:
        do { if (!(0)) { _assert_fail("0", "/home/b/test/sel4test/kernel/src/arch/arm/64/machine/capdl.c", 131, __func__); } } while(0);

    }
    printf("frame_%p_%04lu ", ret.ptSlot, table_index);
    cap_frame_print_attrs_pt(ret.ptSlot);
}

void cap_frame_print_attrs_vptr(word_t vptr, cap_t vspace)
{
    _cap_frame_print_attrs_vptr(vptr, ((vspace_root_t *)(((pptr_t)cap_get_capPtr(vspace)))));
}

/*
 * print object slots
 */
static void arm64_cap_pt_print_slots(pte_t *pdSlot, vptr_t vptr)
{
    pte_t *pt = ptrFromPAddr(pte_pte_table_ptr_get_pt_base_address(pdSlot));
    printf("pt_%p_%04lu {\n", pdSlot, (((vptr) >> (((9) * (((4) - 1) - ((2)))) + 12)) & ((1ul << (((2) == 0 ? 9 : 9))) - 1ul)));

    for (word_t i = 0; i < (1ul << (9)); i ++) {
        pte_t *ptSlot = pt + i;

        if (pte_4k_page_ptr_get_present(ptSlot)) {
            // print pte entries
            printf("0x%lx: frame_%p_%04lu", i, ptSlot, i);
            cap_frame_print_attrs_pt(ptSlot);
        }
    }
    printf("}\n"); /* pt */
}

static void arm64_cap_pd_print_slots(pte_t *pudSlot, vptr_t vptr)
{
    printf("pd_%p_%04lu {\n", pudSlot, (((vptr) >> (((9) * (((4) - 1) - ((1)))) + 12)) & ((1ul << (((1) == 0 ? 9 : 9))) - 1ul)));
    pte_t *pd = ptrFromPAddr(pte_pte_table_ptr_get_pt_base_address(pudSlot));

    for (word_t i = 0; i < (1ul << (9)); i++) {
        pte_t *pdSlot = pd + i;

        switch (pte_ptr_get_pte_type(pdSlot)) {

        case pte_pte_page:
            printf("0x%lx: frame_%p_%04lu", i, pdSlot, i);
            cap_frame_print_attrs_pt(pdSlot);
            break;

        case pte_pte_table:
            printf("0x%lx: pt_%p_%04lu\n", i, pdSlot, i);
            break;
        }
    }

    printf("}\n"); /* pd */

    for (word_t i = 0; i < (1ul << (9)); i++) {
        pte_t *pdSlot = pd + i;
        if (pte_ptr_get_pte_type(pdSlot) == pte_pte_table) {
            arm64_cap_pt_print_slots(pdSlot, vptr + (i * (1ul << ((((9) * (((4) - 1) - (((2))))) + 12)))));
        }
    }
}

static void arm64_cap_pud_print_slots(void *pgdSlot_or_vspace, vptr_t vptr)
{





    pte_t *pud = ptrFromPAddr(pte_pte_table_ptr_get_pt_base_address(pgdSlot_or_vspace));
    word_t index_bits = 9;
    printf("pud_%p_%04lu {\n", pgdSlot_or_vspace, (((vptr) >> (((9) * (((4) - 1) - ((0)))) + 12)) & ((1ul << (((0) == 0 ? 9 : 9))) - 1ul)));


    for (word_t i = 0; i < (1ul << (index_bits)); i++) {
        pte_t *pudSlot = pud + i;
        if (pte_ptr_get_pte_type(pudSlot) == pte_pte_table) {
            printf("0x%lx: pd_%p_%04lu\n", i, pudSlot, i);
        }
    }

    printf("}\n"); /* pgd/pud */

    for (word_t i = 0; i < (1ul << (index_bits)); i++) {
        pte_t *pudSlot = pud + (((i) >> (((9) * (((4) - 1) - ((1)))) + 12)) & ((1ul << (((1) == 0 ? 9 : 9))) - 1ul));
        if (pte_ptr_get_pte_type(pudSlot) == pte_pte_table) {
            arm64_cap_pd_print_slots(pudSlot, vptr + (i * (1ul << ((((9) * (((4) - 1) - (((1))))) + 12)))));
        }
    }
}

void obj_vtable_print_slots(tcb_t *tcb)
{
    if (isVTableRoot((((cte_t *)((word_t)(tcb)&~((1ul << (12)) - 1ul)))+(tcbVTable))->cap) && !seen((((cte_t *)((word_t)(tcb)&~((1ul << (12)) - 1ul)))+(tcbVTable))->cap)) {
        add_to_seen((((cte_t *)((word_t)(tcb)&~((1ul << (12)) - 1ul)))+(tcbVTable))->cap);
        vspace_root_t *vspace = ((vspace_root_t *)(cap_vspace_cap_get_capVSBasePtr((((cte_t *)((word_t)(tcb)&~((1ul << (12)) - 1ul)))+(tcbVTable))->cap)));

        /*
        * ARM hyp uses 3 level translation rather than the usual 4 level.
        * levels: PGD -> UPD -> PD -> PT
        */



        printf("%p_pd {\n", vspace);
        for (word_t i = 0; i < 9; i++) {
            pte_t *ptSlot = vspace + i;
            if (pte_pte_table_ptr_get_present(ptSlot)) {
                printf("0x%lx: pud_%p_%04lu\n", i, ptSlot, i);
            }
        }
        printf("}\n"); /* pd */

        for (word_t i = 0; i < 9; i++) {
            pte_t *ptSlot = vspace + i;
            if (pte_pte_table_ptr_get_present(ptSlot)) {
                arm64_cap_pud_print_slots(ptSlot, i * (1ul << ((((9) * (((4) - 1) - ((0)))) + 12))));
            }
        }

    }
}

void print_ipc_buffer_slot(tcb_t *tcb)
{
    word_t vptr = tcb->tcbIPCBuffer;
    printf("ipc_buffer_slot: ");
    cap_frame_print_attrs_vptr(vptr, (((cte_t *)((word_t)(tcb)&~((1ul << (12)) - 1ul)))+(tcbVTable))->cap);
}

void print_cap_arch(cap_t cap)
{

    switch (cap_get_capType(cap)) {
    case cap_page_table_cap: {
        asid_t asid = cap_page_table_cap_get_capPTMappedASID(cap);
        vptr_t vptr = cap_page_table_cap_get_capPTMappedAddress(cap);
        pte_t *target_pt = ((pte_t *)(cap_page_table_cap_get_capPTBasePtr(cap)));

        findVSpaceForASID_ret_t find_ret = findVSpaceForASID(asid);
        pte_t *ptSlot = ((void *)0);
        pte_t *pt = (pte_t *)find_ret.vspace_root;
        word_t level;
        for (level = 0; level < 4 - 1 && pt != target_pt; level++) {
            ptSlot = pt + (((vptr) >> (((9) * (((4) - 1) - (level))) + 12)) & ((1ul << ((level == 0 ? 9 : 9))) - 1ul));
            if (__builtin_expect(!!(!pte_pte_table_ptr_get_present(ptSlot)), 0)) {
                /* couldn't find it */
                break;
            }
            pt = ptrFromPAddr(pte_pte_table_ptr_get_pt_base_address(ptSlot));
        }
        if (pt != target_pt) {
            /* didn't find it */
            break;
        }


        if (asid) {
            printf("pt_%p_%04lu (asid: %lu)\n",
                   target_pt, (((vptr) >> (((9) * (((4) - 1) - (level))) + 12)) & ((1ul << ((level == 0 ? 9 : 9))) - 1ul)), (long unsigned int)asid);
        } else {
            printf("pt_%p_%04lu\n", target_pt, (((vptr) >> (((9) * (((4) - 1) - (level))) + 12)) & ((1ul << ((level == 0 ? 9 : 9))) - 1ul)));
        }
        break;
    }
    case cap_vspace_cap: {
        asid_t asid = cap_vspace_cap_get_capVSMappedASID(cap);
        findVSpaceForASID_ret_t find_ret = findVSpaceForASID(asid);
        if (asid) {
            printf("%p_pd (asid: %lu)\n",
                   find_ret.vspace_root, (long unsigned int)asid);
        } else {
            printf("%p_pd\n", find_ret.vspace_root);
        }
        break;
    }
    case cap_asid_control_cap: {
        /* only one in the system */
        printf("asid_control\n");
        break;
    }
    case cap_frame_cap: {
        vptr_t vptr = cap_frame_cap_get_capFMappedAddress(cap);
        findVSpaceForASID_ret_t find_ret = findVSpaceForASID(cap_frame_cap_get_capFMappedASID(cap));
        do { if (!(find_ret.status == EXCEPTION_NONE)) { _assert_fail("find_ret.status == EXCEPTION_NONE", "/home/b/test/sel4test/kernel/src/arch/arm/64/machine/capdl.c", 316, __func__); } } while(0);
        _cap_frame_print_attrs_vptr(vptr, find_ret.vspace_root);
        break;
    }
    case cap_asid_pool_cap: {
        printf("%p_asid_pool\n", (void *)cap_asid_pool_cap_get_capASIDPool(cap));
        break;
    }







        /* ARM specific caps */






    default: {
        printf("[unknown cap %lu]\n", (long unsigned int)cap_get_capType(cap));
        break;
    }
    }
}

void print_object_arch(cap_t cap)
{
    switch (cap_get_capType(cap)) {
    case cap_frame_cap:
    case cap_page_table_cap:
    case cap_vspace_cap:
        /* don't need to deal with these objects since they get handled from vtable */
        break;

    case cap_asid_pool_cap: {
        printf("%p_asid_pool = asid_pool ",
               (void *)cap_asid_pool_cap_get_capASIDPool(cap));
        obj_asidpool_print_attrs(cap);
        break;
    }






        /* ARM specific objects */







    default: {
        printf("[unknown object %lu]\n", (long unsigned int)cap_get_capType(cap));
        break;
    }
    }
}

void obj_frame_print_attrs(vm_page_size_t frameSize, paddr_t frameBase)
{
    printf("(");

    /* VM size */
    switch (frameSize) {
    case ARMHugePage:
        printf("1G");
        break;
    case ARMLargePage:
        printf("2M");
        break;
    case ARMSmallPage:
        printf("4k");
        break;
    }

    printf(", paddr: 0x%p)\n", (void *)frameBase);
}

void arm64_obj_pt_print_slots(pte_t *pdSlot)
{
    pte_t *pt = ptrFromPAddr(pte_pte_table_ptr_get_pt_base_address(pdSlot));

    for (word_t i = 0; i < (1ul << (9)); i++) {
        pte_t *ptSlot = pt + i;

        if (pte_4k_page_ptr_get_present(ptSlot)) {
            printf("frame_%p_%04lu = frame ", ptSlot, i);
            obj_frame_print_attrs(ARMSmallPage, pte_page_ptr_get_page_base_address(ptSlot));
        }
    }
}

void arm64_obj_pd_print_slots(pte_t *pudSlot)
{
    pte_t *pd = ptrFromPAddr(pte_pte_table_ptr_get_pt_base_address(pudSlot));

    for (word_t i = 0; i < (1ul << (9)); i++) {
        pte_t *pdSlot = pd + i;

        if (pte_ptr_get_pte_type(pdSlot) == pte_pte_page) {
            printf("frame_%p_%04lu = frame ", pdSlot, i);
            obj_frame_print_attrs(ARMLargePage, pte_page_ptr_get_page_base_address(pdSlot));
        }

        if (pte_ptr_get_pte_type(pdSlot) == pte_pte_table) {
            printf("pt_%p_%04lu = pt\n", pdSlot, i);
            arm64_obj_pt_print_slots(pdSlot);
        }
    }
}

void arm64_obj_pud_print_slots(void *pgdSlot_or_vspace)
{
    pte_t *pud = ptrFromPAddr(pte_pte_table_ptr_get_pt_base_address(pgdSlot_or_vspace));



    word_t index_bits = 9;

    for (word_t i = 0; i < (1ul << (index_bits)); i++) {
        pte_t *pudSlot = pud + i;

        switch (pte_ptr_get_pte_type(pudSlot)) {
        case pte_pte_page:
            printf("frame_%p_%04lu = frame ", pudSlot, i);
            obj_frame_print_attrs(ARMHugePage, pte_page_ptr_get_page_base_address(pudSlot));
            break;

        case pte_pte_table: {
            printf("pd_%p_%04lu = pd\n", pudSlot, i);
            arm64_obj_pd_print_slots(pudSlot);

        }
        }
    }
}

void obj_tcb_print_vtable(tcb_t *tcb)
{
    if (isVTableRoot((((cte_t *)((word_t)(tcb)&~((1ul << (12)) - 1ul)))+(tcbVTable))->cap) && !seen((((cte_t *)((word_t)(tcb)&~((1ul << (12)) - 1ul)))+(tcbVTable))->cap)) {
        add_to_seen((((cte_t *)((word_t)(tcb)&~((1ul << (12)) - 1ul)))+(tcbVTable))->cap);
        vspace_root_t *vspace = ((vspace_root_t *)(cap_vspace_cap_get_capVSBasePtr((((cte_t *)((word_t)(tcb)&~((1ul << (12)) - 1ul)))+(tcbVTable))->cap)));

        /*
         * ARM hyp uses 3 level translation rather than the usual 4 level.
         * levels: PGD -> PUD -> PD -> PT
         */




        printf("%p_pd = pgd\n", vspace);
        for (word_t i = 0; i < 9; i++) {
            pte_t *ptSlot = vspace + i;
            if (pte_pte_table_ptr_get_present(ptSlot)) {
                printf("pud_%p_%04lu = pud\n", ptSlot, i);
                arm64_obj_pud_print_slots(ptSlot);
            }
        }

    }
}



void debug_capDL(void)
{
    printf("arch aarch64\n");
    printf("objects {\n");

    print_objects();

    printf("}\n");

    printf("caps {\n");

    /* reset the seen list */
    reset_seen_list();


    print_caps();
    printf("}\n");

    obj_irq_print_maps();

}
# 1 "/home/b/test/sel4test/kernel/src/arch/arm/64/machine/debug.c"
/*
 * Copyright 2020, Data61, CSIRO (ABN 41 687 119 230)
 *
 * SPDX-License-Identifier: GPL-2.0-only
 */
# 37 "/home/b/test/sel4test/kernel/src/arch/arm/64/machine/debug.c"
bool_t byte8WatchpointsSupported(void)
{
    return true;
}

exception_t handleDebugFaultEvent(word_t esr)
{
    { { current_fault = handleUserLevelDebugException(esr, getRestartPC(ksCurThread)); if (seL4_Fault_get_seL4_FaultType(current_fault) != seL4_Fault_NullFault) { handleFault(ksCurThread); } } }






    schedule();
    activateThread();

    return EXCEPTION_NONE;
}

/** Initiates or halts single-stepping on the target process.
 *
 * @param at arch_tcb_t for the target process to be configured.
 * @param bp_num The hardware ID of the breakpoint register to be used.
 * @param n_instr The number of instructions to step over.
 */
bool_t configureSingleStepping(tcb_t *t, uint16_t bp_num, word_t n_instr,
                               bool_t is_reply)
{
    if (n_instr > 0) {
        /* Enable single stepping */
        t->tcbArch.tcbContext.breakpointState.single_step_enabled = true;
    } else {
        /* Disable single stepping */
        t->tcbArch.tcbContext.breakpointState.single_step_enabled = false;
    }

    t->tcbArch.tcbContext.breakpointState.n_instructions = n_instr;
    return true;
}

/* Guides the debug hardware initialization sequence. */
__attribute__((__section__(".boot.text"))) bool_t Arch_initHardwareBreakpoints(void)
{
    /*
     * ARMv8 Architecture Reference Manual for A-profile Architecture
     * D2.2: The Enable controls for each debug exception are:
     *    ... MDSCR_EL1.MDE
     */

    word_t mdscr = 0;
    __asm__ volatile("mrs %x0," "MDSCR_EL1" : "=r"(mdscr));
    mdscr |= ((1ul << (15)));
    do { word_t _v = mdscr; __asm__ volatile("msr " "MDSCR_EL1" ",%x0" :: "r" (_v)); }while(0);

    /*
     * ARMv8 Architecture Reference Manual for A-profile Architecture
     * D2.4: A debug exception can be taken only if all the following are true:
     *    - The OS Lock is unlocked
     *    - DoubleLockStatus() = False
     *    - The debug exception is enabled from the current exception level
     *    - The debug exception is enabled from the current security state
     */

    /* Ensure that the OS double lock is unset */
    word_t osdlr = 0;
    __asm__ volatile("mrs %x0," "osdlr_el1" : "=r"(osdlr));
    osdlr &= ~((1ul << (0)));
    do { word_t _v = osdlr; __asm__ volatile("msr " "osdlr_el1" ",%x0" :: "r" (_v)); }while(0);

    /* Ensure that the OS lock is unset */
    word_t oslar = 0;
    do { word_t _v = oslar; __asm__ volatile("msr " "oslar_el1" ",%x0" :: "r" (_v)); }while(0);

    /* Ensure that all the breakpoint and watchpoint registers are initially disabled */
    disableAllBpsAndWps();

    /* Ensure that single stepping is initially disabled */
    __asm__ volatile("mrs %x0," "MDSCR_EL1" : "=r"(mdscr));
    mdscr &= ~((1ul << (0)));
    do { word_t _v = mdscr; __asm__ volatile("msr " "MDSCR_EL1" ",%x0" :: "r" (_v)); }while(0);

    /* Finally, also pre-load some initial register state that can be used
     * for all new threads so that their initial saved debug register state
     * is valid when it's first loaded onto the CPU.
     */
    for (int i = 0; i < 6; i++) {
        armKSNullBreakpointState.breakpoint[i].cr = readBcrCp(i) & ~((1ul << (0)));
    }
    for (int i = 0; i < 4; i++) {
        armKSNullBreakpointState.watchpoint[i].cr = readWcrCp(i) & ~((1ul << (0)));
    }

    return true;
}

/* Abstract wrapper around the ESR fault status value */

static word_t getFaultStatus(word_t esr)
{
    return (esr & 0xFC000000) >> 26;
}

/** Called to determine if an abort was a debug exception.
 *
 * The ARM debug exceptions look like Prefetch Aborts or Data Aborts, and you
 * have to examine some extra register state to determine whether or not the
 * abort you currently have on your hands is actually a debug exception.
 *
 * This routine takes care of the checks.
 * @param fs An abstraction of the DFSR/IFSR values, meant to make it irrelevant
 *           whether we're using the long/short descriptors. Bit positions and
 *           values change. This also makes the debug code forward compatible
 *           aarch64.
 */
bool_t isDebugFault(word_t esr)
{
    word_t exception_class = getFaultStatus(esr);
    return (exception_class == 0x30 ||
            exception_class == 0x32 ||
            exception_class == 0x34 ||
            exception_class == 0x3C);
}

/** Called to process a debug exception.
 *
 * On x86, you're told which breakpoint register triggered the exception. On
 * ARM, you're told the virtual address that triggered the exception and what
 * type of access (data access vs instruction execution) triggered the exception
 * and you have to figure out which register triggered it.
 *
 * For watchpoints, it's not very complicated: just check to see which
 * register matches the virtual address.
 *
 * For breakpoints, it's a bit more complex: since both breakpoints and single-
 * stepping are configured using the same registers, we need to first detect
 * whether single-stepping is enabled. If not, then we check for a breakpoint.
 * @param fault_vaddr The instruction vaddr which triggered the exception, as
 *                    extracted by the kernel.
 */
seL4_Fault_t handleUserLevelDebugException(word_t esr, word_t fault_vaddr)
{
    int active_bp;
    word_t bp_reason, bp_vaddr;
    word_t exception_class = getFaultStatus(esr);


    ksKernelEntry.path = Entry_DebugFault;
    ksKernelEntry.word = exception_class;


    switch (exception_class) {
    case 0x30:
        bp_reason = seL4_InstructionBreakpoint;
        bp_vaddr = fault_vaddr;
        break;
    case 0x34:
        bp_reason = seL4_DataBreakpoint;
        bp_vaddr = getFAR();
        break;
    case 0x32:
        bp_reason = seL4_SingleStep;
        bp_vaddr = fault_vaddr;
        active_bp = 0;
        break;
    default: /* EXPLICIT_BKPT: BKPT instruction */
        do { if (!(exception_class == 0x3C)) { _assert_fail("exception_class == DEBUG_ENTRY_EXPLICIT_BKPT", "/home/b/test/sel4test/kernel/src/arch/arm/64/machine/debug.c", 203, __func__); } } while(0);
        bp_reason = seL4_SoftwareBreakRequest;
        bp_vaddr = fault_vaddr;
        active_bp = 0;
    }

    /* There is no hardware register associated with BKPT instruction
    * triggers or single stepping.
    */
    if (bp_reason != seL4_SoftwareBreakRequest && bp_reason != seL4_SingleStep) {
        active_bp = getAndResetActiveBreakpoint(bp_vaddr, bp_reason);
        active_bp = getBpNumFromType(active_bp, bp_reason);
        do { if (!(active_bp >= 0)) { _assert_fail("active_bp >= 0", "/home/b/test/sel4test/kernel/src/arch/arm/64/machine/debug.c", 215, __func__); } } while(0);
    }

    if (bp_reason == seL4_SingleStep && !singleStepFaultCounterReady(ksCurThread)) {
        return seL4_Fault_NullFault_new();
    }

    return seL4_Fault_DebugException_new(bp_vaddr, active_bp, bp_reason);
}





/** Pops debug register context for a thread into the CPU.
 *
 * Mirrors the idea of restore_user_context.
 */
void aarch64_restore_user_debug_context(tcb_t *target_thread)
{
    do { if (!(target_thread != ((void *)0))) { _assert_fail("target_thread != NULL", "/home/b/test/sel4test/kernel/src/arch/arm/64/machine/debug.c", 235, __func__); } } while(0);

    /* Set/unset single stepping if applicable */
    word_t mdscr = 0, spsr = 0;
    __asm__ volatile("mrs %x0," "MDSCR_EL1" : "=r"(mdscr));
    spsr = getRegister(target_thread, SPSR_EL1);
    if (target_thread->tcbArch.tcbContext.breakpointState.single_step_enabled) {
        /* Enable single stepping */
        mdscr |= ((1ul << (0)));
        spsr |= ((1ul << (21)));
    } else {
        /* Disable single stepping */
        mdscr &= ~((1ul << (0)));
        spsr &= ~((1ul << (21)));
    }
    do { word_t _v = mdscr; __asm__ volatile("msr " "MDSCR_EL1" ",%x0" :: "r" (_v)); }while(0);
    setRegister(target_thread, SPSR_EL1, spsr);
}
# 1 "/home/b/test/sel4test/kernel/src/arch/arm/64/machine/fpu.c"
/*
 * Copyright 2020, Data61, CSIRO (ABN 41 687 119 230)
 *
 * SPDX-License-Identifier: GPL-2.0-only
 */






/* Initialise the FP/SIMD for this machine. */
__attribute__((__section__(".boot.text"))) bool_t fpsimd_init(void)
{
    /* Set the FPU to lazy switch mode */
    disableFpu();
    if (wrap_config_set(0)) {
        enableFpuEL01();
    }

    return true;
}


__attribute__((__section__(".boot.text"))) bool_t fpsimd_HWCapTest(void)
{
    word_t id_aa64pfr0;

    /* Check if the hardware has FP and ASIMD support... */
    __asm__ volatile("mrs %x0," "id_aa64pfr0_el1" : "=r"(id_aa64pfr0));
    if (((id_aa64pfr0 >> 16 /* HWCap for Floating Point*/) & ((1ul << (4)) - 1ul)) == ((1ul << (4)) - 1ul) ||
        ((id_aa64pfr0 >> 20 /* HWCap for Advanced SIMD*/) & ((1ul << (4)) - 1ul)) == ((1ul << (4)) - 1ul)) {
        return false;
    }

    return true;
}
# 1 "/home/b/test/sel4test/kernel/src/arch/arm/64/machine/registerset.c"
/*
 * Copyright 2020, Data61, CSIRO (ABN 41 687 119 230)
 *
 * SPDX-License-Identifier: GPL-2.0-only
 */




const register_t msgRegisters[] = {
    X2, X3, X4, X5
};
_Static_assert(sizeof(msgRegisters) / sizeof(msgRegisters[0]) == n_msgRegisters, "consistent_message_registers");


 ;

const register_t frameRegisters[] = {
    FaultIP, SP_EL0, SPSR_EL1,
    X0, X1, X2, X3, X4, X5, X6, X7, X8, X16, X17, X18, X29, X30
};
_Static_assert(sizeof(frameRegisters) / sizeof(frameRegisters[0]) == n_frameRegisters, "consistent_frame_registers");


 ;

const register_t gpRegisters[] = {
    X9, X10, X11, X12, X13, X14, X15,
    X19, X20, X21, X22, X23, X24, X25, X26, X27, X28,
    TPIDR_EL0, TPIDRRO_EL0,
};
_Static_assert(sizeof(gpRegisters) / sizeof(gpRegisters[0]) == n_gpRegisters, "consistent_gp_registers");


 ;
# 1 "/home/b/test/sel4test/kernel/src/arch/arm/64/model/statedata.c"
/*
 * Copyright 2020, Data61, CSIRO (ABN 41 687 119 230)
 *
 * SPDX-License-Identifier: GPL-2.0-only
 */
# 21 "/home/b/test/sel4test/kernel/src/arch/arm/64/model/statedata.c"
asid_pool_t *armKSASIDTable[(1ul << (asidHighBits))];

/* AArch64 Memory map explanation:
 *
 * EL1 and EL2 kernel build vaddrspace layouts:
 *
 * On AArch64, the EL1 and EL2 builds of the kernel both use approx 512GiB of
 * virtual address space.
 *
 * The EL1 kernel resides within approx 512 GiB of virtual memory somewhere
 * within the canonical top (not necessarily the same as the actual top, but all
 * the unused high bits of kernel virtual addresses are set to 1) of every
 * user VSpace.
 *
 * The EL2 kernel resides within approx 512 GiB of virtual memory somewhere
 * within the canonical bottom (all the unused high bits are set to 0) of its
 * own separate virtual address space.
 *
 * Common Aarch64 address space layout:
 *
 * The reason why 512 GiB was chosen is because assuming a 48-bit virtual
 * address space using a 4KiB Translation Granule (and therefore, 4 levels of
 * page tables):
 *
 * One top-level page-structure entry maps 512 GiB, so by limiting ourselves to
 * 512 GiB, we only need to pre-allocate 1 level0 table (lvl0 is the top level),
 * 1 level1 table, 512 level2 tables (with each level2 entry mapping 2MiB), and
 * skip level3.
 *
 * We do maintain a single level3 table however, which is mapped into the last
 * entry in the last level2 table, such that the last 2MiB are mapped using
 * 4KiB pages instead of 2MiB pages. The reason for this last 2MiB being mapped
 * using small pages is because this last 2MiB is where the kernel maps all the
 * different devices it uses (see map_kernel_devices()). This implies that the
 * kernel can only have up to approx 512GiB of kernel untypeds.
 *
 * If you wish for your AArch64 platform to use more than 512 GiB of virtual
 * memory, you will need to change the number of pre-allocated page tables below
 * to be sufficient to contain the mapping you want. And don't forget to update
 * this comment here afterward.
 */

/* User vaddrspace layouts:
 *
 * For EL2:
 *
 * A plain-english explanation of the memory layout is that the
 * the user address spaces cover the address range from 0x0 to the maximum
 * IPA.
 *
 * So for a CPU that can generate 44 bits of IPA/PA (such as the TX1/TX2), user
 * vaddr spaces will cover 16TiB from 0x0 to 0x00000fff_ffffffff.
 *
 * Basically by making the guest physical address spaces 44 bits, the guest
 * kernels can access all of (what they think is) physical memory, while
 * allowing us to potentially trap accesses by the guests to physical memory
 * beyond what the processor can address.
 *
 * For EL1:
 *
 * The standard canonical-high and canonical-low split using TCR_EL1.TBI
 * applies.
 */

vspace_root_t armKSGlobalUserVSpace[(1ul << (9))] __attribute__((__aligned__((1ul << (12))))) __attribute__((__section__(".bss.aligned")));
pte_t armKSGlobalKernelPGD[(1ul << (9))] __attribute__((__aligned__((1ul << (12))))) __attribute__((__section__(".bss.aligned")));

pte_t armKSGlobalKernelPUD[(1ul << (9))] __attribute__((__aligned__((1ul << (12))))) __attribute__((__section__(".bss.aligned")));
pte_t armKSGlobalKernelPDs[(1ul << (9))][(1ul << (9))] __attribute__((__aligned__((1ul << (12))))) __attribute__((__section__(".bss.aligned")));
pte_t armKSGlobalKernelPT[(1ul << (9))] __attribute__((__aligned__((1ul << (12))))) __attribute__((__section__(".bss.aligned")));
# 129 "/home/b/test/sel4test/kernel/src/arch/arm/64/model/statedata.c"
/* Null state for the Debug coprocessor's break/watchpoint registers */
user_breakpoint_state_t armKSNullBreakpointState;
# 1 "/home/b/test/sel4test/kernel/src/arch/arm/64/object/objecttype.c"
/*
 * Copyright 2020, Data61, CSIRO (ABN 41 687 119 230)
 *
 * SPDX-License-Identifier: GPL-2.0-only
 */
# 16 "/home/b/test/sel4test/kernel/src/arch/arm/64/object/objecttype.c"
bool_t Arch_isFrameType(word_t type)
{
    switch (type) {
    case seL4_ARM_SmallPageObject:
        return true;
    case seL4_ARM_LargePageObject:
        return true;
    case seL4_ARM_HugePageObject:
        return true;
    default:
        return false;
    }
}

deriveCap_ret_t Arch_deriveCap(cte_t *slot, cap_t cap)
{
    deriveCap_ret_t ret;

    switch (cap_get_capType(cap)) {
    case cap_vspace_cap:
        if (cap_vspace_cap_get_capVSIsMapped(cap)) {
            ret.cap = cap;
            ret.status = EXCEPTION_NONE;
        } else {
            do { printf("\033[0m" "\033[1m" "<<" "\033[0m" "\033[32m" "seL4(CPU %" "lu" ")" "\033[0m" "\033[1m" " [%s/%d T%p \"%s\" @%lx]: " "Deriving a VSpace cap without an assigned ASID" ">>" "\033[0m" "\n", 0lu, __func__, 40, ksCurThread, ((debug_tcb_t *)(((cte_t *)((word_t)(ksCurThread)&~((1ul << (12)) - 1ul)))+(tcbCNodeEntries)))->tcbName, (word_t)getRestartPC(ksCurThread)); } while (0);
            current_syscall_error.type = seL4_IllegalOperation;
            ret.cap = cap_null_cap_new();
            ret.status = EXCEPTION_SYSCALL_ERROR;
        }
        return ret;

    case cap_page_table_cap:
        if (cap_page_table_cap_get_capPTIsMapped(cap)) {
            ret.cap = cap;
            ret.status = EXCEPTION_NONE;
        } else {
            do { printf("\033[0m" "\033[1m" "<<" "\033[0m" "\033[32m" "seL4(CPU %" "lu" ")" "\033[0m" "\033[1m" " [%s/%d T%p \"%s\" @%lx]: " "Deriving a PT cap without an assigned ASID" ">>" "\033[0m" "\n", 0lu, __func__, 52, ksCurThread, ((debug_tcb_t *)(((cte_t *)((word_t)(ksCurThread)&~((1ul << (12)) - 1ul)))+(tcbCNodeEntries)))->tcbName, (word_t)getRestartPC(ksCurThread)); } while (0);
            current_syscall_error.type = seL4_IllegalOperation;
            ret.cap = cap_null_cap_new();
            ret.status = EXCEPTION_SYSCALL_ERROR;
        }
        return ret;

    case cap_frame_cap:
        ret.cap = cap_frame_cap_set_capFMappedASID(cap, asidInvalid);
        ret.status = EXCEPTION_NONE;
        return ret;

    case cap_asid_control_cap:
    case cap_asid_pool_cap:
        ret.cap = cap;
        ret.status = EXCEPTION_NONE;
        return ret;
# 77 "/home/b/test/sel4test/kernel/src/arch/arm/64/object/objecttype.c"
    case cap_sgi_signal_cap:
        ret.cap = cap;
        ret.status = EXCEPTION_NONE;
        return ret;
# 101 "/home/b/test/sel4test/kernel/src/arch/arm/64/object/objecttype.c"
    default:
        /* This assert has no equivalent in haskell,
         * as the options are restricted by type */
        _fail("Invalid arch cap", "/home/b/test/sel4test/kernel/src/arch/arm/64/object/objecttype.c", 104, __func__);
    }
}

cap_t __attribute__((__const__)) Arch_updateCapData(bool_t preserve, word_t data, cap_t cap)
{
# 119 "/home/b/test/sel4test/kernel/src/arch/arm/64/object/objecttype.c"
        return cap;



}

cap_t __attribute__((__const__)) Arch_maskCapRights(seL4_CapRights_t cap_rights_mask, cap_t cap)
{
    if (cap_get_capType(cap) == cap_frame_cap) {
        vm_rights_t vm_rights;

        vm_rights = vmRightsFromWord(cap_frame_cap_get_capFVMRights(cap));
        vm_rights = maskVMRights(vm_rights, cap_rights_mask);

        return cap_frame_cap_set_capFVMRights(cap, wordFromVMRights(vm_rights));
    } else {
        return cap;
    }
}

finaliseCap_ret_t Arch_finaliseCap(cap_t cap, bool_t final)
{
    finaliseCap_ret_t fc_ret;

    switch (cap_get_capType(cap)) {
    case cap_asid_pool_cap:
        if (final) {
            deleteASIDPool(cap_asid_pool_cap_get_capASIDBase(cap),
                           ((asid_pool_t*)cap_asid_pool_cap_get_capASIDPool(cap)));
        }
        break;

    case cap_vspace_cap:






        if (final && cap_vspace_cap_get_capVSIsMapped(cap)) {
            deleteASID(cap_vspace_cap_get_capVSMappedASID(cap),
                       ((vspace_root_t *)(cap_vspace_cap_get_capVSBasePtr(cap))));
        }
        break;

    case cap_page_table_cap:
        if (final && cap_page_table_cap_get_capPTIsMapped(cap)) {
            unmapPageTable(cap_page_table_cap_get_capPTMappedASID(cap),
                           cap_page_table_cap_get_capPTMappedAddress(cap),
                           ((pte_t *)(cap_page_table_cap_get_capPTBasePtr(cap))));
        }
        break;

    case cap_frame_cap:
        if (cap_frame_cap_get_capFMappedASID(cap)) {
            unmapPage(cap_frame_cap_get_capFSize(cap),
                      cap_frame_cap_get_capFMappedASID(cap),
                      cap_frame_cap_get_capFMappedAddress(cap),
                      cap_frame_cap_get_capFBasePtr(cap));
        }
        break;
# 188 "/home/b/test/sel4test/kernel/src/arch/arm/64/object/objecttype.c"
    case cap_sgi_signal_cap:
        // do nothing
        break;
# 209 "/home/b/test/sel4test/kernel/src/arch/arm/64/object/objecttype.c"
    }

    fc_ret.remainder = cap_null_cap_new();
    fc_ret.cleanupInfo = cap_null_cap_new();
    return fc_ret;
}

bool_t __attribute__((__const__)) Arch_sameRegionAs(cap_t cap_a, cap_t cap_b)
{
    switch (cap_get_capType(cap_a)) {
    case cap_frame_cap:
        if (cap_get_capType(cap_b) == cap_frame_cap) {

            word_t botA, botB, topA, topB;
            botA = cap_frame_cap_get_capFBasePtr(cap_a);
            botB = cap_frame_cap_get_capFBasePtr(cap_b);
            topA = botA + ((1ul << (pageBitsForSize(cap_frame_cap_get_capFSize(cap_a)))) - 1ul);
            topB = botB + ((1ul << (pageBitsForSize(cap_frame_cap_get_capFSize(cap_b)))) - 1ul) ;
            return ((botA <= botB) && (topA >= topB) && (botB <= topB));
        }
        break;

    case cap_page_table_cap:
        if (cap_get_capType(cap_b) == cap_page_table_cap) {
            return cap_page_table_cap_get_capPTBasePtr(cap_a) ==
                   cap_page_table_cap_get_capPTBasePtr(cap_b);
        }
        break;

    case cap_vspace_cap:
        if (cap_get_capType(cap_b) == cap_vspace_cap) {
            return cap_vspace_cap_get_capVSBasePtr(cap_a) ==
                   cap_vspace_cap_get_capVSBasePtr(cap_b);
        }
        break;

    case cap_asid_control_cap:
        if (cap_get_capType(cap_b) == cap_asid_control_cap) {
            return true;
        }
        break;

    case cap_asid_pool_cap:
        if (cap_get_capType(cap_b) == cap_asid_pool_cap) {
            return cap_asid_pool_cap_get_capASIDPool(cap_a) ==
                   cap_asid_pool_cap_get_capASIDPool(cap_b);
        }
        break;
# 267 "/home/b/test/sel4test/kernel/src/arch/arm/64/object/objecttype.c"
    case cap_sgi_signal_cap:
        if (cap_get_capType(cap_b) == cap_sgi_signal_cap) {
            return (cap_sgi_signal_cap_get_capSGIIRQ(cap_a) ==
                    cap_sgi_signal_cap_get_capSGIIRQ(cap_b) &&
                    cap_sgi_signal_cap_get_capSGITarget(cap_a) ==
                    cap_sgi_signal_cap_get_capSGITarget(cap_b));
        }
        break;
# 310 "/home/b/test/sel4test/kernel/src/arch/arm/64/object/objecttype.c"
    }
    return false;
}

bool_t __attribute__((__const__)) Arch_sameObjectAs(cap_t cap_a, cap_t cap_b)
{
    if (cap_get_capType(cap_a) == cap_frame_cap) {
        if (cap_get_capType(cap_b) == cap_frame_cap) {
            return ((cap_frame_cap_get_capFBasePtr(cap_a) ==
                     cap_frame_cap_get_capFBasePtr(cap_b)) &&
                    (cap_frame_cap_get_capFSize(cap_a) ==
                     cap_frame_cap_get_capFSize(cap_b)) &&
                    ((cap_frame_cap_get_capFIsDevice(cap_a) == 0) ==
                     (cap_frame_cap_get_capFIsDevice(cap_b) == 0)));
        }
    }
# 337 "/home/b/test/sel4test/kernel/src/arch/arm/64/object/objecttype.c"
    if (cap_get_capType(cap_a) == cap_sgi_signal_cap) {
        return false;
    }


    return Arch_sameRegionAs(cap_a, cap_b);
}

word_t Arch_getObjectSize(word_t t)
{
    switch (t) {
    case seL4_ARM_SmallPageObject:
        return ARMSmallPageBits;
    case seL4_ARM_LargePageObject:
        return ARMLargePageBits;
    case seL4_ARM_HugePageObject:
        return ARMHugePageBits;
    case seL4_ARM_PageTableObject:
        return 12;
    case seL4_ARM_VSpaceObject:
        return 12;




    default:
        _fail("Invalid object type", "/home/b/test/sel4test/kernel/src/arch/arm/64/object/objecttype.c", 363, __func__);
        return 0;
    }
}

cap_t Arch_createObject(object_t t, void *regionBase, word_t userSize, bool_t deviceMemory)
{
    switch (t) {
    case seL4_ARM_SmallPageObject:
        if (deviceMemory) {
            /** AUXUPD: "(True, ptr_retyps 1
                     (Ptr (ptr_val \<acute>regionBase) :: user_data_device_C ptr))" */
            /** GHOSTUPD: "(True, gs_new_frames vmpage_size.ARMSmallPage
                                                    (ptr_val \<acute>regionBase)
                                                    (unat ARMSmallPageBits))" */
        } else {
            /** AUXUPD: "(True, ptr_retyps 1
                     (Ptr (ptr_val \<acute>regionBase) :: user_data_C ptr))" */
            /** GHOSTUPD: "(True, gs_new_frames vmpage_size.ARMSmallPage
                                                    (ptr_val \<acute>regionBase)
                                                    (unat ARMSmallPageBits))" */
            cleanCacheRange_RAM((word_t)regionBase,
                                (word_t)regionBase + ((1ul << (pageBitsForSize(ARMSmallPage))) - 1ul),
                                addrFromPPtr(regionBase));
        }
        return cap_frame_cap_new(
                   asidInvalid, /* capFMappedASID */
                   (word_t)regionBase, /* capFBasePtr */
                   ARMSmallPage, /* capFSize */
                   0, /* capFMappedAddress */
                   VMReadWrite, /* capFVMRights */
                   !!deviceMemory /* capFIsDevice */
               );

    case seL4_ARM_LargePageObject:
        if (deviceMemory) {
            /** AUXUPD: "(True, ptr_retyps (2^9)
                     (Ptr (ptr_val \<acute>regionBase) :: user_data_device_C ptr))" */
            /** GHOSTUPD: "(True, gs_new_frames vmpage_size.ARMLargePage
                                                    (ptr_val \<acute>regionBase)
                                                    (unat ARMLargePageBits))" */
        } else {
            /** AUXUPD: "(True, ptr_retyps (2^9)
                     (Ptr (ptr_val \<acute>regionBase) :: user_data_C ptr))" */
            /** GHOSTUPD: "(True, gs_new_frames vmpage_size.ARMLargePage
                                                    (ptr_val \<acute>regionBase)
                                                    (unat ARMLargePageBits))" */
            cleanCacheRange_RAM((word_t)regionBase,
                                (word_t)regionBase + ((1ul << (pageBitsForSize(ARMLargePage))) - 1ul),
                                addrFromPPtr(regionBase));
        }
        return cap_frame_cap_new(
                   asidInvalid, /* capFMappedASID */
                   (word_t)regionBase, /* capFBasePtr */
                   ARMLargePage, /* capFSize */
                   0, /* capFMappedAddress */
                   VMReadWrite, /* capFVMRights */
                   !!deviceMemory /* capFIsDevice */
               );

    case seL4_ARM_HugePageObject:
        if (deviceMemory) {
            /** AUXUPD: "(True, ptr_retyps (2^18)
                     (Ptr (ptr_val \<acute>regionBase) :: user_data_device_C ptr))" */
            /** GHOSTUPD: "(True, gs_new_frames vmpage_size.ARMHugePage
                                                    (ptr_val \<acute>regionBase)
                                                    (unat ARMHugePageBits))" */
        } else {
            /** AUXUPD: "(True, ptr_retyps (2^18)
                     (Ptr (ptr_val \<acute>regionBase) :: user_data_C ptr))" */
            /** GHOSTUPD: "(True, gs_new_frames vmpage_size.ARMHugePage
                                                    (ptr_val \<acute>regionBase)
                                                    (unat ARMHugePageBits))" */
            cleanCacheRange_RAM((word_t)regionBase,
                                (word_t)regionBase + ((1ul << (pageBitsForSize(ARMHugePage))) - 1ul),
                                addrFromPPtr(regionBase));
        }
        return cap_frame_cap_new(
                   asidInvalid, /* capFMappedASID */
                   (word_t)regionBase, /* capFBasePtr */
                   ARMHugePage, /* capFSize */
                   0, /* capFMappedAddress */
                   VMReadWrite, /* capFVMRights */
                   !!deviceMemory /* capFIsDevice */
               );
    case seL4_ARM_VSpaceObject:
        /** AUXUPD: "(True, ptr_retyps 1
              (Ptr (ptr_val \<acute>regionBase) :: (pte_C[vs_array_len]) ptr))" */
        /** GHOSTUPD: "(True, gs_new_pt_t VSRootPT_T (ptr_val \<acute>regionBase))" */
        cleanCacheRange_PoU((word_t)regionBase,
                            (word_t)regionBase + ((1ul << (12)) - 1ul),
                            addrFromPPtr(regionBase));
# 463 "/home/b/test/sel4test/kernel/src/arch/arm/64/object/objecttype.c"
        return cap_vspace_cap_new(
                   asidInvalid, /* capVSMappedASID */
                   (word_t)regionBase, /* capVSBasePtr    */
                   0 /* capVSIsMapped   */
               );

    case seL4_ARM_PageTableObject:
        /** AUXUPD: "(True, ptr_retyps 1
              (Ptr (ptr_val \<acute>regionBase) :: (pte_C[pt_array_len]) ptr))" */
        /** GHOSTUPD: "(True, gs_new_pt_t NormalPT_T (ptr_val \<acute>regionBase))" */
        cleanCacheRange_PoU((word_t)regionBase,
                            (word_t)regionBase + ((1ul << (12)) - 1ul),
                            addrFromPPtr(regionBase));
        return cap_page_table_cap_new(
                   asidInvalid, /* capPTMappedASID    */
                   (word_t)regionBase, /* capPTBasePtr       */
                   0, /* capPTIsMapped      */
                   0 /* capPTMappedAddress */
               );
# 491 "/home/b/test/sel4test/kernel/src/arch/arm/64/object/objecttype.c"
    default:
        _fail("Arch_createObject got an API type or invalid object type", "/home/b/test/sel4test/kernel/src/arch/arm/64/object/objecttype.c", 492, __func__);
    }
}

exception_t Arch_decodeInvocation(word_t label, word_t length, cptr_t cptr,
                                  cte_t *slot, cap_t cap,
                                  bool_t call, word_t *buffer)
{

    /* The C parser cannot handle a switch statement with only a default
     * case. So we need to do some gymnastics to remove the switch if
     * there are no other cases */

    switch (cap_get_capType(cap)) {





    case cap_sgi_signal_cap:
        return decodeSGISignalInvocation(label, length, cap, buffer);
# 528 "/home/b/test/sel4test/kernel/src/arch/arm/64/object/objecttype.c"
    default:



    return decodeARMMMUInvocation(label, length, cptr, slot, cap, call, buffer);
}
}

void
Arch_prepareThreadDelete(tcb_t * thread) {







    fpuRelease(thread);

}
# 1 "/home/b/test/sel4test/kernel/src/arch/arm/api/faults.c"
/*
 * Copyright 2020, Data61, CSIRO (ABN 41 687 119 230)
 *
 * SPDX-License-Identifier: GPL-2.0-only
 */
# 14 "/home/b/test/sel4test/kernel/src/arch/arm/api/faults.c"
bool_t Arch_handleFaultReply(tcb_t *receiver, tcb_t *sender, word_t faultType)
{
    switch (faultType) {
    case seL4_Fault_VMFault:
        return true;
# 28 "/home/b/test/sel4test/kernel/src/arch/arm/api/faults.c"
    default:
        _fail("Invalid fault", "/home/b/test/sel4test/kernel/src/arch/arm/api/faults.c", 29, __func__);
    }
}

word_t Arch_setMRs_fault(tcb_t *sender, tcb_t *receiver, word_t *receiveIPCBuffer, word_t faultType)
{
    switch (faultType) {
    case seL4_Fault_VMFault: {
        setMR(receiver, receiveIPCBuffer, seL4_VMFault_IP, getRestartPC(sender));
        setMR(receiver, receiveIPCBuffer, seL4_VMFault_Addr,
              seL4_Fault_VMFault_get_address(sender->tcbFault));
        setMR(receiver, receiveIPCBuffer, seL4_VMFault_PrefetchFault,
              seL4_Fault_VMFault_get_instructionFault(sender->tcbFault));
        return setMR(receiver, receiveIPCBuffer, seL4_VMFault_FSR,
                     seL4_Fault_VMFault_get_FSR(sender->tcbFault));
    }
# 60 "/home/b/test/sel4test/kernel/src/arch/arm/api/faults.c"
    default:
        _fail("Invalid fault", "/home/b/test/sel4test/kernel/src/arch/arm/api/faults.c", 61, __func__);
    }
}
# 1 "/home/b/test/sel4test/kernel/src/arch/arm/armv/armv8-a/64/cache.c"
/*
 * Copyright 2020, Data61, CSIRO (ABN 41 687 119 230)
 *
 * SPDX-License-Identifier: GPL-2.0-only
 */



static inline void cleanByWSL(word_t wsl)
{
    __asm__ volatile("dc csw, %0" : : "r"(wsl));
}

static inline void cleanInvalidateByWSL(word_t wsl)
{
    __asm__ volatile("dc cisw, %0" : : "r"(wsl));
}

static inline word_t readCLID(void)
{
    word_t CLID;
    __asm__ volatile("mrs %x0," "clidr_el1" : "=r"(CLID));
    return CLID;
}






enum arm_cache_type {
    ARMCacheI = 1,
    ARMCacheD = 2,
    ARMCacheID = 3,
};

static inline word_t readCacheSize(int level, bool_t instruction)
{
    word_t size, csselr_old;
    /* Save CSSELR */
    __asm__ volatile("mrs %x0," "csselr_el1" : "=r"(csselr_old));
    /* Select cache level */
    do { word_t _v = ((level << 1) | instruction); __asm__ volatile("msr " "csselr_el1" ",%x0" :: "r" (_v)); }while(0);
    /* Read 'size' */
    __asm__ volatile("mrs %x0," "ccsidr_el1" : "=r"(size));
    /* Restore CSSELR */
    do { word_t _v = csselr_old; __asm__ volatile("msr " "csselr_el1" ",%x0" :: "r" (_v)); }while(0);
    return size;
}





void clean_D_PoU(void)
{
    int clid = readCLID();
    int lou = (((clid) >> 27) & ((1ul << (3)) - 1ul));

    for (int l = 0; l < lou; l++) {
        if ((((clid) >> (l*3)) & ((1ul << (3)) - 1ul)) > ARMCacheI) {
            word_t lsize = readCacheSize(l, 0);
            int lbits = (((lsize) & ((1ul << (3)) - 1ul)) + 4);
            int assoc = ((((lsize) >> 3) & ((1ul << (10)) - 1ul)) + 1);
            int assoc_bits = (1 << 6) - clzl(assoc - 1);
            int nsets = ((((lsize) >> 13) & ((1ul << (15)) - 1ul)) + 1);
            for (int w = 0; w < assoc; w++) {
                for (int s = 0; s < nsets; s++) {
                    cleanByWSL((w << (32 - assoc_bits)) |
                               (s << lbits) | (l << 1));
                }
            }
        }
    }
}

static inline void cleanInvalidate_D_by_level(int l)
{
    word_t lsize = readCacheSize(l, 0);
    int lbits = (((lsize) & ((1ul << (3)) - 1ul)) + 4);
    int assoc = ((((lsize) >> 3) & ((1ul << (10)) - 1ul)) + 1);
    int assoc_bits = (1 << 6) - clzl(assoc - 1);
    int nsets = ((((lsize) >> 13) & ((1ul << (15)) - 1ul)) + 1);

    for (int w = 0; w < assoc; w++) {
        for (int s = 0; s < nsets; s++) {
            cleanInvalidateByWSL((w << (32 - assoc_bits)) |
                                 (s << lbits) | (l << 1));
        }
    }
}

void cleanInvalidate_D_PoC(void)
{
    int clid = readCLID();
    int loc = (((clid) >> 24) & ((1ul << (3)) - 1ul));

    for (int l = 0; l < loc; l++) {
        if ((((clid) >> (l*3)) & ((1ul << (3)) - 1ul)) > ARMCacheI) {
            cleanInvalidate_D_by_level(l);
        }
    }
}

void cleanInvalidate_L1D(void)
{
    cleanInvalidate_D_by_level(0);
}
# 1 "/home/b/test/sel4test/kernel/src/arch/arm/armv/armv8-a/64/user_access.c"
/*
 * Copyright 2020, Data61, CSIRO (ABN 41 687 119 230)
 *
 * SPDX-License-Identifier: GPL-2.0-only
 */


# 1 "/home/b/test/sel4test/kernel/include/arch/arm/arch/user_access.h" 1
/*
 * Copyright 2014, General Dynamics C4 Systems
 *
 * SPDX-License-Identifier: GPL-2.0-only
 */
       

void armv_init_user_access(void);
# 9 "/home/b/test/sel4test/kernel/src/arch/arm/armv/armv8-a/64/user_access.c" 2

/* bits in the CNTKCTL_EL1 */





/* bits in CNTHCTL_EL2 */





static void check_export_pmu(void)
{





}

static void check_export_arch_timer(void)
{
    uint32_t val = 0;

    val |= (1ul << (0));
# 46 "/home/b/test/sel4test/kernel/src/arch/arm/armv/armv8-a/64/user_access.c"
    do { word_t _v = val; __asm__ volatile("msr " "CNTKCTL_EL1" ",%x0" :: "r" (_v)); }while(0);
# 58 "/home/b/test/sel4test/kernel/src/arch/arm/armv/armv8-a/64/user_access.c"
}

void armv_init_user_access(void)
{
    check_export_pmu();
    check_export_arch_timer();
}
# 1 "/home/b/test/sel4test/kernel/src/arch/arm/benchmark/benchmark.c"
/*
 * Copyright 2014, General Dynamics C4 Systems
 *
 * SPDX-License-Identifier: GPL-2.0-only
 */
# 1 "/home/b/test/sel4test/kernel/src/arch/arm/c_traps.c"
/*
 * Copyright 2020, Data61, CSIRO (ABN 41 687 119 230)
 *
 * SPDX-License-Identifier: GPL-2.0-only
 */
# 20 "/home/b/test/sel4test/kernel/src/arch/arm/c_traps.c"
void __attribute__((externally_visible)) __attribute__((__noreturn__)) c_handle_undefined_instruction(void)
{
    do {} while (0);
    c_entry_hook();


    ksKernelEntry.path = Entry_UserLevelFault;
    ksKernelEntry.word = getRegister(ksCurThread, NextIP);


    /* There's only one user-level fault on ARM, and the code is (0,0) */



    word_t esr = getESR();

    if (isDebugFault(esr)) {
        handleDebugFaultEvent(esr);
    } else

    {
        handleUserLevelFault(esr, 0);
    }

    restore_user_context();
    __builtin_unreachable();
}
# 57 "/home/b/test/sel4test/kernel/src/arch/arm/c_traps.c"
static inline void __attribute__((__noreturn__)) c_handle_vm_fault(vm_fault_type_t type)
{
    do {} while (0);
    c_entry_hook();


    ksKernelEntry.path = Entry_VMFault;
    ksKernelEntry.word = getRegister(ksCurThread, NextIP);
    ksKernelEntry.is_fastpath = false;





    handleVMFaultEvent(type);
    restore_user_context();

    __builtin_unreachable();
}

void __attribute__((externally_visible)) __attribute__((__noreturn__)) c_handle_data_fault(void)
{
    c_handle_vm_fault(0);
}

void __attribute__((externally_visible)) __attribute__((__noreturn__)) c_handle_instruction_fault(void)
{
    c_handle_vm_fault(1);
}

void __attribute__((externally_visible)) __attribute__((__noreturn__)) c_handle_interrupt(void)
{
    do {} while (0);
    c_entry_hook();


    ksKernelEntry.path = Entry_Interrupt;
    ksKernelEntry.word = (getActiveIRQ());
    ksKernelEntry.core = 0lu;


    handleInterruptEntry();
    restore_user_context();
}

void __attribute__((__noreturn__)) slowpath(syscall_t syscall)
{
    if (__builtin_expect(!!(syscall < (-8) || syscall > (-1)), 0)) {

        ksKernelEntry.path = Entry_UnknownSyscall;
        /* ksKernelEntry.word word is already set to syscall */

        /* Contrary to the name, this handles all non-standard syscalls used in
         * debug builds also.
         */
        handleUnknownSyscall(syscall);
    } else {

        ksKernelEntry.is_fastpath = 0;

        handleSyscall(syscall);
    }

    restore_user_context();
    __builtin_unreachable();
}

void __attribute__((externally_visible)) c_handle_syscall(word_t cptr, word_t msgInfo, syscall_t syscall)
{
    do {} while (0);

    c_entry_hook();

    benchmark_debug_syscall_start(cptr, msgInfo, syscall);
    ksKernelEntry.is_fastpath = 0;


    slowpath(syscall);
    __builtin_unreachable();
}


__attribute__((__aligned__((1ul << (6)))))
void __attribute__((externally_visible)) c_handle_fastpath_call(word_t cptr, word_t msgInfo)
{
    do {} while (0);

    c_entry_hook();

    benchmark_debug_syscall_start(cptr, msgInfo, SysCall);
    ksKernelEntry.is_fastpath = 1;


    fastpath_call(cptr, msgInfo);
    __builtin_unreachable();
}
# 172 "/home/b/test/sel4test/kernel/src/arch/arm/c_traps.c"
__attribute__((__aligned__((1ul << (6)))))



void __attribute__((externally_visible)) c_handle_fastpath_reply_recv(word_t cptr, word_t msgInfo)

{
    do {} while (0);

    c_entry_hook();

    benchmark_debug_syscall_start(cptr, msgInfo, SysReplyRecv);
    ksKernelEntry.is_fastpath = 1;





    fastpath_reply_recv(cptr, msgInfo);

    __builtin_unreachable();
}
# 1 "/home/b/test/sel4test/kernel/src/arch/arm/kernel/boot.c"
/*
 * Copyright 2014, General Dynamics C4 Systems
 * Copyright 2021, HENSOLDT Cyber
 *
 * SPDX-License-Identifier: GPL-2.0-only
 */
# 15 "/home/b/test/sel4test/kernel/src/arch/arm/kernel/boot.c"
# 1 "/home/b/test/sel4test/kernel/include/arch/arm/arch/kernel/boot.h" 1
/*
 * Copyright 2014, General Dynamics C4 Systems
 *
 * SPDX-License-Identifier: GPL-2.0-only
 */

       



cap_t create_unmapped_it_frame_cap(pptr_t pptr, bool_t use_large);
cap_t create_mapped_it_frame_cap(cap_t pd_cap, pptr_t pptr, vptr_t vptr, asid_t asid, bool_t use_large,
                                 bool_t executable);

void init_kernel(
    paddr_t ui_p_reg_start,
    paddr_t ui_p_reg_end,
    sword_t pv_offset,
    vptr_t v_entry,
    paddr_t dtb_addr_p,
    uint32_t dtb_size
);
# 16 "/home/b/test/sel4test/kernel/src/arch/arm/kernel/boot.c" 2
# 40 "/home/b/test/sel4test/kernel/src/arch/arm/kernel/boot.c"
__attribute__((__section__(".boot.bss"))) static region_t reserved[(3 + 0)];

__attribute__((__section__(".boot.text"))) static bool_t arch_init_freemem(p_region_t ui_p_reg,
                                          p_region_t dtb_p_reg,
                                          v_region_t it_v_reg,
                                          word_t extra_bi_size_bits)
{
    /* reserve the kernel image region */
    reserved[0] = paddr_to_pptr_reg(get_p_reg_kernel_img());

    int index = 1;

    /* add the dtb region, if it is not empty */
    if (dtb_p_reg.start) {
        if (index >= (sizeof(reserved) / sizeof((reserved)[0]))) {
            printf("ERROR: no slot to add DTB to reserved regions\n");
            return false;
        }
        reserved[index].start = (pptr_t) ptrFromPAddr(dtb_p_reg.start);
        reserved[index].end = (pptr_t) ptrFromPAddr(dtb_p_reg.end);
        index++;
    }

    /* Reserve the user image region and the mode-reserved regions. For now,
     * only one mode-reserved region is supported, because this is all that is
     * needed.
     */
    if (0 > 1) {
        printf("ERROR: MODE_RESERVED > 1 unsupported!\n");
        return false;
    }
    if (ui_p_reg.start < (0xffffffffc0000000ul - (0xffffff8000000000ul - 0x0ul))) {
        region_t ui_reg = paddr_to_pptr_reg(ui_p_reg);
        if (0 == 1) {
            if (index + 1 >= (sizeof(reserved) / sizeof((reserved)[0]))) {
                printf("ERROR: no slot to add the user image and the "
                       "mode-reserved region to the reserved regions\n");
                return false;
            }
            if (ui_reg.end > mode_reserved_region[0].start) {
                reserved[index] = mode_reserved_region[0];
                index++;
                reserved[index] = ui_reg;
            } else {
                reserved[index] = ui_reg;
                index++;
                reserved[index] = mode_reserved_region[0];
            }
            index++;
        } else {
            if (index >= (sizeof(reserved) / sizeof((reserved)[0]))) {
                printf("ERROR: no slot to add the user image to the reserved"
                       "regions\n");
                return false;
            }
            reserved[index] = ui_reg;
            index++;
        }
    } else {
        if (0 == 1) {
            if (index >= (sizeof(reserved) / sizeof((reserved)[0]))) {
                printf("ERROR: no slot to add the mode-reserved region\n");
                return false;
            }
            reserved[index] = mode_reserved_region[0];
            index++;
        }

        /* Reserve the ui_p_reg region still so it doesn't get turned into device UT. */
        reserve_region(ui_p_reg);
    }

    /* avail_p_regs comes from the auto-generated code */
    return init_freemem((sizeof(avail_p_regs) / sizeof((avail_p_regs)[0])), avail_p_regs,
                        index, reserved,
                        it_v_reg, extra_bi_size_bits);
}


__attribute__((__section__(".boot.text"))) static void init_irqs(cap_t root_cnode_cap)
{
    unsigned i;

    for (i = 0; i <= maxIRQ ; i++) {
        setIRQState(IRQInactive, (i));
    }
    setIRQState(IRQTimer, (27));
# 152 "/home/b/test/sel4test/kernel/src/arch/arm/kernel/boot.c"
    /* provide the IRQ control cap */
    write_slot((((slot_ptr_t)(((pptr_t)cap_get_capPtr(root_cnode_cap)))) + (seL4_CapIRQControl)), cap_irq_control_cap_new());
}
# 175 "/home/b/test/sel4test/kernel/src/arch/arm/kernel/boot.c"
/** This and only this function initialises the CPU.
 *
 * It does NOT initialise any kernel state.
 * @return For the verification build, this currently returns true always.
 */
__attribute__((__section__(".boot.text"))) static bool_t init_cpu(void)
{
    bool_t haveHWFPU;


    if (wrap_config_set(0)) {
        if (!checkTCR_EL2()) {
            return false;
        }
    }


    activate_kernel_vspace();
    if (wrap_config_set(0)) {
        do {} while(0);
    }


    if (!Arch_initHardwareBreakpoints()) {
        printf("Kernel built with CONFIG_HARDWARE_DEBUG_API, but this board doesn't "
               "reliably support it.\n");
        return false;
    }


    /* Setup kernel stack pointer.
     * On ARM SMP, the array index here is the CPU ID
     */
    word_t stack_top = ((word_t) kernel_stack_alloc[0lu]) + (1ul << (12));
# 218 "/home/b/test/sel4test/kernel/src/arch/arm/kernel/boot.c"
    setKernelStack(stack_top);


    /* initialise CPU's exception vector table */
    setVtable((pptr_t)arm_vector_table);


    haveHWFPU = fpsimd_HWCapTest();

    /* Disable FPU to avoid channels where a platform has an FPU but doesn't make use of it */
    if (haveHWFPU) {
        disableFpu();
    }


    if (haveHWFPU) {
        if (!fpsimd_init()) {
            return false;
        }
    } else {
        printf("Platform claims to have FP hardware, but does not!\n");
        return false;
    }


    cpu_initLocalIRQController();





    /* Export selected CPU features for access by PL0 */
    armv_init_user_access();

    initTimer();

    return true;
}

/* This and only this function initialises the platform. It does NOT initialise any kernel state. */

__attribute__((__section__(".boot.text"))) static void init_plat(void)
{
    initIRQController();
    initL2Cache();



}
# 332 "/home/b/test/sel4test/kernel/src/arch/arm/kernel/boot.c"
/* Main kernel initialisation function. */

static __attribute__((__section__(".boot.text"))) bool_t try_init_kernel(
    paddr_t ui_p_reg_start,
    paddr_t ui_p_reg_end,
    sword_t pv_offset,
    vptr_t v_entry,
    paddr_t dtb_phys_addr,
    word_t dtb_size
)
{
    cap_t root_cnode_cap;
    cap_t it_ap_cap;
    cap_t it_pd_cap;
    cap_t ipcbuf_cap;
    p_region_t ui_p_reg = (p_region_t) {
        ui_p_reg_start, ui_p_reg_end
    };
    region_t ui_reg = paddr_to_pptr_reg(ui_p_reg);
    word_t extra_bi_size = 0;
    pptr_t extra_bi_offset = 0;
    vptr_t extra_bi_frame_vptr;
    vptr_t bi_frame_vptr;
    vptr_t ipcbuf_vptr;
    create_frames_of_region_ret_t create_frames_ret;
    create_frames_of_region_ret_t extra_bi_ret;

    /* convert from physical addresses to userland vptrs */
    v_region_t ui_v_reg = {
        .start = ui_p_reg_start - pv_offset,
        .end = ui_p_reg_end - pv_offset
    };

    ipcbuf_vptr = ui_v_reg.end;
    bi_frame_vptr = ipcbuf_vptr + (1ul << (12));
    extra_bi_frame_vptr = bi_frame_vptr + (1ul << (12));

    /* setup virtual memory for the kernel */
    map_kernel_window();

    /* initialise the CPU */
    if (!init_cpu()) {
        printf("ERROR: CPU init failed\n");
        return false;
    }

    /* debug output via serial port is only available from here */
    printf("Bootstrapping kernel\n");

    /* initialise the platform */
    init_plat();

    /* If a DTB was provided, pass the data on as extra bootinfo */
    p_region_t dtb_p_reg = (p_region_t){ .start = 0, .end = 0 };
    if (dtb_size > 0) {
        paddr_t dtb_phys_end = dtb_phys_addr + dtb_size;
        if (dtb_phys_end < dtb_phys_addr) {
            /* An integer overflow happened in DTB end address calculation, the
             * location or size passed seems invalid.
             */
            printf("ERROR: DTB location at %""lx"
                   " len %""lu"" invalid\n",
                   dtb_phys_addr, dtb_size);
            return false;
        }
        /* If the DTB is located in physical memory that is not mapped in the
         * kernel window we cannot access it.
         */
        if (dtb_phys_end >= (0xffffffffc0000000ul - (0xffffff8000000000ul - 0x0ul))) {
            printf("ERROR: DTB at [%""lx""..%""lx"") "
                   "exceeds PADDR_TOP (%""lx"")\n",
                   dtb_phys_addr, dtb_phys_end, (0xffffffffc0000000ul - (0xffffff8000000000ul - 0x0ul)));
            return false;
        }
        /* DTB seems valid and accessible, pass it on in bootinfo. */
        extra_bi_size += sizeof(seL4_BootInfoHeader) + dtb_size;
        /* Remember the memory region it uses. */
        dtb_p_reg = (p_region_t) {
            .start = dtb_phys_addr,
            .end = dtb_phys_end
        };
    }

    /* The region of the initial thread is the user image + ipcbuf and boot info */
    word_t extra_bi_size_bits = calculate_extra_bi_size_bits(extra_bi_size);
    v_region_t it_v_reg = {
        .start = ui_v_reg.start,
        .end = extra_bi_frame_vptr + (extra_bi_size_bits > 0 ? (1ul << (extra_bi_size_bits)) : 0)
    };
    if (it_v_reg.end >= 0x00007fffffffffff) {
        /* Variable arguments for printf() require well defined integer types to
         * work properly. Unfortunately, the definition of USER_TOP differs
         * between platforms (int, long), so we have to cast here to play safe.
         */
        printf("ERROR: userland image virt [%""lx""..%""lx"")"
               "exceeds USER_TOP (%""lx"")\n",
               it_v_reg.start, it_v_reg.end, (word_t)0x00007fffffffffff);
        return false;
    }

    if (!arch_init_freemem(ui_p_reg, dtb_p_reg, it_v_reg, extra_bi_size_bits)) {
        printf("ERROR: free memory management initialization failed\n");
        return false;
    }

    /* create the root cnode */
    root_cnode_cap = create_root_cnode();
    if (cap_get_capType(root_cnode_cap) == cap_null_cap) {
        printf("ERROR: root c-node creation failed\n");
        return false;
    }

    /* create the cap for managing thread domains */
    create_domain_cap(root_cnode_cap);

    /* initialise the IRQ states and provide the IRQ control cap */
    init_irqs(root_cnode_cap);
# 458 "/home/b/test/sel4test/kernel/src/arch/arm/kernel/boot.c"
    populate_bi_frame(0, 1, ipcbuf_vptr, extra_bi_size);

    /* put DTB in the bootinfo block, if present. */
    seL4_BootInfoHeader header;
    if (dtb_size > 0) {
        header.id = SEL4_BOOTINFO_HEADER_FDT;
        header.len = sizeof(header) + dtb_size;
        *(seL4_BootInfoHeader *)(rootserver.extra_bi + extra_bi_offset) = header;
        extra_bi_offset += sizeof(header);
        memcpy((void *)(rootserver.extra_bi + extra_bi_offset),
               ptrFromPAddr(dtb_phys_addr),
               dtb_size);
        extra_bi_offset += dtb_size;
    }

    if (extra_bi_size > extra_bi_offset) {
        /* provide a chunk for any leftover padding in the extended boot info */
        header.id = SEL4_BOOTINFO_HEADER_PADDING;
        header.len = (extra_bi_size - extra_bi_offset);
        *(seL4_BootInfoHeader *)(rootserver.extra_bi + extra_bi_offset) = header;
    }

    if (wrap_config_set(0)) {
        ndks_boot.bi_frame->ioSpaceCaps = create_iospace_caps(root_cnode_cap);
        if (ndks_boot.bi_frame->ioSpaceCaps.start == 0 &&
            ndks_boot.bi_frame->ioSpaceCaps.end == 0) {
            printf("ERROR: SMMU I/O space creation failed\n");
            return false;
        }
    } else {
        ndks_boot.bi_frame->ioSpaceCaps = (seL4_SlotRegion){ .start = 0, .end = 0 };
    }

    /* Construct an initial address space with enough virtual addresses
     * to cover the user image + ipc buffer and bootinfo frames */
    it_pd_cap = create_it_address_space(root_cnode_cap, it_v_reg);
    if (cap_get_capType(it_pd_cap) == cap_null_cap) {
        printf("ERROR: address space creation for initial thread failed\n");
        return false;
    }

    /* Create and map bootinfo frame cap */
    create_bi_frame_cap(
        root_cnode_cap,
        it_pd_cap,
        bi_frame_vptr
    );

    /* create and map extra bootinfo region */
    if (extra_bi_size > 0) {
        region_t extra_bi_region = {
            .start = rootserver.extra_bi,
            .end = rootserver.extra_bi + extra_bi_size
        };
        extra_bi_ret =
            create_frames_of_region(
                root_cnode_cap,
                it_pd_cap,
                extra_bi_region,
                true,
                addrFromPPtr((void *)extra_bi_region.start) - extra_bi_frame_vptr
            );
        if (!extra_bi_ret.success) {
            printf("ERROR: mapping extra boot info to initial thread failed\n");
            return false;
        }
        ndks_boot.bi_frame->extraBIPages = extra_bi_ret.region;
    }





    /* create the initial thread's IPC buffer */
    ipcbuf_cap = create_ipcbuf_frame_cap(root_cnode_cap, it_pd_cap, ipcbuf_vptr);
    if (cap_get_capType(ipcbuf_cap) == cap_null_cap) {
        printf("ERROR: could not create IPC buffer for initial thread\n");
        return false;
    }

    /* create all userland image frames */
    create_frames_ret =
        create_frames_of_region(
            root_cnode_cap,
            it_pd_cap,
            ui_reg,
            true,
            pv_offset
        );
    if (!create_frames_ret.success) {
        printf("ERROR: could not create all userland image frames\n");
        return false;
    }
    ndks_boot.bi_frame->userImageFrames = create_frames_ret.region;

    /* create/initialise the initial thread's ASID pool */
    it_ap_cap = create_it_asid_pool(root_cnode_cap);
    if (cap_get_capType(it_ap_cap) == cap_null_cap) {
        printf("ERROR: could not create ASID pool for initial thread\n");
        return false;
    }
    write_it_asid_pool(it_ap_cap, it_pd_cap);





    /* create the idle thread */
    create_idle_thread();

    /* Before creating the initial thread (which also switches to it)
     * we clean the cache so that any page table information written
     * as a result of calling create_frames_of_region will be correctly
     * read by the hardware page table walker */
    cleanInvalidateL1Caches();

    /* create the initial thread */
    tcb_t *initial = create_initial_thread(
                         root_cnode_cap,
                         it_pd_cap,
                         v_entry,
                         bi_frame_vptr,
                         ipcbuf_vptr,
                         ipcbuf_cap
                     );

    if (initial == ((void *)0)) {
        printf("ERROR: could not create initial thread\n");
        return false;
    }

    init_core_state(initial);

    /* create all of the untypeds. Both devices and kernel window memory */
    if (!create_untypeds(root_cnode_cap)) {
        printf("ERROR: could not create untypteds for kernel image boot memory\n");
        return false;
    }

    /* no shared-frame caps (ARM has no multikernel support) */
    ndks_boot.bi_frame->sharedFrames = (seL4_SlotRegion){ .start = 0, .end = 0 };

    /* finalise the bootinfo frame */
    bi_finalise();

    /* Flushing the L1 cache and invalidating the TLB is good enough here to
     * make sure everything written by the kernel is visible to userland. There
     * are no uncached userland frames at this stage that require enforcing
     * flushing to RAM. Any retyping operation will clean the memory down to RAM
     * anyway.
     */
    cleanInvalidateL1Caches();
    invalidateLocalTLB();
    if (wrap_config_set(0)) {
        invalidateHypTLB();
    }

    ksNumCPUs = 1;

    /* initialize BKL before booting up other cores */
    ;
    ;

    /* All cores are up now, so there can be concurrency. The kernel booting is
     * supposed to be finished before the secondary cores are released, all the
     * primary has to do now is schedule the initial thread. Currently there is
     * nothing that touches any global data structures, nevertheless we grab the
     * BKL here to play safe. It is released when the kernel is left. */
    do {} while (0);

    printf("Booting all finished, dropped to user space\n");

    /* kernel successfully initialized */
    return true;
}

__attribute__((__section__(".boot.text"))) __attribute__((externally_visible)) void init_kernel(
    paddr_t ui_p_reg_start,
    paddr_t ui_p_reg_end,
    sword_t pv_offset,
    vptr_t v_entry,
    paddr_t dtb_addr_p,
    uint32_t dtb_size
)
{
    bool_t result;
# 658 "/home/b/test/sel4test/kernel/src/arch/arm/kernel/boot.c"
    result = try_init_kernel(ui_p_reg_start,
                             ui_p_reg_end,
                             pv_offset,
                             v_entry,
                             dtb_addr_p, dtb_size);



    if (!result) {
        _fail("ERROR: kernel init failed", "/home/b/test/sel4test/kernel/src/arch/arm/kernel/boot.c", 667, __func__);
        __builtin_unreachable();
    }





    schedule();
    activateThread();
}
# 1 "/home/b/test/sel4test/kernel/src/arch/arm/kernel/thread.c"
/*
 * Copyright 2020, Data61, CSIRO (ABN 41 687 119 230)
 *
 * SPDX-License-Identifier: GPL-2.0-only
 */



void Arch_postModifyRegisters(tcb_t *tptr)
{
    /* Nothing to do */
}
# 1 "/home/b/test/sel4test/kernel/src/arch/arm/machine/cache.c"
/*
 * Copyright 2014, General Dynamics C4 Systems
 *
 * SPDX-License-Identifier: GPL-2.0-only
 */




# 1 "/home/b/test/sel4test/kernel/include/arch/arm/arch/machine/l2c_310.h" 1
/*
 * Copyright 2014, General Dynamics C4 Systems
 *
 * SPDX-License-Identifier: GPL-2.0-only
 */

/*
 * ARM L2 Cache controller L2C-310
 */

       




void initL2Cache(void);

void plat_cleanInvalidateL2Cache(void);
void plat_cleanCache(void);
void plat_cleanL2Range(paddr_t start, paddr_t end);
void plat_invalidateL2Range(paddr_t start, paddr_t end);
void plat_cleanInvalidateL2Range(paddr_t start, paddr_t end);
# 11 "/home/b/test/sel4test/kernel/src/arch/arm/machine/cache.c" 2




static void cleanCacheRange_PoC(vptr_t start, vptr_t end, paddr_t pstart)
{
    vptr_t line;
    word_t index;

    for (index = ((((start) >> (6)) << (6))>>6); index < ((((end) >> (6)) << (6))>>6) + 1; index++) {
        line = index << 6;
        cleanByVA(line, pstart + (line - start));
    }
}

void cleanInvalidateCacheRange_RAM(vptr_t start, vptr_t end, paddr_t pstart)
{
    vptr_t line;
    word_t index;
    /** GHOSTUPD: "((gs_get_assn cap_get_capSizeBits_'proc \<acute>ghost'state = 0
            \<or> \<acute>end - \<acute>start <= gs_get_assn cap_get_capSizeBits_'proc \<acute>ghost'state)
        \<and> \<acute>start <= \<acute>end, id)" */

    /* First clean the L1 range */
    cleanCacheRange_PoC(start, end, pstart);

    /* ensure operation completes and visible in L2 */
    dsb();

    /* Now clean and invalidate the L2 range */
    plat_cleanInvalidateL2Range(pstart, pstart + (end - start));

    /* Finally clean and invalidate the L1 range. The extra clean is only strictly necessary
     * in a multiprocessor environment to prevent a write being lost if another core is
     * attempting a store at the same time. As the range should already be clean asking
     * it to clean again should not affect performance */
    for (index = ((((start) >> (6)) << (6))>>6); index < ((((end) >> (6)) << (6))>>6) + 1; index++) {
        line = index << 6;
        cleanInvalByVA(line, pstart + (line - start));
    }
    /* ensure clean and invalidate complete */
    dsb();
}

void cleanCacheRange_RAM(vptr_t start, vptr_t end, paddr_t pstart)
{
    /** GHOSTUPD: "((gs_get_assn cap_get_capSizeBits_'proc \<acute>ghost'state = 0
            \<or> \<acute>end - \<acute>start <= gs_get_assn cap_get_capSizeBits_'proc \<acute>ghost'state)
        \<and> \<acute>start <= \<acute>end
        \<and> \<acute>pstart <= \<acute>pstart + (\<acute>end - \<acute>start), id)" */

    /* clean l1 to l2 */
    cleanCacheRange_PoC(start, end, pstart);

    /* ensure cache operation completes before cleaning l2 */
    dsb();

    /** GHOSTUPD: "((gs_get_assn cap_get_capSizeBits_'proc \<acute>ghost'state = 0
            \<or> \<acute>end - \<acute>start <= gs_get_assn cap_get_capSizeBits_'proc \<acute>ghost'state)
        \<and> \<acute>start <= \<acute>end
        \<and> \<acute>pstart <= \<acute>pstart + (\<acute>end - \<acute>start), id)" */

    /* now clean l2 to RAM */
    plat_cleanL2Range(pstart, pstart + (end - start));
}

void cleanCacheRange_PoU(vptr_t start, vptr_t end, paddr_t pstart)
{
    vptr_t line;
    word_t index;

    /** GHOSTUPD: "((gs_get_assn cap_get_capSizeBits_'proc \<acute>ghost'state = 0
            \<or> \<acute>end - \<acute>start <= gs_get_assn cap_get_capSizeBits_'proc \<acute>ghost'state)
        \<and> \<acute>start <= \<acute>end
        \<and> \<acute>pstart <= \<acute>pstart + (\<acute>end - \<acute>start), id)" */

    for (index = ((((start) >> (6)) << (6))>>6); index < ((((end) >> (6)) << (6))>>6) + 1; index++) {
        line = index << 6;
        cleanByVA_PoU(line, pstart + (line - start));
    }
}

void invalidateCacheRange_RAM(vptr_t start, vptr_t end, paddr_t pstart)
{
    vptr_t line;
    word_t index;

    /* If the start and end are not aligned to a cache line boundary
     * then we need to clean the line first to prevent invalidating
     * bytes we didn't mean to. Calling the functions in this way is
     * not the most efficient method, but we assume the user will
     * rarely be this silly */
    if (start != (((start) >> (6)) << (6))) {
        cleanCacheRange_RAM(start, start, pstart);
    }
    if (end + 1 != (((end + 1) >> (6)) << (6))) {
        line = (((end) >> (6)) << (6));
        cleanCacheRange_RAM(line, line, pstart + (line - start));
    }

    /** GHOSTUPD: "((gs_get_assn cap_get_capSizeBits_'proc \<acute>ghost'state = 0
            \<or> \<acute>end - \<acute>start <= gs_get_assn cap_get_capSizeBits_'proc \<acute>ghost'state)
        \<and> \<acute>start <= \<acute>end
        \<and> \<acute>pstart <= \<acute>pstart + (\<acute>end - \<acute>start), id)" */

    /* Invalidate L2 range. Invalidating the L2 before the L1 is the order
     * given in the l2c_310 manual, as an L1 line might be allocated from the L2
     * before the L2 can be invalidated. */
    plat_invalidateL2Range(pstart, pstart + (end - start));

    /** GHOSTUPD: "((gs_get_assn cap_get_capSizeBits_'proc \<acute>ghost'state = 0
            \<or> \<acute>end - \<acute>start <= gs_get_assn cap_get_capSizeBits_'proc \<acute>ghost'state)
        \<and> \<acute>start <= \<acute>end
        \<and> \<acute>pstart <= \<acute>pstart + (\<acute>end - \<acute>start), id)" */

    /* Now invalidate L1 range */
    for (index = ((((start) >> (6)) << (6))>>6); index < ((((end) >> (6)) << (6))>>6) + 1; index++) {
        line = index << 6;
        invalidateByVA(line, pstart + (line - start));
    }
    /* Ensure invalidate completes */
    dsb();
}

void invalidateCacheRange_I(vptr_t start, vptr_t end, paddr_t pstart)
{
# 149 "/home/b/test/sel4test/kernel/src/arch/arm/machine/cache.c"
    vptr_t line;
    word_t index;

    for (index = ((((start) >> (6)) << (6))>>6); index < ((((end) >> (6)) << (6))>>6) + 1; index++) {
        line = index << 6;
        invalidateByVA_I(line, pstart + (line - start));
    }

}

void branchFlushRange(vptr_t start, vptr_t end, paddr_t pstart)
{
    vptr_t line;
    word_t index;

    for (index = ((((start) >> (6)) << (6))>>6); index < ((((end) >> (6)) << (6))>>6) + 1; index++) {
        line = index << 6;
        branchFlush(line, pstart + (line - start));
    }
}

void cleanCaches_PoU(void)
{
    dsb();
    clean_D_PoU();
    dsb();
    invalidate_I_PoU();
    dsb();
}

void cleanInvalidateL1Caches(void)
{
    dsb();
    cleanInvalidate_D_PoC();
    dsb();
    invalidate_I_PoU();
    dsb();
}

void arch_clean_invalidate_caches(void)
{
    cleanCaches_PoU();
    plat_cleanInvalidateL2Cache();
    cleanInvalidateL1Caches();
    isb();
}

void arch_clean_invalidate_L1_caches(word_t type)
{
    dsb();
    if (type & (1ul << (1))) {
        cleanInvalidate_L1D();
        dsb();
    }
    if (type & (1ul << (0))) {
        invalidate_I_PoU();
        dsb();
        isb();
    }
}
# 1 "/home/b/test/sel4test/kernel/src/arch/arm/machine/debug.c"
/*
 * Copyright 2020, Data61, CSIRO (ABN 41 687 119 230)
 *
 * SPDX-License-Identifier: GPL-2.0-only
 */
# 21 "/home/b/test/sel4test/kernel/src/arch/arm/machine/debug.c"
/* ARMv7 Manuals, c3.3.1:
 *  "Breakpoint debug events are synchronous. That is, the debug event acts
 *  like an exception that cancels the breakpointed instruction."
 *
 * ARMv7 Manuals, c3.4.1:
 *  "Watchpoint debug events are precise and can be synchronous or asynchronous:
 *  a synchronous Watchpoint debug event acts like a synchronous abort
 *  exception on the memory access instruction itself. An asynchronous
 *  Watchpoint debug event acts like a precise asynchronous abort exception that
 *  cancels a later instruction."
 */

enum watchpoint_privilege /* WCR[2:1] */ {
    DBGWCR_PRIV_RESERVED = 0u,
    DBGWCR_PRIV_PRIVILEGED = 1u,
    DBGWCR_PRIV_USER = 2u,
    DBGWCR_PRIV_EITHER = 3u
};

enum watchpoint_access /* WCR[4:3] */ {
    DBGWCR_ACCESS_RESERVED = 0u,
    DBGWCR_ACCESS_LOAD = 1u,
    DBGWCR_ACCESS_STORE = 2u,
    DBGWCR_ACCESS_EITHER = 3u
};





/* These next few functions (read*Context()/write*Context()) read from TCB
 * context and not from the hardware registers.
 */
word_t readBcrContext(tcb_t *t, uint16_t index)
{
    do { if (!(index < 6)) { _assert_fail("index < seL4_NumExclusiveBreakpoints", "/home/b/test/sel4test/kernel/src/arch/arm/machine/debug.c", 56, __func__); } } while(0);
    return t->tcbArch.tcbContext.breakpointState.breakpoint[index].cr;
}

static word_t readBvrContext(tcb_t *t, uint16_t index)
{
    do { if (!(index < 6)) { _assert_fail("index < seL4_NumExclusiveBreakpoints", "/home/b/test/sel4test/kernel/src/arch/arm/machine/debug.c", 62, __func__); } } while(0);
    return t->tcbArch.tcbContext.breakpointState.breakpoint[index].vr;
}

static word_t readWcrContext(tcb_t *t, uint16_t index)
{
    do { if (!(index < 4)) { _assert_fail("index < seL4_NumExclusiveWatchpoints", "/home/b/test/sel4test/kernel/src/arch/arm/machine/debug.c", 68, __func__); } } while(0);
    return t->tcbArch.tcbContext.breakpointState.watchpoint[index].cr;
}

static word_t readWvrContext(tcb_t *t, uint16_t index)
{
    do { if (!(index < 4)) { _assert_fail("index < seL4_NumExclusiveWatchpoints", "/home/b/test/sel4test/kernel/src/arch/arm/machine/debug.c", 74, __func__); } } while(0);
    return t->tcbArch.tcbContext.breakpointState.watchpoint[index].vr;
}

void writeBcrContext(tcb_t *t, uint16_t index, word_t val)
{
    do { if (!(index < 6)) { _assert_fail("index < seL4_NumExclusiveBreakpoints", "/home/b/test/sel4test/kernel/src/arch/arm/machine/debug.c", 80, __func__); } } while(0);
    t->tcbArch.tcbContext.breakpointState.breakpoint[index].cr = val;
}

void writeBvrContext(tcb_t *t, uint16_t index, word_t val)
{
    do { if (!(index < 6)) { _assert_fail("index < seL4_NumExclusiveBreakpoints", "/home/b/test/sel4test/kernel/src/arch/arm/machine/debug.c", 86, __func__); } } while(0);
    t->tcbArch.tcbContext.breakpointState.breakpoint[index].vr = val;
}

static void writeWcrContext(tcb_t *t, uint16_t index, word_t val)
{
    do { if (!(index < 4)) { _assert_fail("index < seL4_NumExclusiveWatchpoints", "/home/b/test/sel4test/kernel/src/arch/arm/machine/debug.c", 92, __func__); } } while(0);
    t->tcbArch.tcbContext.breakpointState.watchpoint[index].cr = val;
}

static void writeWvrContext(tcb_t *t, uint16_t index, word_t val)
{
    do { if (!(index < 4)) { _assert_fail("index < seL4_NumExclusiveWatchpoints", "/home/b/test/sel4test/kernel/src/arch/arm/machine/debug.c", 98, __func__); } } while(0);
    t->tcbArch.tcbContext.breakpointState.watchpoint[index].vr = val;
}





/** For debugging: prints out the debug register pair values as returned by the
 * coprocessor.
 *
 * @param nBp Number of breakpoint reg pairs to print, starting at BP #0.
 * @param nBp Number of watchpoint reg pairs to print, starting at WP #0.
 */
__attribute__((unused)) static void dumpBpsAndWpsCp(int nBp, int nWp)
{
    int i;

    for (i = 0; i < nBp; i++) {
        do { printf("\033[0m" "\033[1m" "<<" "\033[0m" "\033[32m" "seL4(CPU %" "lu" ")" "\033[0m" "\033[1m" " [%s/%d T%p \"%s\" @%lx]: " "CP BP %d: Bcr %lx, Bvr %lx" ">>" "\033[0m" "\n", 0lu, __func__, 117, ksCurThread, ((debug_tcb_t *)(((cte_t *)((word_t)(ksCurThread)&~((1ul << (12)) - 1ul)))+(tcbCNodeEntries)))->tcbName, (word_t)getRestartPC(ksCurThread), i, readBcrCp(i), readBvrCp(i)); } while (0);
    }

    for (i = 0; i < nWp; i++) {
        do { printf("\033[0m" "\033[1m" "<<" "\033[0m" "\033[32m" "seL4(CPU %" "lu" ")" "\033[0m" "\033[1m" " [%s/%d T%p \"%s\" @%lx]: " "CP WP %d: Wcr %lx, Wvr %lx" ">>" "\033[0m" "\n", 0lu, __func__, 121, ksCurThread, ((debug_tcb_t *)(((cte_t *)((word_t)(ksCurThread)&~((1ul << (12)) - 1ul)))+(tcbCNodeEntries)))->tcbName, (word_t)getRestartPC(ksCurThread), i, readWcrCp(i), readWvrCp(i)); } while (0);
    }
}

/** Print a thread's saved debug context. For debugging. This differs from
 * dumpBpsAndWpsCp in that it reads from a thread's saved register context, and
 * not from the hardware coprocessor registers.
 *
 * @param at arch_tcb_t where the thread's reg context is stored.
 * @param nBp Number of BP regs to print, beginning at BP #0.
 * @param mWp Number of WP regs to print, beginning at WP #0.
 */
__attribute__((unused)) static void dumpBpsAndWpsContext(tcb_t *t, int nBp, int nWp)
{
    int i;

    for (i = 0; i < nBp; i++) {
        do { printf("\033[0m" "\033[1m" "<<" "\033[0m" "\033[32m" "seL4(CPU %" "lu" ")" "\033[0m" "\033[1m" " [%s/%d T%p \"%s\" @%lx]: " "Ctxt BP %d: Bcr %lx, Bvr %lx" ">>" "\033[0m" "\n", 0lu, __func__, 138, ksCurThread, ((debug_tcb_t *)(((cte_t *)((word_t)(ksCurThread)&~((1ul << (12)) - 1ul)))+(tcbCNodeEntries)))->tcbName, (word_t)getRestartPC(ksCurThread), i, readBcrContext(t, i), readBvrContext(t, i)); } while (0);
    }

    for (i = 0; i < nWp; i++) {
        do { printf("\033[0m" "\033[1m" "<<" "\033[0m" "\033[32m" "seL4(CPU %" "lu" ")" "\033[0m" "\033[1m" " [%s/%d T%p \"%s\" @%lx]: " "Ctxt WP %d: Wcr %lx, Wvr %lx" ">>" "\033[0m" "\n", 0lu, __func__, 142, ksCurThread, ((debug_tcb_t *)(((cte_t *)((word_t)(ksCurThread)&~((1ul << (12)) - 1ul)))+(tcbCNodeEntries)))->tcbName, (word_t)getRestartPC(ksCurThread), i, readWcrContext(t, i), readWvrContext(t, i)); } while (0);
    }
}

/* ARM allows watchpoint trigger on load, load-exclusive, and "swap" accesses.
 * store, store-exclusive and "swap" accesses. All accesses.
 *
 * The mask defines which bits are EXCLUDED from the comparison.
 * Always program the DBGDWVR with a WORD aligned address, and use the BAS to
 * state which bits form part of the match.
 *
 * It seems the BAS works as a bitmask of bytes to select in the range.
 *
 * To detect support for the 8-bit BAS field:
 *  * If the 8-bit BAS is unsupported, then BAS[7:4] is RAZ/WI.
 *
 * When using an 8-byte watchpoint that is not dword aligned, the result is
 * undefined. You should program it as the aligned base of the range, and select
 * only the relevant bytes then.
 *
 * You cannot do sparse byte selection: you either select a single byte in the
 * BAS or you select a contiguous range. ARM has deprecated sparse byte
 * selection.
 */

/** Convert an arch specific encoded watchpoint size back into a simple integer
 * representation.
 */
static word_t convertArchToSize(word_t archsize)
{
    switch (archsize) {
    case 0x1:
        return 1;
    case 0x3:
        return 2;
    case 0xFF:
        return 8;
    default:
        do { if (!(archsize == 0xF)) { _assert_fail("archsize == 0xF", "/home/b/test/sel4test/kernel/src/arch/arm/machine/debug.c", 180, __func__); } } while(0);
        return 4;
    }
}

/** Convert an access perms API value (seL4_BreakOnRead, etc) into the register
 * encoding that matches it.
 */
static word_t convertAccessToArch(word_t access)
{
    switch (access) {
    case seL4_BreakOnRead:
        return DBGWCR_ACCESS_LOAD;
    case seL4_BreakOnWrite:
        return DBGWCR_ACCESS_STORE;
    default:
        do { if (!(access == seL4_BreakOnReadWrite)) { _assert_fail("access == seL4_BreakOnReadWrite", "/home/b/test/sel4test/kernel/src/arch/arm/machine/debug.c", 196, __func__); } } while(0);
        return DBGWCR_ACCESS_EITHER;
    }
}

/** Convert an arch-specific register encoding back into an API access perms
 * value.
 */
static word_t convertArchToAccess(word_t archaccess)
{
    switch (archaccess) {
    case DBGWCR_ACCESS_LOAD:
        return seL4_BreakOnRead;
    case DBGWCR_ACCESS_STORE:
        return seL4_BreakOnWrite;
    default:
        do { if (!(archaccess == DBGWCR_ACCESS_EITHER)) { _assert_fail("archaccess == DBGWCR_ACCESS_EITHER", "/home/b/test/sel4test/kernel/src/arch/arm/machine/debug.c", 212, __func__); } } while(0);
        return seL4_BreakOnReadWrite;
    }
}

uint16_t getBpNumFromType(uint16_t bp_num, word_t type)
{
    do { if (!(type == seL4_InstructionBreakpoint || type == seL4_DataBreakpoint || type == seL4_SingleStep)) { _assert_fail("type == seL4_InstructionBreakpoint || type == seL4_DataBreakpoint || type == seL4_SingleStep", "/home/b/test/sel4test/kernel/src/arch/arm/machine/debug.c", 219, __func__); } } while(0)
                                      ;

    switch (type) {
    case seL4_InstructionBreakpoint:
    case seL4_SingleStep:
        return bp_num;
    default: /* seL4_DataBreakpoint: */
        do { if (!(type == seL4_DataBreakpoint)) { _assert_fail("type == seL4_DataBreakpoint", "/home/b/test/sel4test/kernel/src/arch/arm/machine/debug.c", 227, __func__); } } while(0);
        return bp_num + 6;
    }
}

/** Sets up the requested hardware breakpoint register.
 *
 * Acts as the backend for seL4_TCB_SetBreakpoint. Doesn't actually operate
 * on the hardware coprocessor, but just modifies the thread's debug register
 * context. The thread will pop off the updated register context when it is
 * popping its context the next time it runs.
 *
 * On ARM the hardware breakpoints are consumed by all operations, including
 * single-stepping, unlike x86, where single-stepping doesn't require the use
 * of an actual hardware breakpoint register (just uses the EFLAGS.TF bit).
 *
 * @param at arch_tcb_t that points to the register context of the thread we
 *           want to modify.
 * @param bp_num The hardware register we want to set up.
 * @params vaddr, type, size, rw: seL4 API values for seL4_TCB_SetBreakpoint.
 *         All documented in the seL4 API Manuals.
 */
void setBreakpoint(tcb_t *t,
                   uint16_t bp_num,
                   word_t vaddr, word_t type, word_t size, word_t rw)
{
    bp_num = convertBpNumToArch(bp_num);

    /* C3.3.4: "A debugger can use either byte address selection or address range
     *  masking, if it is implemented. However, it must not attempt to use both at
     * the same time"
     *
     * "v7 Debug and v7.1 Debug deprecate any use of the DBGBCR.MASK field."
     * ^ So prefer to use DBGBCR.BAS instead. When using masking, you must set
     * BAS to all 1s, and when using BAS you must set the MASK field to all 0s.
     *
     * To detect support for BPAddrMask:
     *  * When it's unsupported: DBGBCR.MASK is always RAZ/WI, and EITHER:
     *      * DBGIDR.DEVID_tmp is RAZ
     *      * OR DBGIDR.DEVID_tmp is RAO and DBGDEVID.{CIDMask, BPAddrMask} are RAZ.
     *  * OR:
     *      * DBGDEVID.BPAddrMask indicates whether addr masking is supported.
     *      * DBGBCR.MASK is UNK/SBZP.
     *
     * Setting BAS to 0b0000 makes the cpu break on every instruction.
     * Be aware that the processor checks the MASK before the BAS.
     * You must set BAS to 0b1111 for all context match comparisons.
     */
    if (type == seL4_InstructionBreakpoint) {
        dbg_bcr_t bcr;

        writeBvrContext(t, bp_num, vaddr);

        /* Preserve reserved bits. */
        bcr.words[0] = readBcrContext(t, bp_num);
        bcr = dbg_bcr_set_enabled(bcr, 1);
        bcr = dbg_bcr_set_lbn(bcr, 0);
        bcr = dbg_bcr_set_pmc(bcr, DBGBCR_PRIV_USER);
        bcr = dbg_bcr_set_hmc(bcr, 0);
        bcr = dbg_bcr_set_ssc(bcr, 0);
        bcr = dbg_bcr_set_bas(bcr, convertSizeToArch(4));
        bcr = Arch_setupBcr(bcr, true);
        writeBcrContext(t, bp_num, bcr.words[0]);
    } else {
        dbg_wcr_t wcr;

        writeWvrContext(t, bp_num, vaddr);

        /* Preserve reserved bits */
        wcr.words[0] = readWcrContext(t, bp_num);
        wcr = dbg_wcr_set_enabled(wcr, 1);
        wcr = dbg_wcr_set_pac(wcr, DBGWCR_PRIV_USER);
        wcr = dbg_wcr_set_bas(wcr, convertSizeToArch(size));
        wcr = dbg_wcr_set_lsc(wcr, convertAccessToArch(rw));
        wcr = dbg_wcr_set_watchpointType(wcr, 0);
        wcr = dbg_wcr_set_lbn(wcr, 0);
        wcr = dbg_wcr_set_addressMask(wcr, 0);
        wcr = dbg_wcr_set_hmc(wcr, 0);
        wcr = dbg_wcr_set_ssc(wcr, 0);
        writeWcrContext(t, bp_num, wcr.words[0]);
    }
}

/** Retrieves the current configuration of a hardware breakpoint for a given
 * thread.
 *
 * Doesn't modify the configuration of that thread's breakpoints.
 *
 * @param at arch_tcb_t that holds the register context for the thread you wish
 *           to query.
 * @param bp_num Hardware breakpoint ID.
 * @return A struct describing the current configuration of the requested
 *         breakpoint.
 */
getBreakpoint_t getBreakpoint(tcb_t *t, uint16_t bp_num)
{
    getBreakpoint_t ret;

    ret.type = getTypeFromBpNum(bp_num);
    bp_num = convertBpNumToArch(bp_num);

    if (ret.type == seL4_InstructionBreakpoint) {
        dbg_bcr_t bcr;

        bcr.words[0] = readBcrContext(t, bp_num);
        if (Arch_breakpointIsSingleStepping(t, bp_num)) {
            ret.type = seL4_SingleStep;
        };
        ret.size = 0;
        ret.rw = seL4_BreakOnRead;
        ret.vaddr = readBvrContext(t, bp_num);
        ret.is_enabled = dbg_bcr_get_enabled(bcr);
    } else {
        dbg_wcr_t wcr;

        wcr.words[0] = readWcrContext(t, bp_num);
        ret.size = convertArchToSize(dbg_wcr_get_bas(wcr));
        ret.rw = convertArchToAccess(dbg_wcr_get_lsc(wcr));
        ret.vaddr = readWvrContext(t, bp_num);
        ret.is_enabled = dbg_wcr_get_enabled(wcr);
    }
    return ret;
}

/** Disables and clears the configuration of a hardware breakpoint.
 *
 * @param at arch_tcb_t holding the reg context for the target thread.
 * @param bp_num The hardware breakpoint you want to disable+clear.
 */
void unsetBreakpoint(tcb_t *t, uint16_t bp_num)
{
    word_t type;

    type = getTypeFromBpNum(bp_num);
    bp_num = convertBpNumToArch(bp_num);

    if (type == seL4_InstructionBreakpoint) {
        dbg_bcr_t bcr;

        bcr.words[0] = readBcrContext(t, bp_num);
        bcr = dbg_bcr_set_enabled(bcr, 0);
        writeBcrContext(t, bp_num, bcr.words[0]);
        writeBvrContext(t, bp_num, 0);
    } else {
        dbg_wcr_t wcr;

        wcr.words[0] = readWcrContext(t, bp_num);
        wcr = dbg_wcr_set_enabled(wcr, 0);
        writeWcrContext(t, bp_num, wcr.words[0]);
        writeWvrContext(t, bp_num, 0);
    }
}

/** Load an initial, all-disabled setup state for the registers.
 */
__attribute__((__section__(".boot.text"))) void disableAllBpsAndWps(void)
{
    int i;

    for (i = 0; i < 6; i++) {
        writeBvrCp(i, 0);
        writeBcrCp(i, readBcrCp(i) & ~((1ul << (0))));
    }
    for (i = 0; i < 4; i++) {
        writeWvrCp(i, 0);
        writeWcrCp(i, readWcrCp(i) & ~((1ul << (0))));
    }

    isb();
}

/** Determines which breakpoint or watchpoint register caused the debug
 * exception to be triggered.
 *
 * Checks to see which hardware breakpoint was triggered, and saves
 * the ID of that breakpoint.
 * There is no short way to do this on ARM. On x86 there is a status
 * register that tells you which watchpoint has been triggered. On ARM
 * there is no such register, so you have to manually check each to see which
 * one was triggered.
 *
 * The arguments also work a bit differently from x86 as well. On x86 the
 * 2 arguments are dummy values, while on ARM, they contain useful information.
 *
 * @param vaddr The virtual address stored in the IFSR/DFSR register, which
 *              is either the watchpoint address or breakpoint address.
 * @param reason The presumed reason for the exception, which is based on
 *               whether it was a prefetch or data abort.
 * @return Struct with a member "bp_num", which is a positive integer if we
 *         successfully detected which debug register triggered the exception.
 *         "Bp_num" will be negative otherwise.
 */
int getAndResetActiveBreakpoint(word_t vaddr, word_t reason)
{
    word_t align_mask;
    int i, ret = -1;

    if (reason == seL4_InstructionBreakpoint) {
        for (i = 0; i < 6; i++) {
            dbg_bcr_t bcr;
            word_t bvr = readBvrCp(i);

            bcr.words[0] = readBcrCp(i);
            /* The actual trigger address may be an unaligned sub-byte of the
             * range, which means it's not guaranteed to match the aligned value
             * that was programmed into the address register.
             */
            align_mask = convertArchToSize(dbg_bcr_get_bas(bcr));
            align_mask = ~(align_mask - 1);

            if (bvr != (vaddr & align_mask) || !dbg_bcr_get_enabled(bcr)) {
                continue;
            }

            ret = i;
            return ret;
        }
    }

    if (reason == seL4_DataBreakpoint) {
        for (i = 0; i < 4; i++) {
            dbg_wcr_t wcr;
            word_t wvr = readWvrCp(i);

            wcr.words[0] = readWcrCp(i);
            align_mask = convertArchToSize(dbg_wcr_get_bas(wcr));
            align_mask = ~(align_mask - 1);

            if (wvr != (vaddr & align_mask) || !dbg_wcr_get_enabled(wcr)) {
                continue;
            }

            ret = i;
            return ret;
        }
    }

    return ret;
}





/** Mirrors Arch_initFpuContext.
 *
 * Zeroes out the BVR thread context and preloads reserved bit values from the
 * control regs into the thread context so we can operate solely on the values
 * cached in RAM in API calls, rather than retrieving the values from the
 * coprocessor.
 */
void Arch_initBreakpointContext(user_context_t *uc)
{
    uc->breakpointState = armKSNullBreakpointState;
}

void loadAllDisabledBreakpointState(void)
{
    int i;

    /* We basically just want to read-modify-write each reg to ensure its
     * "ENABLE" bit is clear. We did preload the register context with the
     * reserved values from the control registers, so we can read our
     * initial values from either the coprocessor or the thread's register
     * context.
     *
     * Both are perfectly fine, and the only discriminant factor is performance.
     * I suspect that reading from RAM is faster than reading from the
     * coprocessor, but I can't be sure.
     */
    for (i = 0; i < 6; i++) {
        writeBcrCp(i, readBcrCp(i) & ~((1ul << (0))));
    }
    for (i = 0; i < 4; i++) {
        writeWcrCp(i, readWcrCp(i) & ~((1ul << (0))));
    }
}

/* We only need to save the breakpoint state in the hypervisor
 * build, and only for threads that have an associated VCPU.
 *
 * When the normal kernel is running with the debug API, all
 * changes to the debug regs are done through the debug API.
 * In the hypervisor build, the guest VM has full access to the
 * debug regs in PL1, so we need to save its values on vmexit.
 *
 * When saving the debug regs we will always save all of them.
 * When restoring, we will restore only those that have been used
 * for native threads; and we will restore all of them
 * unconditionally for VCPUs (because we don't know which of
 * them have been changed by the guest).
 *
 * To ensure that all the debug regs are restored unconditionally,
 * we just set the "used_breakpoints_bf" bitfield to all 1s in
 * associateVcpu.
 */
void saveAllBreakpointState(tcb_t *t)
{
    int i;

    do { if (!(t != ((void *)0))) { _assert_fail("t != NULL", "/home/b/test/sel4test/kernel/src/arch/arm/machine/debug.c", 527, __func__); } } while(0);

    for (i = 0; i < 6; i++) {
        writeBvrContext(t, i, readBvrCp(i));
        writeBcrContext(t, i, readBcrCp(i));
    }

    for (i = 0; i < 4; i++) {
        writeWvrContext(t, i, readWvrCp(i));
        writeWcrContext(t, i, readWcrCp(i));
    }
}
# 558 "/home/b/test/sel4test/kernel/src/arch/arm/machine/debug.c"
static void loadBreakpointState(tcb_t *t)
{
    int i;

    do { if (!(t != ((void *)0))) { _assert_fail("t != NULL", "/home/b/test/sel4test/kernel/src/arch/arm/machine/debug.c", 562, __func__); } } while(0);

    for (i = 0; i < 6; i++) {
        if (t->tcbArch.tcbContext.breakpointState.used_breakpoints_bf & (1ul << (i))) {
            writeBvrCp(i, readBvrContext(t, i));
            writeBcrCp(i, readBcrContext(t, i));
        } else {
            /* If the thread isn't using the BP, then just load
             * a default "disabled" state.
             */
            writeBcrCp(i, readBcrCp(i) & ~((1ul << (0))));
        }
    }

    for (i = 0; i < 4; i++) {
        if (t->tcbArch.tcbContext.breakpointState.used_breakpoints_bf &
            (1ul << (i + 6))) {
            writeWvrCp(i, readWvrContext(t, i));
            writeWcrCp(i, readWcrContext(t, i));
        } else {
            writeWcrCp(i, readWcrCp(i) & ~((1ul << (0))));
        }
    }
}

/** Pops debug register context for a thread into the CPU.
 *
 * Mirrors the idea of restore_user_context.
 */
void restore_user_debug_context(tcb_t *target_thread)
{
    do { if (!(target_thread != ((void *)0))) { _assert_fail("target_thread != NULL", "/home/b/test/sel4test/kernel/src/arch/arm/machine/debug.c", 593, __func__); } } while(0);

    if (target_thread->tcbArch.tcbContext.breakpointState.used_breakpoints_bf == 0) {
        loadAllDisabledBreakpointState();
    } else {
        loadBreakpointState(target_thread);
    }

    /* ARMv7 manual, sec C3.7:
     * "Usually, an exception return sequence is a context change operation as
     * well as a context synchronization operation, in which case the context
     * change operation is guaranteed to take effect on the debug logic by the
     * end of that exception return sequence."
     *
     * So we don't need to execute ISB here because we're about to RFE.
     */


    aarch64_restore_user_debug_context(target_thread);

}
# 1 "/home/b/test/sel4test/kernel/src/arch/arm/machine/errata.c"
/*
 * Copyright 2014, General Dynamics C4 Systems
 *
 * SPDX-License-Identifier: GPL-2.0-only
 */







/* Prototyped here as this is referenced from assembly */
void arm_errata(void);
# 39 "/home/b/test/sel4test/kernel/src/arch/arm/machine/errata.c"
__attribute__((__section__(".boot.text"))) void __attribute__((externally_visible)) arm_errata(void)
{



}
# 1 "/home/b/test/sel4test/kernel/src/arch/arm/machine/gic_v3.c"
/*
 * Copyright 2019, DornerWorks
 * Copyright 2019, Data61, CSIRO (ABN 41 687 119 230)
 *
 * SPDX-License-Identifier: GPL-2.0-only
 */
# 15 "/home/b/test/sel4test/kernel/src/arch/arm/machine/gic_v3.c"
/* One GICR region and one GICR_SGI region */

/* Assume 8 cores */
# 37 "/home/b/test/sel4test/kernel/src/arch/arm/machine/gic_v3.c"
volatile struct gic_dist_map *const gic_dist = (volatile struct gic_dist_map *)((0xffffffffffe00000ul + 0x1000));
volatile void *const gicr_base = (volatile uint8_t *)((0xffffffffffe00000ul + 0x11000));

word_t active_irq[1];
volatile struct gic_rdist_map *gic_rdist_map[1] = { 0 };
volatile struct gic_rdist_sgi_ppi_map *gic_rdist_sgi_ppi_map[1] = { 0 };
# 58 "/home/b/test/sel4test/kernel/src/arch/arm/machine/gic_v3.c"
static word_t mpidr_map[1];

static inline word_t get_mpidr(word_t core_id)
{
    return mpidr_map[core_id];
}

static inline word_t get_current_mpidr(void)
{
    word_t core_id = 0lu;
    return get_mpidr(core_id);
}

static inline uint64_t mpidr_to_gic_affinity(void)
{
    word_t mpidr = get_current_mpidr();
    uint64_t affinity = 0;
    affinity = (uint64_t)((mpidr >> 32) & 0xff) << 32 | ((mpidr >> 16) & 0xff) << 16 |
               ((mpidr >> 8) & 0xff) << 8 | (mpidr & 0xff);
    return affinity;
}

static inline uint64_t sgir_word_from_args(word_t irq, word_t target)
{
    uint64_t t = target; /* make sure shifts below are on 64 bit */
    return (uint64_t) irq << (24)
           | (1llu << (t & 0xf)) // AFF0 base
           | ((t >> 4) & 0x0f) << (44) // AFF0 Range select
           | ((t >> 8) & 0xff) << (16) // AFF1
           | ((t >> 16) & 0xff) << (32) // AFF2
           | ((t >> 24) & 0xff) << (32); // AFF3
}

/* Wait for completion of a distributor change */
/** DONT_TRANSLATE */
static uint32_t gicv3_do_wait_for_rwp(volatile uint32_t *ctlr_addr)
{
    uint32_t val;
    bool_t waiting = true;
    uint32_t ret = 0;

    uint64_t gpt_cnt_tval = 0;
    uint32_t deadline_ms = 2;
    uint64_t gpt_cnt_ciel;

    /* Check the value before reading the generic timer */
    val = *ctlr_addr;
    if (!(val & (1ul << (31)))) {
        return 0;
    }
    __asm__ volatile("mrs %x0," "cntvct_el0" : "=r"(gpt_cnt_tval));
    gpt_cnt_ciel = gpt_cnt_tval + (deadline_ms * (50000000llu / 1000llu));

    while (waiting) {
        __asm__ volatile("mrs %x0," "cntvct_el0" : "=r"(gpt_cnt_tval));
        val = *ctlr_addr;

        if (gpt_cnt_tval >= gpt_cnt_ciel) {
            printf("GICV3 RWP Timeout after %u ms\n", deadline_ms);
            ret = 1;
            waiting = false;

        } else if (!(val & (1ul << (31)))) {
            ret = 0;
            waiting = false;
        }
    }
    return ret;
}

static void gicv3_dist_wait_for_rwp(void)
{
    gicv3_do_wait_for_rwp(&gic_dist->ctlr);
}

static void gicv3_redist_wait_for_rwp(void)
{
    gicv3_do_wait_for_rwp(&gic_rdist_map[0lu]->ctlr);
}

static void gicv3_enable_sre(void)
{
    word_t val = 0;

    /* ICC_SRE_EL1 */
    __asm__ volatile("mrs %x0," "S3_0_C12_C12_5" : "=r"(val));
    val |= (1ul << (0));

    do { word_t _v = val; __asm__ volatile("msr " "S3_0_C12_C12_5" ",%x0" :: "r" (_v)); }while(0);
    isb();
}


__attribute__((__section__(".boot.text"))) static void dist_init(void)
{
    word_t i;
    uint32_t type;
    unsigned int nr_lines;
    uint64_t affinity;
    uint32_t priority;

    /* Disable GIC Distributor */
    gic_dist->ctlr = 0;
    gicv3_dist_wait_for_rwp();

    type = gic_dist->typer;

    nr_lines = 32 * ((type & 0x01f) + 1);

    /* Assume level-triggered */
    for (i = 32u; i < nr_lines; i += 16) {
        gic_dist->icfgrn[(i / 16)] = 0;
    }

    /* Default priority for global interrupts */
    priority = (0xa0 << 24 | 0xa0 << 16 | 0xa0 << 8 |
                0xa0);
    for (i = 32u; i < nr_lines; i += 4) {
        gic_dist->ipriorityrn[(i / 4)] = priority;
    }
    /* Disable and clear all global interrupts */
    for (i = 32u; i < nr_lines; i += 32) {
        gic_dist->icenablern[(i / 32)] = 0xffffffff;
        gic_dist->icpendrn[(i / 32)] = 0xffffffff;
    }

    /* Turn on the distributor */
    gic_dist->ctlr = (1ul << (5)) | (1ul << (1)) | (1ul << (0));
    gicv3_dist_wait_for_rwp();

    /* Route all global IRQs to this CPU */
    affinity = mpidr_to_gic_affinity();
    for (i = 32u; i < nr_lines; i++) {
        gic_dist->iroutern[i - 32u] = affinity;
    }
}

__attribute__((__section__(".boot.text"))) static void gicr_locate_interface(void)
{
    word_t offset;
    int core_id = 0lu;
    word_t mpidr = get_current_mpidr();
    uint32_t val;

    /*
     * Iterate through all redistributor interfaces looking for one that matches
     * our mpidr.
     */
    for (offset = 0; offset < (0x100000); offset += (0x20000)) {

        uint64_t typer = ((struct gic_rdist_map *)((word_t)gicr_base + offset))->typer;
        if ((typer >> 32) == ((((mpidr >> 32) & 0xff) << 24) |
                              (((mpidr >> 16) & 0xff) << 16) |
                              (((mpidr >> 8) & 0xff) << 8) |
                              (mpidr & 0xff))) {

            word_t gicr = (word_t)gicr_base + offset;
            if (gic_rdist_map[core_id] != ((void *)0) || gic_rdist_sgi_ppi_map[core_id] != ((void *)0)) {
                printf("GICv3: %s[%d] %p is not null\n",
                       gic_rdist_map[core_id] == ((void *)0) ? "gic_rdist_map" : "gic_rdist_sgi_ppi_map",
                       core_id,
                       gic_rdist_map[core_id] == ((void *)0) ? (void *)gic_rdist_map[core_id] : (void *)gic_rdist_sgi_ppi_map[core_id]);
                halt();
            }
            gic_rdist_map[core_id] = (void *)gicr;
            gic_rdist_sgi_ppi_map[core_id] = (void *)(gicr + 0x00010000);

            /*
             * GICR_WAKER should be Read-all-zeros in Non-secure world
             * and we expect redistributors to be already awoken by an earlier loader.
             * However if we get a value back then something is probably wrong.
             */
            val = gic_rdist_map[core_id]->waker;
            if (val & (1ul << (2))) {
                printf("GICv3: GICR_WAKER returned non-zero %x\n", val);
                halt();
            }

            break;
        }
    }
    if (offset >= (0x100000)) {
        printf("GICv3: GICR base for CPU %d %d %d %d (Logic ID %d) not found\n",
               (int)((mpidr >> 32) & 0xff), (int)((mpidr >> 16) & 0xff),
               (int)((mpidr >> 8) & 0xff), (int)(mpidr & 0xff), core_id);
        halt();
    }


}

__attribute__((__section__(".boot.text"))) static void gicr_init(void)
{
    int i;
    uint32_t priority;

    /* Find redistributor for this core. */
    gicr_locate_interface();

    /* Deactivate SGIs/PPIs */
    gic_rdist_sgi_ppi_map[0lu]->icactiver0 = ~0;

    /* Set priority on PPI and SGI interrupts */
    priority = (0xa0 << 24 | 0xa0 << 16 | 0xa0 << 8 |
                0xa0);
    for (i = 0; i < 32u; i += 4) {
        gic_rdist_sgi_ppi_map[0lu]->ipriorityrn[i / 4] = priority;
    }

    /*
     * Disable all PPI interrupts, ensure all SGI interrupts are
     * enabled.
     */
    gic_rdist_sgi_ppi_map[0lu]->icenabler0 = 0xffff0000;
    gic_rdist_sgi_ppi_map[0lu]->isenabler0 = 0x0000ffff;

    /* Set ICFGR1 for PPIs as level-triggered */
    gic_rdist_sgi_ppi_map[0lu]->icfgr1 = 0x0;

    gicv3_redist_wait_for_rwp();
}

__attribute__((__section__(".boot.text"))) static void cpu_iface_init(void)
{
    word_t icc_ctlr = 0;

    /* Enable system registers */
    gicv3_enable_sre();

    /* No priority grouping: ICC_BPR1_EL1 */
    do { word_t _v = 0; __asm__ volatile("msr " "S3_0_C12_C12_3" ",%x0" :: "r" (_v)); }while(0);

    /* Set priority mask register: ICC_PMR_EL1 */
    do { word_t _v = 0xff; __asm__ volatile("msr " "S3_0_C4_C6_0" ",%x0" :: "r" (_v)); }while(0);

    /* EOI drops priority of the interrupt, deactivation happens separately: ICC_CTLR_EL1 */
    __asm__ volatile("mrs %x0," "S3_0_C12_C12_4" : "=r"(icc_ctlr));
    icc_ctlr |= (1ul << (1));
    do { word_t _v = icc_ctlr; __asm__ volatile("msr " "S3_0_C12_C12_4" ",%x0" :: "r" (_v)); }while(0);

    /* Enable Group1 interrupts: ICC_IGRPEN1_EL1 */
    do { word_t _v = 1; __asm__ volatile("msr " "S3_0_C12_C12_7" ",%x0" :: "r" (_v)); }while(0);

    /* Sync at once at the end of cpu interface configuration */
    isb();
}

void setIRQTrigger(irq_t irq, bool_t trigger)
{

    /* GICv3 has read-only GICR_ICFG0 for SGI with
     * default value 0xaaaaaaaa, and read-write GICR_ICFG1
     * for PPI with default 0x00000000.*/
    word_t hw_irq = (irq);
    word_t core = 0;
    if (((hw_irq) < 16u)) {
        return;
    }
    int word = hw_irq >> 4;
    int bit = ((hw_irq & 0xf) * 2);
    uint32_t icfgr = 0;
    if (((hw_irq) < 32u)) {
        icfgr = gic_rdist_sgi_ppi_map[core]->icfgr1;
    } else {
        icfgr = gic_dist->icfgrn[word];
    }

    if (trigger) {
        icfgr |= (2 << bit);
    } else {
        icfgr &= ~(3 << bit);
    }

    if (((hw_irq) < 32u)) {
        gic_rdist_sgi_ppi_map[core]->icfgr1 = icfgr;
    } else {
        /* Update GICD_ICFGR<n>. Note that the interrupt should
         * be disabled before changing the field, and this function
         * assumes the caller has disabled the interrupt. */
        gic_dist->icfgrn[word] = icfgr;
    }

    return;
}

__attribute__((__section__(".boot.text"))) void initIRQController(void)
{
    dist_init();
}

__attribute__((__section__(".boot.text"))) void cpu_initLocalIRQController(void)
{
    word_t mpidr = 0;
    __asm__ volatile("mrs %x0," "mpidr_el1" : "=r"(mpidr));

    mpidr_map[0lu] = mpidr;
    active_irq[0lu] = 1023u;

    gicr_init();
    cpu_iface_init();
}

bool_t plat_SGITargetValid(word_t target)
{
    return target < 16;
}

void plat_sendSGI(word_t irq, word_t target)
{
    uint64_t sgi1r_base = sgir_word_from_args(irq, target);
    do { word_t _v = sgi1r_base; __asm__ volatile("msr " "S3_0_C12_C11_5" ",%x0" :: "r" (_v)); }while(0);
    isb();
}
# 1 "/home/b/test/sel4test/kernel/src/arch/arm/machine/hardware.c"
/*
 * Copyright 2014, General Dynamics C4 Systems
 *
 * SPDX-License-Identifier: GPL-2.0-only
 */






word_t __attribute__((__pure__)) getRestartPC(tcb_t *thread)
{
    return getRegister(thread, FaultIP);
}

void setNextPC(tcb_t *thread, word_t v)
{
    setRegister(thread, ELR_EL1, v);
}

__attribute__((__section__(".boot.text"))) void map_kernel_devices(void)
{
    /* If there are no kernel device frames at all, then kernel_device_frames is
     * NULL. Thus we can't use ARRAY_SIZE(kernel_device_frames) here directly,
     * but have to use NUM_KERNEL_DEVICE_FRAMES that is defined accordingly.
     */
    for (int i = 0; i < (sizeof(kernel_device_frames) / sizeof((kernel_device_frames)[0])); i++) {
        const kernel_frame_t *frame = &kernel_device_frames[i];
        /* all frames are supposed to describe device memory, so they should
         * never be marked as executable.
         */
        do { if (!(frame->armExecuteNever)) { _assert_fail("frame->armExecuteNever", "/home/b/test/sel4test/kernel/src/arch/arm/machine/hardware.c", 33, __func__); } } while(0);
        map_kernel_frame(frame->paddr, frame->pptr, VMKernelOnly,
                         vm_attributes_new(frame->armExecuteNever, false,
                                           false));
        if (!frame->userAvailable) {
            reserve_region((p_region_t) {
                .start = frame->paddr,
                .end = frame->paddr + (1ul << (12))
            });
        }
    }
}
# 1 "/home/b/test/sel4test/kernel/src/arch/arm/machine/io.c"
/*
 * Copyright 2021, Axel Heider <axelheider@gmx.de>
 *
 * SPDX-License-Identifier: GPL-2.0-or-later
 */



# 1 "/home/b/test/sel4test/kernel/include/drivers/uart.h" 1
/*
 * Copyright 2021, Axel Heider <axelheider@gmx.de>
 *
 * SPDX-License-Identifier: GPL-2.0-or-later
 */

       



void uart_drv_putchar(unsigned char c);

static inline void uart_console_putchar(
    unsigned char c)
{
    /* UART console requires printing a '\r' (CR) before any '\n' (LF) */
    if (c == '\n') {
        uart_drv_putchar('\r');
    }
    uart_drv_putchar(c);
}




unsigned char uart_drv_getchar(void);
# 10 "/home/b/test/sel4test/kernel/src/arch/arm/machine/io.c" 2


void kernel_putDebugChar(unsigned char c)
{
    uart_console_putchar(c);
}



unsigned char kernel_getDebugChar(void)
{
    return uart_drv_getchar();
}
# 1 "/home/b/test/sel4test/kernel/src/arch/arm/machine/l2c_nop.c"
/*
 * Copyright 2020, Data61, CSIRO (ABN 41 687 119 230)
 *
 * SPDX-License-Identifier: GPL-2.0-only
 */






void initL2Cache(void) {}
void plat_cleanL2Range(paddr_t start, paddr_t end) {}
void plat_invalidateL2Range(paddr_t start, paddr_t end) {}
void plat_cleanInvalidateL2Range(paddr_t start, paddr_t end) {}
void plat_cleanInvalidateL2Cache(void) {}
# 1 "/home/b/test/sel4test/kernel/src/arch/arm/object/interrupt.c"
/*
 * Copyright 2014, General Dynamics C4 Systems
 *
 * SPDX-License-Identifier: GPL-2.0-only
 */







static exception_t Arch_invokeIRQControl(irq_t irq, cte_t *handlerSlot, cte_t *controlSlot, bool_t trigger)
{

    setIRQTrigger(irq, trigger);

    return invokeIRQControl(irq, handlerSlot, controlSlot);
}




static exception_t Arch_invokeIssueSGISignal(word_t irq, word_t target, cte_t *sgiSlot, cte_t *controlSlot)
{
    cteInsert(cap_sgi_signal_cap_new(target, irq), controlSlot, sgiSlot);
    return EXCEPTION_NONE;
}



exception_t Arch_decodeIRQControlInvocation(word_t invLabel, word_t length,
                                            cte_t *srcSlot, word_t *buffer)
{
    if (invLabel == ARMIRQIssueIRQHandlerTrigger) {
        if (length < 4 || current_extra_caps.excaprefs[0] == ((void *)0)) {
            current_syscall_error.type = seL4_TruncatedMessage;
            return EXCEPTION_SYSCALL_ERROR;
        }

        if (!wrap_config_set(1)) {
            do { printf("\033[0m" "\033[1m" "<<" "\033[0m" "\033[32m" "seL4(CPU %" "lu" ")" "\033[0m" "\033[1m" " [%s/%d T%p \"%s\" @%lx]: " "This platform does not support setting the IRQ trigger" ">>" "\033[0m" "\n", 0lu, __func__, 42, ksCurThread, ((debug_tcb_t *)(((cte_t *)((word_t)(ksCurThread)&~((1ul << (12)) - 1ul)))+(tcbCNodeEntries)))->tcbName, (word_t)getRestartPC(ksCurThread)); } while (0);
            current_syscall_error.type = seL4_IllegalOperation;
            return EXCEPTION_SYSCALL_ERROR;
        }

        word_t irq_w = getSyscallArg(0, buffer);
        irq_t irq = (irq_t) (irq_w);
        bool_t trigger = !!getSyscallArg(1, buffer);
        word_t index = getSyscallArg(2, buffer);
        word_t depth = getSyscallArg(3, buffer);

        cap_t cnodeCap = current_extra_caps.excaprefs[0]->cap;

        exception_t status = Arch_checkIRQ(irq_w);
        if (status != EXCEPTION_NONE) {
            return status;
        }
# 67 "/home/b/test/sel4test/kernel/src/arch/arm/object/interrupt.c"
        if (isIRQActive(irq)) {
            current_syscall_error.type = seL4_RevokeFirst;
            do { printf("\033[0m" "\033[1m" "<<" "\033[0m" "\033[32m" "seL4(CPU %" "lu" ")" "\033[0m" "\033[1m" " [%s/%d T%p \"%s\" @%lx]: " "Rejecting request for IRQ %u. Already active." ">>" "\033[0m" "\n", 0lu, __func__, 69, ksCurThread, ((debug_tcb_t *)(((cte_t *)((word_t)(ksCurThread)&~((1ul << (12)) - 1ul)))+(tcbCNodeEntries)))->tcbName, (word_t)getRestartPC(ksCurThread), (int)(irq)); } while (0);
            return EXCEPTION_SYSCALL_ERROR;
        }

        lookupSlot_ret_t lu_ret = lookupTargetSlot(cnodeCap, index, depth);
        if (lu_ret.status != EXCEPTION_NONE) {
            do { printf("\033[0m" "\033[1m" "<<" "\033[0m" "\033[32m" "seL4(CPU %" "lu" ")" "\033[0m" "\033[1m" " [%s/%d T%p \"%s\" @%lx]: " "Target slot for new IRQ Handler cap invalid: cap %lu, IRQ %u." ">>" "\033[0m" "\n", 0lu, __func__, 75, ksCurThread, ((debug_tcb_t *)(((cte_t *)((word_t)(ksCurThread)&~((1ul << (12)) - 1ul)))+(tcbCNodeEntries)))->tcbName, (word_t)getRestartPC(ksCurThread), getExtraCPtr(buffer, 0), (int)(irq)); } while (0)
                                                                     ;
            return lu_ret.status;
        }

        cte_t *destSlot = lu_ret.slot;

        status = ensureEmptySlot(destSlot);
        if (status != EXCEPTION_NONE) {
            do { printf("\033[0m" "\033[1m" "<<" "\033[0m" "\033[32m" "seL4(CPU %" "lu" ")" "\033[0m" "\033[1m" " [%s/%d T%p \"%s\" @%lx]: " "Target slot for new IRQ Handler cap not empty: cap %lu, IRQ %u." ">>" "\033[0m" "\n", 0lu, __func__, 84, ksCurThread, ((debug_tcb_t *)(((cte_t *)((word_t)(ksCurThread)&~((1ul << (12)) - 1ul)))+(tcbCNodeEntries)))->tcbName, (word_t)getRestartPC(ksCurThread), getExtraCPtr(buffer, 0), (int)(irq)); } while (0)
                                                                     ;
            return status;
        }

        setThreadState(ksCurThread, ThreadState_Restart);
        return Arch_invokeIRQControl(irq, destSlot, srcSlot, trigger);

    } else if (invLabel == ARMIRQIssueSGISignal) {
        if (length < 4 || current_extra_caps.excaprefs[0] == ((void *)0)) {
            do { printf("\033[0m" "\033[1m" "<<" "\033[0m" "\033[32m" "seL4(CPU %" "lu" ")" "\033[0m" "\033[1m" " [%s/%d T%p \"%s\" @%lx]: " "IRQControl: IssueSGISignal: Truncated message." ">>" "\033[0m" "\n", 0lu, __func__, 94, ksCurThread, ((debug_tcb_t *)(((cte_t *)((word_t)(ksCurThread)&~((1ul << (12)) - 1ul)))+(tcbCNodeEntries)))->tcbName, (word_t)getRestartPC(ksCurThread)); } while (0);
            current_syscall_error.type = seL4_TruncatedMessage;
            return EXCEPTION_SYSCALL_ERROR;
        }
        word_t irq = getSyscallArg(0, buffer);
        word_t target = getSyscallArg(1, buffer);
        word_t index = getSyscallArg(2, buffer);
        word_t depth = getSyscallArg(3, buffer);

        cap_t cnodeCap = current_extra_caps.excaprefs[0]->cap;

        if (irq >= 16u) {
            current_syscall_error.type = seL4_RangeError;
            current_syscall_error.rangeErrorMin = 0;
            current_syscall_error.rangeErrorMax = 16u - 1;
            do { printf("\033[0m" "\033[1m" "<<" "\033[0m" "\033[32m" "seL4(CPU %" "lu" ")" "\033[0m" "\033[1m" " [%s/%d T%p \"%s\" @%lx]: " "IRQControl: IssueSGISignal: Invalid SGI IRQ 0x%lx." ">>" "\033[0m" "\n", 0lu, __func__, 109, ksCurThread, ((debug_tcb_t *)(((cte_t *)((word_t)(ksCurThread)&~((1ul << (12)) - 1ul)))+(tcbCNodeEntries)))->tcbName, (word_t)getRestartPC(ksCurThread), irq); } while (0);
            return EXCEPTION_SYSCALL_ERROR;
        }

        if (!plat_SGITargetValid(target)) {
            current_syscall_error.type = seL4_InvalidArgument;
            current_syscall_error.invalidArgumentNumber = 1;
            do { printf("\033[0m" "\033[1m" "<<" "\033[0m" "\033[32m" "seL4(CPU %" "lu" ")" "\033[0m" "\033[1m" " [%s/%d T%p \"%s\" @%lx]: " "IRQControl: IssueSGISignal: Invalid SGI Target 0x%lx." ">>" "\033[0m" "\n", 0lu, __func__, 116, ksCurThread, ((debug_tcb_t *)(((cte_t *)((word_t)(ksCurThread)&~((1ul << (12)) - 1ul)))+(tcbCNodeEntries)))->tcbName, (word_t)getRestartPC(ksCurThread), target); } while (0);
            return EXCEPTION_SYSCALL_ERROR;
        }

        lookupSlot_ret_t lu_ret = lookupTargetSlot(cnodeCap, index, depth);
        if (lu_ret.status != EXCEPTION_NONE) {
            do { printf("\033[0m" "\033[1m" "<<" "\033[0m" "\033[32m" "seL4(CPU %" "lu" ")" "\033[0m" "\033[1m" " [%s/%d T%p \"%s\" @%lx]: " "IRQControl: IssueSGISignal: Target slot for new ARM_SGI_Signal cap invalid: cap %lu." ">>" "\033[0m" "\n", 0lu, __func__, 122, ksCurThread, ((debug_tcb_t *)(((cte_t *)((word_t)(ksCurThread)&~((1ul << (12)) - 1ul)))+(tcbCNodeEntries)))->tcbName, (word_t)getRestartPC(ksCurThread), getExtraCPtr(buffer, 0)); } while (0)
                                              ;
            return lu_ret.status;
        }
        cte_t *destSlot = lu_ret.slot;

        exception_t status = ensureEmptySlot(destSlot);
        if (status != EXCEPTION_NONE) {
            do { printf("\033[0m" "\033[1m" "<<" "\033[0m" "\033[32m" "seL4(CPU %" "lu" ")" "\033[0m" "\033[1m" " [%s/%d T%p \"%s\" @%lx]: " "IRQControl: IssueSGISignal: Target slot for new ARM_SGI_Signal cap not empty: cap %lu." ">>" "\033[0m" "\n", 0lu, __func__, 130, ksCurThread, ((debug_tcb_t *)(((cte_t *)((word_t)(ksCurThread)&~((1ul << (12)) - 1ul)))+(tcbCNodeEntries)))->tcbName, (word_t)getRestartPC(ksCurThread), getExtraCPtr(buffer, 0)); } while (0)
                                              ;
            return status;
        }
        setThreadState(ksCurThread, ThreadState_Restart);
        return Arch_invokeIssueSGISignal(irq, target, destSlot, srcSlot);
# 191 "/home/b/test/sel4test/kernel/src/arch/arm/object/interrupt.c"
    } else {
        current_syscall_error.type = seL4_IllegalOperation;
        return EXCEPTION_SYSCALL_ERROR;
    }
}



static exception_t invokeSGISignalGenerate(word_t irq, word_t target)
{
    plat_sendSGI(irq, target);
    return EXCEPTION_NONE;
}


exception_t decodeSGISignalInvocation(word_t invLabel, word_t length,
                                      cap_t cap, word_t *buffer)
{

    word_t irq = cap_sgi_signal_cap_get_capSGIIRQ(cap);
    word_t target = cap_sgi_signal_cap_get_capSGITarget(cap);

    setThreadState(ksCurThread, ThreadState_Restart);
    return invokeSGISignalGenerate(irq, target);
}
# 1 "/home/b/test/sel4test/kernel/src/arch/arm/object/iospace.c"
/*
 * Copyright 2016, General Dynamics C4 Systems
 *
 * SPDX-License-Identifier: GPL-2.0-only
 */
# 1 "/home/b/test/sel4test/kernel/src/arch/arm/object/smc.c"
/*
 * Copyright 2021, DornerWorks Ltd.
 *
 * SPDX-License-Identifier: GPL-2.0-only
 */
# 1 "/home/b/test/sel4test/kernel/src/arch/arm/object/smmu.c"
/*
 * Copyright 2020, Data61, CSIRO (ABN 41 687 119 230)
 *
 * SPDX-License-Identifier: GPL-2.0-only
 */
# 1 "/home/b/test/sel4test/kernel/src/arch/arm/object/tcb.c"
/*
 * Copyright 2014, General Dynamics C4 Systems
 *
 * SPDX-License-Identifier: GPL-2.0-only
 */
# 15 "/home/b/test/sel4test/kernel/src/arch/arm/object/tcb.c"
word_t __attribute__((__const__)) Arch_decodeTransfer(word_t flags)
{
    return 0;
}

exception_t __attribute__((__const__)) Arch_performTransfer(word_t arch, tcb_t *tcb_src, tcb_t *tcb_dest)
{
    return EXCEPTION_NONE;
}
# 1 "/home/b/test/sel4test/kernel/src/arch/arm/object/vcpu.c"
/*
 * Copyright 2014, General Dynamics C4 Systems
 *
 * SPDX-License-Identifier: GPL-2.0-only
 */
# 1 "/home/b/test/sel4test/kernel/src/arch/arm/smp/ipi.c"
/*
 * Copyright 2020, Data61, CSIRO (ABN 41 687 119 230)
 *
 * SPDX-License-Identifier: GPL-2.0-only
 */


# 1 "/home/b/test/sel4test/kernel/include/arch/arm/arch/64/mode/smp/ipi.h" 1
/*
 * Copyright 2020, Data61, CSIRO (ABN 41 687 119 230)
 *
 * SPDX-License-Identifier: GPL-2.0-only
 */

       
# 9 "/home/b/test/sel4test/kernel/src/arch/arm/smp/ipi.c" 2
# 1 "/home/b/test/sel4test/kernel/src/assert.c"
/*
 * Copyright 2014, General Dynamics C4 Systems
 *
 * SPDX-License-Identifier: GPL-2.0-only
 */






void _fail(
    const char *s,
    const char *file,
    unsigned int line,
    const char *function)
{
    printf(
        "seL4 called fail at %s:%u in function %s, saying \"%s\"\n",
        file,
        line,
        function,
        s
    );
    halt();
}

void _assert_fail(
    const char *assertion,
    const char *file,
    unsigned int line,
    const char *function)
{
    printf("seL4 failed assertion '%s' at %s:%u in function %s\n",
           assertion,
           file,
           line,
           function
          );
    halt();
}
# 1 "/home/b/test/sel4test/kernel/src/benchmark/benchmark.c"
/*
 * Copyright 2016, General Dynamics C4 Systems
 *
 * SPDX-License-Identifier: GPL-2.0-only
 */
# 1 "/home/b/test/sel4test/kernel/src/benchmark/benchmark_track.c"
/*
 * Copyright 2016, General Dynamics C4 Systems
 *
 * SPDX-License-Identifier: GPL-2.0-only
 */
# 1 "/home/b/test/sel4test/kernel/src/benchmark/benchmark_utilisation.c"
/*
 * Copyright 2016, General Dynamics C4 Systems
 *
 * SPDX-License-Identifier: GPL-2.0-only
 */
# 1 "/home/b/test/sel4test/kernel/src/drivers/serial/pl011.c"
/*
 * Copyright 2016, General Dynamics C4 Systems
 *
 * SPDX-License-Identifier: GPL-2.0-only
 */
# 22 "/home/b/test/sel4test/kernel/src/drivers/serial/pl011.c"
void uart_drv_putchar(unsigned char c)
{
    while ((*((volatile uint32_t *)((0xffffffffffe00000ul + 0x0) + (0x018))) & (1ul << (5))) != 0);

    *((volatile uint32_t *)((0xffffffffffe00000ul + 0x0) + (0x000))) = c;
}



unsigned char uart_drv_getchar(void)
{
    while ((*((volatile uint32_t *)((0xffffffffffe00000ul + 0x0) + (0x018))) & (1ul << (4))) != 0);

    return *((volatile uint32_t *)((0xffffffffffe00000ul + 0x0) + (0x000)));
}
# 1 "/home/b/test/sel4test/kernel/src/drivers/timer/generic_timer.c"
/*
 * Copyright 2020, Data61, CSIRO (ABN 41 687 119 230)
 *
 * SPDX-License-Identifier: GPL-2.0-only
 */



__attribute__((__section__(".boot.text"))) void initGenericTimer(void)
{
    if (wrap_config_set(1)) {
        /* check the frequency is correct */
        word_t gpt_cntfrq = 0;
        __asm__ volatile("mrs %x0," "cntfrq_el0" : "=r"(gpt_cntfrq));
        /* The CNTFRQ register is a 32-bit register, its value can safely be
         * compared with TIMER_CLOCK_HZ.
         */
        if ((gpt_cntfrq != 0) && (gpt_cntfrq != 50000000llu)) {
            /* TIMER_CLOCK_HZ is defined as a unsigned long long constant on
             * every architecture. */
            printf("Warning:  gpt_cntfrq %""lu"", expected %llu\n",
                   gpt_cntfrq, 50000000llu);
        }
    }





    resetTimer();

    do { word_t _v = (1ul << (0)); __asm__ volatile("msr " "cntv_ctl_el0" ",%x0" :: "r" (_v)); }while(0);
}

/*
 * The exynos5 platforms require custom hardware initialisation before the
 * generic timer is usable. They need to overwrite initTimer before calling
 * initGenericTimer because of this. We cannot use a `weak` symbol definition
 * in this case because the kernel is built as a single file and multiple
 * symbol definitions with the same name are not allowed. We therefore resort
 * to ifdef'ing out this initTimer definition for exynos5 platforms.
 */

__attribute__((__section__(".boot.text"))) void initTimer(void)
{
    initGenericTimer();
}
# 1 "/home/b/test/sel4test/kernel/src/fastpath/fastpath.c"
/*
 * Copyright 2014, General Dynamics C4 Systems
 *
 * SPDX-License-Identifier: GPL-2.0-only
 */


# 1 "/home/b/test/sel4test/kernel/include/fastpath/fastpath.h" 1
/*
 * Copyright 2014, General Dynamics C4 Systems
 *
 * SPDX-License-Identifier: GPL-2.0-only
 */

       
# 88 "/home/b/test/sel4test/kernel/include/fastpath/fastpath.h"
/* Fastpath cap lookup.  Returns a null_cap on failure. */
static inline cap_t __attribute__((always_inline)) lookup_fp(cap_t cap, cptr_t cptr)
{
    word_t cptr2;
    cte_t *slot;
    word_t guardBits, radixBits, bits;
    word_t radix, capGuard;

    bits = 0;

    if (__builtin_expect(!!(! cap_capType_equals(cap, cap_cnode_cap)), 0)) {
        return cap_null_cap_new();
    }

    do {
        guardBits = cap_cnode_cap_get_capCNodeGuardSize(cap);
        radixBits = cap_cnode_cap_get_capCNodeRadix(cap);
        cptr2 = cptr << bits;

        capGuard = cap_cnode_cap_get_capCNodeGuard(cap);

        /* Check the guard. Depth mismatch check is deferred.
           The 32MinusGuardSize encoding contains an exception
           when the guard is 0, when 32MinusGuardSize will be
           reported as 0 also. In this case we skip the check */
        if (__builtin_expect(!!(guardBits), 1) && __builtin_expect(!!(cptr2 >> ((1 << 6) - guardBits) != capGuard), 0)) {
            return cap_null_cap_new();
        }

        radix = cptr2 << guardBits >> ((1 << 6) - radixBits);
        slot = ((cte_t *)(cap_cnode_cap_get_capCNodePtr(cap))) + radix;

        cap = slot->cap;
        bits += guardBits + radixBits;

    } while (__builtin_expect(!!(bits < (1 << 6) && cap_capType_equals(cap, cap_cnode_cap)), 0));

    if (__builtin_expect(!!(bits > (1 << 6)), 0)) {
        /* Depth mismatch. We've overshot wordBits bits. The lookup we've done is
           safe, but wouldn't be allowed by the slowpath. */
        return cap_null_cap_new();
    }

    return cap;
}
/* make sure the fastpath functions conform with structure_*.bf */
static inline void thread_state_ptr_set_tsType_np(thread_state_t *ts_ptr, word_t tsType)
{
    ts_ptr->words[0] = tsType;
}

static inline void thread_state_ptr_mset_blockingObject_tsType(thread_state_t *ts_ptr,
                                                               word_t ep_ref,
                                                               word_t tsType)
{
    ts_ptr->words[0] = ep_ref | tsType;
}


static inline void cap_reply_cap_ptr_new_np(cap_t *cap_ptr, word_t capReplyCanGrant,
                                            word_t capReplyMaster, word_t capTCBPtr)
{

    cap_ptr->words[1] = (word_t)capTCBPtr;
    cap_ptr->words[0] = (capReplyMaster) | (capReplyCanGrant << 1) |
                        ((word_t)cap_reply_cap << 59);




}


static inline void endpoint_ptr_mset_epQueue_tail_state(endpoint_t *ep_ptr, word_t epQueue_tail,
                                                        word_t state)
{
    ep_ptr->words[0] = epQueue_tail | state;
}

static inline void endpoint_ptr_set_epQueue_head_np(endpoint_t *ep_ptr, word_t epQueue_head)
{
    ep_ptr->words[1] = epQueue_head;
}
# 9 "/home/b/test/sel4test/kernel/src/fastpath/fastpath.c" 2







static inline
__attribute__((always_inline))

void __attribute__((__noreturn__)) fastpath_call(word_t cptr, word_t msgInfo)
{
    seL4_MessageInfo_t info;
    cap_t ep_cap;
    endpoint_t *ep_ptr;
    word_t length;
    tcb_t *dest;
    word_t badge;
    cap_t newVTable;
    vspace_root_t *cap_pd;
    pde_t stored_hw_asid;
    word_t fault_type;
    dom_t dom;

    /* Get message info, length, and fault type. */
    info = messageInfoFromWord_raw(msgInfo);
    length = seL4_MessageInfo_get_length(info);
    fault_type = seL4_Fault_get_seL4_FaultType(ksCurThread->tcbFault);

    /* Check there's no extra caps, the length is ok and there's no
     * saved fault. */
    if (__builtin_expect(!!(fastpath_mi_check(msgInfo) || fault_type != seL4_Fault_NullFault), 0)
                                                    ) {
        slowpath(SysCall);
    }

    /* Lookup the cap */
    ep_cap = lookup_fp((((cte_t *)((word_t)(ksCurThread)&~((1ul << (12)) - 1ul)))+(tcbCTable))->cap, cptr);

    /* Check it's an endpoint */
    if (__builtin_expect(!!(!cap_capType_equals(ep_cap, cap_endpoint_cap) || !cap_endpoint_cap_get_capCanSend(ep_cap)), 0)
                                                          ) {
        slowpath(SysCall);
    }

    /* Get the endpoint address */
    ep_ptr = ((endpoint_t *)(cap_endpoint_cap_get_capEPPtr(ep_cap)));

    /* Get the destination thread, which is only going to be valid
     * if the endpoint is valid. */
    dest = ((tcb_t *)(endpoint_ptr_get_epQueue_head(ep_ptr)));

    /* Check that there's a thread waiting to receive */
    if (__builtin_expect(!!(endpoint_ptr_get_state(ep_ptr) != EPState_Recv), 0)) {
        slowpath(SysCall);
    }

    /* ensure we are not single stepping the destination in ia32 */






    /* Get destination thread.*/
    newVTable = (((cte_t *)((word_t)(dest)&~((1ul << (12)) - 1ul)))+(tcbVTable))->cap;

    /* Get vspace root. */
    cap_pd = ((vspace_root_t *)(cap_vspace_cap_get_capVSBasePtr(newVTable)));

    /* Ensure that the destination has a valid VTable. */
    if (__builtin_expect(!!(! isValidVTableRoot_fp(newVTable)), 0)) {
        slowpath(SysCall);
    }
# 99 "/home/b/test/sel4test/kernel/src/fastpath/fastpath.c"
    /* Need to test that the ASID is still valid */
    asid_t asid = cap_vspace_cap_get_capVSMappedASID(newVTable);
    asid_map_t asid_map = findMapForASID(asid);
    if (__builtin_expect(!!(asid_map_get_type(asid_map) != asid_map_asid_map_vspace || ((vspace_root_t *)(asid_map_asid_map_vspace_get_vspace_root(asid_map))) != cap_pd), 0)
                                                                                          ) {
        slowpath(SysCall);
    }
# 114 "/home/b/test/sel4test/kernel/src/fastpath/fastpath.c"
    stored_hw_asid.words[0] = asid;
# 123 "/home/b/test/sel4test/kernel/src/fastpath/fastpath.c"
    /* let gcc optimise this out for 1 domain */
    dom = maxDom ? ksCurDomain : 0;
    /* ensure only the idle thread or lower prio threads are present in the scheduler */
    if (__builtin_expect(!!(dest->tcbPriority < ksCurThread->tcbPriority && !isHighestPrio(dom, dest->tcbPriority)), 0)
                                                        ) {
        slowpath(SysCall);
    }

    /* Ensure that the endpoint has has grant or grant-reply rights so that we can
     * create the reply cap */
    if (__builtin_expect(!!(!cap_endpoint_cap_get_capCanGrant(ep_cap) && !cap_endpoint_cap_get_capCanGrantReply(ep_cap)), 0)
                                                                ) {
        slowpath(SysCall);
    }







    /* Ensure the original caller is in the current domain and can be scheduled directly. */
    if (__builtin_expect(!!(dest->tcbDomain != ksCurDomain && 0 < maxDom), 0)) {
        slowpath(SysCall);
    }
# 167 "/home/b/test/sel4test/kernel/src/fastpath/fastpath.c"
    /*
     * --- POINT OF NO RETURN ---
     *
     * At this stage, we have committed to performing the IPC.
     */





    /* Dequeue the destination. */
    endpoint_ptr_set_epQueue_head_np(ep_ptr, ((word_t)(dest->tcbEPNext)));
    if (__builtin_expect(!!(dest->tcbEPNext), 0)) {
        dest->tcbEPNext->tcbEPPrev = ((void *)0);
    } else {
        endpoint_ptr_mset_epQueue_tail_state(ep_ptr, 0, EPState_Idle);
    }

    badge = cap_endpoint_cap_get_capEPBadge(ep_cap);

    /* Unlink dest <-> reply, link src (cur thread) <-> reply */
    thread_state_ptr_set_tsType_np(&ksCurThread->tcbState,
                                   ThreadState_BlockedOnReply);
# 208 "/home/b/test/sel4test/kernel/src/fastpath/fastpath.c"
    /* Get sender reply slot */
    cte_t *replySlot = (((cte_t *)((word_t)(ksCurThread)&~((1ul << (12)) - 1ul)))+(tcbReply));

    /* Get dest caller slot */
    cte_t *callerSlot = (((cte_t *)((word_t)(dest)&~((1ul << (12)) - 1ul)))+(tcbCaller));

    /* Insert reply cap */
    word_t replyCanGrant = thread_state_ptr_get_blockingIPCCanGrant(&dest->tcbState);;
    cap_reply_cap_ptr_new_np(&callerSlot->cap, replyCanGrant, 0,
                             ((word_t)(ksCurThread)));
    mdb_node_ptr_set_mdbPrev_np(&callerSlot->cteMDBNode, ((word_t)(replySlot)));
    mdb_node_ptr_mset_mdbNext_mdbRevocable_mdbFirstBadged(
        &replySlot->cteMDBNode, ((word_t)(callerSlot)), 1, 1);


    fastpath_copy_mrs(length, ksCurThread, dest);

    /* Dest thread is set Running, but not queued. */
    thread_state_ptr_set_tsType_np(&dest->tcbState,
                                   ThreadState_Running);
    switchToThread_fp(dest, cap_pd, stored_hw_asid);

    msgInfo = wordFromMessageInfo(seL4_MessageInfo_set_capsUnwrapped(info, 0));

    fastpath_restore(badge, msgInfo, ksCurThread);
}


static inline
__attribute__((always_inline))




void __attribute__((__noreturn__)) fastpath_reply_recv(word_t cptr, word_t msgInfo)

{
    seL4_MessageInfo_t info;
    cap_t ep_cap;
    endpoint_t *ep_ptr;
    word_t length;
    tcb_t *caller;
    word_t badge;
    tcb_t *endpointTail;
    word_t fault_type;

    cap_t newVTable;
    vspace_root_t *cap_pd;
    pde_t stored_hw_asid;
    dom_t dom;

    /* Get message info and length */
    info = messageInfoFromWord_raw(msgInfo);
    length = seL4_MessageInfo_get_length(info);
    fault_type = seL4_Fault_get_seL4_FaultType(ksCurThread->tcbFault);

    /* Check there's no extra caps, the length is ok and there's no
     * saved fault. */
    if (__builtin_expect(!!(fastpath_mi_check(msgInfo) || fault_type != seL4_Fault_NullFault), 0)
                                                    ) {
        slowpath(SysReplyRecv);
    }

    /* Lookup the cap */
    ep_cap = lookup_fp((((cte_t *)((word_t)(ksCurThread)&~((1ul << (12)) - 1ul)))+(tcbCTable))->cap,
                       cptr);

    /* Check it's an endpoint */
    if (__builtin_expect(!!(!cap_capType_equals(ep_cap, cap_endpoint_cap) || !cap_endpoint_cap_get_capCanReceive(ep_cap)), 0)
                                                             ) {
        slowpath(SysReplyRecv);
    }
# 291 "/home/b/test/sel4test/kernel/src/fastpath/fastpath.c"
    /* Check there is nothing waiting on the notification */
    if (__builtin_expect(!!(ksCurThread->tcbBoundNotification && notification_ptr_get_state(ksCurThread->tcbBoundNotification) == NtfnState_Active), 0)
                                                                                                               ) {
        slowpath(SysReplyRecv);
    }

    /* Get the endpoint address */
    ep_ptr = ((endpoint_t *)(cap_endpoint_cap_get_capEPPtr(ep_cap)));

    /* Check that there's not a thread waiting to send */
    if (__builtin_expect(!!(endpoint_ptr_get_state(ep_ptr) == EPState_Send), 0)) {
        slowpath(SysReplyRecv);
    }
# 319 "/home/b/test/sel4test/kernel/src/fastpath/fastpath.c"
    /* Only reply if the reply cap is valid. */
    cte_t *callerSlot = (((cte_t *)((word_t)(ksCurThread)&~((1ul << (12)) - 1ul)))+(tcbCaller));
    cap_t callerCap = callerSlot->cap;
    if (__builtin_expect(!!(!fastpath_reply_cap_check(callerCap)), 0)) {
        slowpath(SysReplyRecv);
    }

    /* Determine who the caller is. */
    caller = ((tcb_t *)(cap_reply_cap_get_capTCBPtr(callerCap)));


    /* ensure we are not single stepping the caller in ia32 */






    /* Check that the caller has not faulted, in which case a fault
       reply is generated instead. */
    fault_type = seL4_Fault_get_seL4_FaultType(caller->tcbFault);

    /* Change this as more types of faults are supported */

    if (__builtin_expect(!!(fault_type != seL4_Fault_NullFault), 0)) {
        slowpath(SysReplyRecv);
    }






    /* Get destination thread.*/
    newVTable = (((cte_t *)((word_t)(caller)&~((1ul << (12)) - 1ul)))+(tcbVTable))->cap;

    /* Get vspace root. */
    cap_pd = ((vspace_root_t *)(cap_vspace_cap_get_capVSBasePtr(newVTable)));

    /* Ensure that the destination has a valid MMU. */
    if (__builtin_expect(!!(! isValidVTableRoot_fp(newVTable)), 0)) {
        slowpath(SysReplyRecv);
    }
# 376 "/home/b/test/sel4test/kernel/src/fastpath/fastpath.c"
    /* Need to test that the ASID is still valid */
    asid_t asid = cap_vspace_cap_get_capVSMappedASID(newVTable);
    asid_map_t asid_map = findMapForASID(asid);
    if (__builtin_expect(!!(asid_map_get_type(asid_map) != asid_map_asid_map_vspace || ((vspace_root_t *)(asid_map_asid_map_vspace_get_vspace_root(asid_map))) != cap_pd), 0)
                                                                                          ) {
        slowpath(SysReplyRecv);
    }
# 392 "/home/b/test/sel4test/kernel/src/fastpath/fastpath.c"
    stored_hw_asid.words[0] = asid;







    /* Ensure the original caller can be scheduled directly. */
    dom = maxDom ? ksCurDomain : 0;
    if (__builtin_expect(!!(!isHighestPrio(dom, caller->tcbPriority)), 0)) {
        slowpath(SysReplyRecv);
    }
# 413 "/home/b/test/sel4test/kernel/src/fastpath/fastpath.c"
    /* Ensure the original caller is in the current domain and can be scheduled directly. */
    if (__builtin_expect(!!(caller->tcbDomain != ksCurDomain && 0 < maxDom), 0)) {
        slowpath(SysReplyRecv);
    }
# 431 "/home/b/test/sel4test/kernel/src/fastpath/fastpath.c"
    /*
     * --- POINT OF NO RETURN ---
     *
     * At this stage, we have committed to performing the IPC.
     */





    /* Set thread state to BlockedOnReceive */
    thread_state_ptr_mset_blockingObject_tsType(
        &ksCurThread->tcbState, (word_t)ep_ptr, ThreadState_BlockedOnReceive);







    thread_state_ptr_set_blockingIPCCanGrant(&ksCurThread->tcbState,
                                             cap_endpoint_cap_get_capCanGrant(ep_cap));;


    /* Place the thread in the endpoint queue */
    endpointTail = ((tcb_t *)(endpoint_ptr_get_epQueue_tail(ep_ptr)));
    if (__builtin_expect(!!(!endpointTail), 1)) {
        ksCurThread->tcbEPPrev = ((void *)0);
        ksCurThread->tcbEPNext = ((void *)0);

        /* Set head/tail of queue and endpoint state. */
        endpoint_ptr_set_epQueue_head_np(ep_ptr, ((word_t)(ksCurThread)));
        endpoint_ptr_mset_epQueue_tail_state(ep_ptr, ((word_t)(ksCurThread)),
                                             EPState_Recv);
    } else {






        /* Append current thread onto the queue. */
        endpointTail->tcbEPNext = ksCurThread;
        ksCurThread->tcbEPPrev = endpointTail;
        ksCurThread->tcbEPNext = ((void *)0);

        /* Update tail of queue. */
        endpoint_ptr_mset_epQueue_tail_state(ep_ptr, ((word_t)(ksCurThread)),
                                             EPState_Recv);

    }
# 500 "/home/b/test/sel4test/kernel/src/fastpath/fastpath.c"
    /* Delete the reply cap. */
    mdb_node_ptr_mset_mdbNext_mdbRevocable_mdbFirstBadged(
        &((cte_t *)(mdb_node_get_mdbPrev(callerSlot->cteMDBNode)))->cteMDBNode,
        0, 1, 1);
    callerSlot->cap = cap_null_cap_new();
    callerSlot->cteMDBNode = mdb_node_new(0, false, false, 0);
# 533 "/home/b/test/sel4test/kernel/src/fastpath/fastpath.c"
        /* There's no fault, so straight to the transfer. */

        /* Replies don't have a badge. */
        badge = 0;

        fastpath_copy_mrs(length, ksCurThread, caller);

        /* Dest thread is set Running, but not queued. */
        thread_state_ptr_set_tsType_np(&caller->tcbState, ThreadState_Running);
        switchToThread_fp(caller, cap_pd, stored_hw_asid);

        msgInfo = wordFromMessageInfo(seL4_MessageInfo_set_capsUnwrapped(info, 0));

        fastpath_restore(badge, msgInfo, ksCurThread);




}
# 1 "/home/b/test/sel4test/kernel/src/inlines.c"
/*
 * Copyright 2014, General Dynamics C4 Systems
 *
 * SPDX-License-Identifier: GPL-2.0-only
 */




lookup_fault_t current_lookup_fault;
seL4_Fault_t current_fault;
syscall_error_t current_syscall_error;
# 1 "/home/b/test/sel4test/kernel/src/kernel/boot.c"
/*
 * Copyright 2014, General Dynamics C4 Systems
 *
 * SPDX-License-Identifier: GPL-2.0-only
 */
# 20 "/home/b/test/sel4test/kernel/src/kernel/boot.c"
/* (node-local) state accessed only during bootstrapping */
__attribute__((__section__(".boot.bss"))) ndks_boot_t ndks_boot;

__attribute__((__section__(".boot.bss"))) rootserver_mem_t rootserver;
__attribute__((__section__(".boot.bss"))) static region_t rootserver_mem;

/* Returns the physical region of the kernel image boot part, which is the part
 * that is no longer needed once booting is finished. */
extern char ki_boot_end[1];
__attribute__((__section__(".boot.text"))) p_region_t get_p_reg_kernel_img_boot(void)
{
    return (p_region_t) {
        .start = addrFromKPPtr((const void *)((0xffffff8000000000ul - 0x0ul) + physBase())),
        .end = addrFromKPPtr(ki_boot_end)
    };
}

/* Returns the physical region of the kernel image. */
__attribute__((__section__(".boot.text"))) p_region_t get_p_reg_kernel_img(void)
{
    return (p_region_t) {
        .start = addrFromKPPtr((const void *)((0xffffff8000000000ul - 0x0ul) + physBase())),
        .end = addrFromKPPtr(ki_end)
    };
}

__attribute__((__section__(".boot.text"))) static void merge_regions(void)
{
    /* Walk through reserved regions and see if any can be merged */
    for (word_t i = 1; i < ndks_boot.resv_count;) {
        if (ndks_boot.reserved[i - 1].end == ndks_boot.reserved[i].start) {
            /* extend earlier region */
            ndks_boot.reserved[i - 1].end = ndks_boot.reserved[i].end;
            /* move everything else down */
            for (word_t j = i + 1; j < ndks_boot.resv_count; j++) {
                ndks_boot.reserved[j - 1] = ndks_boot.reserved[j];
            }

            ndks_boot.resv_count--;
            /* don't increment i in case there are multiple adjacent regions */
        } else {
            i++;
        }
    }
}

__attribute__((__section__(".boot.text"))) bool_t reserve_region(p_region_t reg)
{
    word_t i;
    do { if (!(reg.start <= reg.end)) { _assert_fail("reg.start <= reg.end", "/home/b/test/sel4test/kernel/src/kernel/boot.c", 69, __func__); } } while(0);
    if (reg.start == reg.end) {
        return true;
    }

    /* keep the regions in order */
    for (i = 0; i < ndks_boot.resv_count; i++) {
        /* Try and merge the region to an existing one, if possible */
        if (ndks_boot.reserved[i].start == reg.end) {
            ndks_boot.reserved[i].start = reg.start;
            merge_regions();
            return true;
        }
        if (ndks_boot.reserved[i].end == reg.start) {
            ndks_boot.reserved[i].end = reg.end;
            merge_regions();
            return true;
        }
        /* Otherwise figure out where it should go. */
        if (ndks_boot.reserved[i].start > reg.end) {
            /* move regions down, making sure there's enough room */
            if (ndks_boot.resv_count + 1 >= (((sizeof(avail_p_regs) / sizeof((avail_p_regs)[0])) + 0 + 1 + 1) + (sizeof(kernel_device_frames) / sizeof((kernel_device_frames)[0])) + (3 + 0))) {
                printf("Can't mark region 0x%""lx""-0x%""lx"
                       " as reserved, try increasing MAX_NUM_RESV_REG (currently %d)\n",
                       reg.start, reg.end, (int)(((sizeof(avail_p_regs) / sizeof((avail_p_regs)[0])) + 0 + 1 + 1) + (sizeof(kernel_device_frames) / sizeof((kernel_device_frames)[0])) + (3 + 0)));
                return false;
            }
            for (word_t j = ndks_boot.resv_count; j > i; j--) {
                ndks_boot.reserved[j] = ndks_boot.reserved[j - 1];
            }
            /* insert the new region */
            ndks_boot.reserved[i] = reg;
            ndks_boot.resv_count++;
            return true;
        }
    }

    if (i + 1 == (((sizeof(avail_p_regs) / sizeof((avail_p_regs)[0])) + 0 + 1 + 1) + (sizeof(kernel_device_frames) / sizeof((kernel_device_frames)[0])) + (3 + 0))) {
        printf("Can't mark region 0x%""lx""-0x%""lx"
               " as reserved, try increasing MAX_NUM_RESV_REG (currently %d)\n",
               reg.start, reg.end, (int)(((sizeof(avail_p_regs) / sizeof((avail_p_regs)[0])) + 0 + 1 + 1) + (sizeof(kernel_device_frames) / sizeof((kernel_device_frames)[0])) + (3 + 0)));
        return false;
    }

    ndks_boot.reserved[i] = reg;
    ndks_boot.resv_count++;

    return true;
}

__attribute__((__section__(".boot.text"))) static bool_t insert_region(region_t reg)
{
    do { if (!(reg.start <= reg.end)) { _assert_fail("reg.start <= reg.end", "/home/b/test/sel4test/kernel/src/kernel/boot.c", 121, __func__); } } while(0);
    if (is_reg_empty(reg)) {
        return true;
    }

    for (word_t i = 0; i < (sizeof(ndks_boot.freemem) / sizeof((ndks_boot.freemem)[0])); i++) {
        if (is_reg_empty(ndks_boot.freemem[i])) {
            reserve_region(pptr_to_paddr_reg(reg));
            ndks_boot.freemem[i] = reg;
            return true;
        }
    }

    /* We don't know if a platform or architecture picked MAX_NUM_FREEMEM_REG
     * arbitrarily or carefully calculated it to be big enough. Running out of
     * slots here is not really fatal, eventually memory allocation may fail
     * if there is not enough free memory. However, allocations should never
     * blindly assume to work, some error handling must always be in place even
     * if the environment has been crafted carefully to support them. Thus, we
     * don't stop the boot process here, but return an error. The caller should
     * decide how bad this is.
     */
    printf("no free memory slot left for [%""lx""..%""lx""),"
           " consider increasing MAX_NUM_FREEMEM_REG (%u)\n",
           reg.start, reg.end, (unsigned int)((sizeof(avail_p_regs) / sizeof((avail_p_regs)[0])) + 0 + 1 + 1));

    /* For debug builds we consider this a fatal error. Rationale is, that the
     * caller does not check the error code at the moment, but just ignores any
     * failures silently. */
    do { if (!(0)) { _assert_fail("0", "/home/b/test/sel4test/kernel/src/kernel/boot.c", 150, __func__); } } while(0);

    return false;
}

__attribute__((__section__(".boot.text"))) static pptr_t alloc_rootserver_obj(word_t size_bits, word_t n)
{
    pptr_t allocated = rootserver_mem.start;
    /* allocated memory must be aligned */
    do { if (!(allocated % (1ul << (size_bits)) == 0)) { _assert_fail("allocated % BIT(size_bits) == 0", "/home/b/test/sel4test/kernel/src/kernel/boot.c", 159, __func__); } } while(0);
    rootserver_mem.start += (n * (1ul << (size_bits)));
    /* we must not have run out of memory */
    do { if (!(rootserver_mem.start <= rootserver_mem.end)) { _assert_fail("rootserver_mem.start <= rootserver_mem.end", "/home/b/test/sel4test/kernel/src/kernel/boot.c", 162, __func__); } } while(0);
    memzero((void *) allocated, n * (1ul << (size_bits)));
    return allocated;
}

__attribute__((__section__(".boot.text"))) static word_t rootserver_max_size_bits(word_t extra_bi_size_bits)
{
    word_t cnode_size_bits = 13 + 5;
    word_t max = (((cnode_size_bits)>(12))?(cnode_size_bits):(12));
    return (((max)>(extra_bi_size_bits))?(max):(extra_bi_size_bits));
}

__attribute__((__section__(".boot.text"))) static word_t calculate_rootserver_size(v_region_t it_v_reg, word_t extra_bi_size_bits)
{
    /* work out how much memory we need for root server objects */
    word_t size = (1ul << (13 + 5));
    size += (1ul << (12)); // root thread tcb
    size += (1ul << (12)); // ipc buf
    size += (1ul << (12)); // boot info
    size += (1ul << (12));
    size += extra_bi_size_bits > 0 ? (1ul << (extra_bi_size_bits)) : 0;
    size += (1ul << (12)); // root vspace



    /* for all archs, seL4_PageTable Bits is the size of all non top-level paging structures */
    return size + arch_get_n_paging(it_v_reg) * (1ul << (12));
}

__attribute__((__section__(".boot.text"))) static void maybe_alloc_extra_bi(word_t cmp_size_bits, word_t extra_bi_size_bits)
{
    if (extra_bi_size_bits >= cmp_size_bits && rootserver.extra_bi == 0) {
        rootserver.extra_bi = alloc_rootserver_obj(extra_bi_size_bits, 1);
    }
}

/* Create pptrs for all root server objects, starting at a give start address,
 * to cover the virtual memory region v_reg, and any extra boot info.
 */
__attribute__((__section__(".boot.text"))) static void create_rootserver_objects(pptr_t start, v_region_t it_v_reg,
                                                word_t extra_bi_size_bits)
{
    /* the largest object the PD, the root cnode, or the extra boot info */
    word_t cnode_size_bits = 13 + 5;
    word_t max = rootserver_max_size_bits(extra_bi_size_bits);

    word_t size = calculate_rootserver_size(it_v_reg, extra_bi_size_bits);
    rootserver_mem.start = start;
    rootserver_mem.end = start + size;

    maybe_alloc_extra_bi(max, extra_bi_size_bits);

    /* the root cnode is at least 4k, so it could be larger or smaller than a pd. */

    rootserver.cnode = alloc_rootserver_obj(cnode_size_bits, 1);
    maybe_alloc_extra_bi(12, extra_bi_size_bits);
    rootserver.vspace = alloc_rootserver_obj(12, 1);






    /* at this point we are up to creating 4k objects - which is the min size of
     * extra_bi so this is the last chance to allocate it */
    maybe_alloc_extra_bi(12, extra_bi_size_bits);
    _Static_assert(12 == 12, "invalid_seL4_ASIDPoolBits");;
    rootserver.asid_pool = alloc_rootserver_obj(12, 1);
    rootserver.ipc_buf = alloc_rootserver_obj(12, 1);
    /* The boot info size must be at least one page. Due to the hard-coded order
     * of allocations used in the current implementation here, it can't be any
     * bigger.
     */
    _Static_assert(12 == 12, "invalid_seL4_BootInfoFrameBits");;
    rootserver.boot_info = alloc_rootserver_obj(12, 1);

    /* TCBs on aarch32 can be larger than page tables in certain configs */

    rootserver.tcb = alloc_rootserver_obj(12, 1);


    /* paging structures are 4k on every arch except aarch32 (1k) */
    word_t n = arch_get_n_paging(it_v_reg);
    rootserver.paging.start = alloc_rootserver_obj(12, n);
    rootserver.paging.end = rootserver.paging.start + n * (1ul << (12));

    /* for most archs, TCBs are smaller than page tables */







    /* we should have allocated all our memory */
    do { if (!(rootserver_mem.start == rootserver_mem.end)) { _assert_fail("rootserver_mem.start == rootserver_mem.end", "/home/b/test/sel4test/kernel/src/kernel/boot.c", 257, __func__); } } while(0);
}

__attribute__((__section__(".boot.text"))) void write_slot(slot_ptr_t slot_ptr, cap_t cap)
{
    slot_ptr->cap = cap;

    slot_ptr->cteMDBNode = mdb_node_new(0, false, false, 0);
    mdb_node_ptr_set_mdbRevocable(&slot_ptr->cteMDBNode, true);
    mdb_node_ptr_set_mdbFirstBadged(&slot_ptr->cteMDBNode, true);
}

/* Our root CNode needs to be able to fit all the initial caps and not
 * cover all of memory.
 */
_Static_assert(13 < 32 - 5 && (1ul << (13)) >= seL4_NumInitialCaps && 13 >= (12 - 5), "root_cnode_size_valid");




__attribute__((__section__(".boot.text"))) cap_t
create_root_cnode(void)
{
    cap_t cap = cap_cnode_cap_new(
                    13, /* radix */
                    (1 << 6) - 13, /* guard size */
                    0, /* guard */
                    rootserver.cnode); /* pptr */

    /* write the root CNode cap into the root CNode */
    write_slot((((slot_ptr_t)(rootserver.cnode)) + (seL4_CapInitThreadCNode)), cap);

    return cap;
}

/* Check domain scheduler assumptions. */
_Static_assert(1 >= 1 && 1 <= 256, "num_domains_valid");

_Static_assert(256 >= 1 && 256 <= 256, "num_priorities_valid");


__attribute__((__section__(".boot.text"))) void
create_domain_cap(cap_t root_cnode_cap)
{
    /* Check domain scheduler assumptions. */
    do { if (!(ksDomScheduleLength > 0)) { _assert_fail("ksDomScheduleLength > 0", "/home/b/test/sel4test/kernel/src/kernel/boot.c", 302, __func__); } } while(0);
    for (word_t i = 0; i < ksDomScheduleLength; i++) {
        do { if (!(ksDomSchedule[i].domain < 1)) { _assert_fail("ksDomSchedule[i].domain < CONFIG_NUM_DOMAINS", "/home/b/test/sel4test/kernel/src/kernel/boot.c", 304, __func__); } } while(0);
        do { if (!(ksDomSchedule[i].length > 0)) { _assert_fail("ksDomSchedule[i].length > 0", "/home/b/test/sel4test/kernel/src/kernel/boot.c", 305, __func__); } } while(0);
    }

    cap_t cap = cap_domain_cap_new();
    write_slot((((slot_ptr_t)(((pptr_t)cap_get_capPtr(root_cnode_cap)))) + (seL4_CapDomain)), cap);
}

__attribute__((__section__(".boot.text"))) cap_t create_ipcbuf_frame_cap(cap_t root_cnode_cap, cap_t pd_cap, vptr_t vptr)
{
    clearMemory((void *)rootserver.ipc_buf, 12);

    /* create a cap of it and write it into the root CNode */
    cap_t cap = create_mapped_it_frame_cap(pd_cap, rootserver.ipc_buf, vptr, 1 /* initial thread's ASID */, false, false);
    write_slot((((slot_ptr_t)(((pptr_t)cap_get_capPtr(root_cnode_cap)))) + (seL4_CapInitThreadIPCBuffer)), cap);

    return cap;
}

__attribute__((__section__(".boot.text"))) void create_bi_frame_cap(cap_t root_cnode_cap, cap_t pd_cap, vptr_t vptr)
{
    /* create a cap of it and write it into the root CNode */
    cap_t cap = create_mapped_it_frame_cap(pd_cap, rootserver.boot_info, vptr, 1 /* initial thread's ASID */, false, false);
    write_slot((((slot_ptr_t)(((pptr_t)cap_get_capPtr(root_cnode_cap)))) + (seL4_CapBootInfoFrame)), cap);
}

/**
 * the size_bits we return is 0 for extra_size = 0
 * and if it is non-zero the bits are always >= seL4_PageBits
 * this is relied on in a few places, and gives us code of the form
 *
 *     extra_bi_size_bits > 0 ? BIT(extra_bi_size_bits) : 0
 *
 * which handles the 0-size case.
 */
__attribute__((__section__(".boot.text"))) word_t calculate_extra_bi_size_bits(word_t extra_size)
{
    if (extra_size == 0) {
        return 0;
    }

    word_t clzl_ret = clzl((((((extra_size) - 1ul) >> (12)) + 1ul) << (12)));
    word_t msb = (sizeof(seL4_Word) * 8) - 1 - clzl_ret;
    /* If region is bigger than a page, make sure we overallocate rather than
     * underallocate
     */
    if (extra_size > (1ul << (msb))) {
        msb++;
    }
    return msb;
}

__attribute__((__section__(".boot.text"))) void populate_bi_frame(node_id_t node_id, word_t num_nodes,
                                 vptr_t ipcbuf_vptr, word_t extra_bi_size)
{
    /* clear boot info memory */
    clearMemory((void *)rootserver.boot_info, 12);
    if (extra_bi_size) {
        clearMemory((void *)rootserver.extra_bi,
                    calculate_extra_bi_size_bits(extra_bi_size));
    }

    /* initialise bootinfo-related global state */
    seL4_BootInfo *bi = ((seL4_BootInfo*)(rootserver.boot_info));
    bi->nodeID = node_id;
    bi->numNodes = num_nodes;
    bi->numIOPTLevels = 0;
    bi->ipcBuffer = (seL4_IPCBuffer *)ipcbuf_vptr;
    bi->initThreadCNodeSizeBits = 13;
    bi->initThreadDomain = ksDomSchedule[ksDomScheduleIdx].domain;
    bi->extraLen = extra_bi_size;

    ndks_boot.bi_frame = bi;
    ndks_boot.slot_pos_cur = seL4_NumInitialCaps;
}

__attribute__((__section__(".boot.text"))) bool_t provide_cap(cap_t root_cnode_cap, cap_t cap)
{
    if (ndks_boot.slot_pos_cur >= (1ul << (13))) {
        printf("ERROR: can't add another cap, all %""lu"
               " (=2^CONFIG_ROOT_CNODE_SIZE_BITS) slots used\n",
               (1ul << (13)));
        return false;
    }
    write_slot((((slot_ptr_t)(((pptr_t)cap_get_capPtr(root_cnode_cap)))) + (ndks_boot.slot_pos_cur)), cap);
    ndks_boot.slot_pos_cur++;
    return true;
}

__attribute__((__section__(".boot.text"))) create_frames_of_region_ret_t create_frames_of_region(
    cap_t root_cnode_cap,
    cap_t pd_cap,
    region_t reg,
    bool_t do_map,
    sword_t pv_offset
)
{
    pptr_t f;
    cap_t frame_cap;
    seL4_SlotPos slot_pos_before;
    seL4_SlotPos slot_pos_after;

    slot_pos_before = ndks_boot.slot_pos_cur;

    for (f = reg.start; f < reg.end; f += (1ul << (12))) {
        if (do_map) {
            frame_cap = create_mapped_it_frame_cap(pd_cap, f, addrFromPPtr((void *)(f - pv_offset)), 1 /* initial thread's ASID */, false, true);
        } else {
            frame_cap = create_unmapped_it_frame_cap(f, false);
        }
        if (!provide_cap(root_cnode_cap, frame_cap)) {
            return (create_frames_of_region_ret_t) {
                .region = (seL4_SlotRegion){ .start = 0, .end = 0 },
                .success = false
            };
        }
    }

    slot_pos_after = ndks_boot.slot_pos_cur;

    return (create_frames_of_region_ret_t) {
        .region = (seL4_SlotRegion) {
            .start = slot_pos_before,
            .end = slot_pos_after
        },
        .success = true
    };
}

__attribute__((__section__(".boot.text"))) cap_t create_it_asid_pool(cap_t root_cnode_cap)
{
    cap_t ap_cap = cap_asid_pool_cap_new(1 /* initial thread's ASID */ >> asidLowBits, rootserver.asid_pool);
    write_slot((((slot_ptr_t)(((pptr_t)cap_get_capPtr(root_cnode_cap)))) + (seL4_CapInitThreadASIDPool)), ap_cap);

    /* create ASID control cap */
    write_slot(
        (((slot_ptr_t)(((pptr_t)cap_get_capPtr(root_cnode_cap)))) + (seL4_CapASIDControl)),
        cap_asid_control_cap_new()
    );

    return ap_cap;
}
# 477 "/home/b/test/sel4test/kernel/src/kernel/boot.c"
__attribute__((__section__(".boot.text"))) void create_idle_thread(void)
{
    pptr_t pptr;




        pptr = (pptr_t) &ksIdleThreadTCB[0];
        ksIdleThread = ((tcb_t *)(pptr + (1ul << ((12 - 1)))));
        configureIdleThread(ksIdleThread);

        setThreadName(ksIdleThread, "idle_thread");

        ;
# 500 "/home/b/test/sel4test/kernel/src/kernel/boot.c"
}

__attribute__((__section__(".boot.text"))) tcb_t *create_initial_thread(cap_t root_cnode_cap, cap_t it_pd_cap, vptr_t ui_v_entry, vptr_t bi_frame_vptr,
                                       vptr_t ipcbuf_vptr, cap_t ipcbuf_cap)
{
    tcb_t *tcb = ((tcb_t *)(rootserver.tcb + (1ul << ((12 - 1)))));

    tcb->tcbTimeSlice = 5;


    Arch_initContext(&tcb->tcbArch.tcbContext);

    /* derive a copy of the IPC buffer cap for inserting */
    deriveCap_ret_t dc_ret = deriveCap((((slot_ptr_t)(((pptr_t)cap_get_capPtr(root_cnode_cap)))) + (seL4_CapInitThreadIPCBuffer)), ipcbuf_cap);
    if (dc_ret.status != EXCEPTION_NONE) {
        printf("Failed to derive copy of IPC Buffer\n");
        return ((void *)0);
    }

    /* initialise TCB (corresponds directly to abstract specification) */
    cteInsert(
        root_cnode_cap,
        (((slot_ptr_t)(((pptr_t)cap_get_capPtr(root_cnode_cap)))) + (seL4_CapInitThreadCNode)),
        (((slot_ptr_t)(rootserver.tcb)) + (tcbCTable))
    );
    cteInsert(
        it_pd_cap,
        (((slot_ptr_t)(((pptr_t)cap_get_capPtr(root_cnode_cap)))) + (seL4_CapInitThreadVSpace)),
        (((slot_ptr_t)(rootserver.tcb)) + (tcbVTable))
    );
    cteInsert(
        dc_ret.cap,
        (((slot_ptr_t)(((pptr_t)cap_get_capPtr(root_cnode_cap)))) + (seL4_CapInitThreadIPCBuffer)),
        (((slot_ptr_t)(rootserver.tcb)) + (tcbBuffer))
    );
    tcb->tcbIPCBuffer = ipcbuf_vptr;

    setRegister(tcb, capRegister, bi_frame_vptr);
    setNextPC(tcb, ui_v_entry);

    /* initialise TCB */




    tcb->tcbPriority = seL4_MaxPrio;
    tcb->tcbMCP = seL4_MaxPrio;
    tcb->tcbDomain = ksDomSchedule[ksDomScheduleIdx].domain;

    setupReplyMaster(tcb);

    setThreadState(tcb, ThreadState_Running);

    ksCurDomain = ksDomSchedule[ksDomScheduleIdx].domain;



    ksDomainTime = ksDomSchedule[ksDomScheduleIdx].length;

    do { if (!(ksCurDomain < 1 && ksDomainTime > 0)) { _assert_fail("ksCurDomain < CONFIG_NUM_DOMAINS && ksDomainTime > 0", "/home/b/test/sel4test/kernel/src/kernel/boot.c", 559, __func__); } } while(0);


    ;


    /* create initial thread's TCB cap */
    cap_t cap = cap_thread_cap_new(((word_t)(tcb)));
    write_slot((((slot_ptr_t)(((pptr_t)cap_get_capPtr(root_cnode_cap)))) + (seL4_CapInitThreadTCB)), cap);






    setThreadName(tcb, "rootserver");


    return tcb;
}
# 600 "/home/b/test/sel4test/kernel/src/kernel/boot.c"
__attribute__((__section__(".boot.text"))) void init_core_state(tcb_t *scheduler_action)
{

    ksCurFPUOwner = ((void *)0);


    /* add initial threads to the debug queue */
    ksDebugTCBs = ((void *)0);
    if (scheduler_action != ((tcb_t*)0) &&
        scheduler_action != ((tcb_t*) 1)) {
        tcbDebugAppend(scheduler_action);
    }
    tcbDebugAppend(ksIdleThread);

    ksSchedulerAction = scheduler_action;
    ksCurThread = ksIdleThread;
# 624 "/home/b/test/sel4test/kernel/src/kernel/boot.c"
}

/**
 * Sanity check if a kernel-virtual pointer is in the kernel window that maps
 * physical memory.
 *
 * This check is necessary, but not sufficient, because it only checks for the
 * pointer interval, not for any potential holes in the memory window.
 *
 * @param pptr the pointer to check
 * @return false if the pointer is definitely not in the kernel window, true
 *         otherwise.
 */
__attribute__((__section__(".boot.text"))) static bool_t pptr_in_kernel_window(pptr_t pptr)
{
    return pptr >= 0xffffff8000000000ul && pptr < 0xffffffffc0000000ul;
}

/**
 * Create an untyped cap, store it in a cnode and mark it in boot info.
 *
 * The function can fail if basic sanity checks fail, or if there is no space in
 * boot info or cnode to store the cap.
 *
 * @param root_cnode_cap cap to the cnode to store the untyped cap in
 * @param device_memory true if the cap to create is a device untyped
 * @param pptr the kernel-virtual address of the untyped
 * @param size_bits the size of the untyped in bits
 * @param first_untyped_slot next available slot in the boot info structure
 * @return true on success, false on failure
 */
__attribute__((__section__(".boot.text"))) static bool_t provide_untyped_cap(
    cap_t root_cnode_cap,
    bool_t device_memory,
    pptr_t pptr,
    word_t size_bits,
    seL4_SlotPos first_untyped_slot
)
{
    bool_t ret;
    cap_t ut_cap;

    /* Since we are in boot code, we can do extensive error checking and
       return failure if anything unexpected happens. */

    /* Bounds check for size parameter */
    if (size_bits > 47 || size_bits < 4) {
        printf("Kernel init: Invalid untyped size %""lu""\n", size_bits);
        return false;
    }

    /* All cap ptrs must be aligned to object size */
    if (!(!((pptr) & ((1ul << (size_bits)) - 1ul)))) {
        printf("Kernel init: Unaligned untyped pptr %p (alignment %""lu"")\n", (void *)pptr, size_bits);
        return false;
    }

    /* All cap ptrs apart from device untypeds must be in the kernel window. */
    if (!device_memory && !pptr_in_kernel_window(pptr)) {
        printf("Kernel init: Non-device untyped pptr %p outside kernel window\n",
               (void *)pptr);
        return false;
    }

    /* Check that the end of the region is also in the kernel window, so we don't
       need to assume that the kernel window is aligned up to potentially
       seL4_MaxUntypedBits. */
    if (!device_memory && !pptr_in_kernel_window(pptr + ((1ul << (size_bits)) - 1ul))) {
        printf("Kernel init: End of non-device untyped at %p outside kernel window (size %""lu"")\n",
               (void *)pptr, size_bits);
        return false;
    }

    word_t i = ndks_boot.slot_pos_cur - first_untyped_slot;
    if (i < 230) {
        ndks_boot.bi_frame->untypedList[i] = (seL4_UntypedDesc) {
            .paddr = addrFromPPtr((void *)pptr),
            .sizeBits = size_bits,
            .isDevice = device_memory,
            .padding = {0}
        };
        ut_cap = cap_untyped_cap_new(((1ul << ((size_bits) - 4))),
                                     device_memory, size_bits, pptr);
        ret = provide_cap(root_cnode_cap, ut_cap);
    } else {
        printf("Kernel init: Too many untyped regions for boot info\n");
        ret = true;
    }
    return ret;
}

/**
 * Create untyped caps for a region of kernel-virtual memory.
 *
 * Takes care of alignment, size and potentially wrapping memory regions. It is fine to provide a
 * region with end < start if the memory is device memory.
 *
 * If the region start is not aligned to seL4_MinUntypedBits, the part up to the next aligned
 * address will be ignored and is lost, because it is too small to create kernel objects in.
 *
 * @param root_cnode_cap Cap to the CNode to store the untypeds in.
 * @param device_memory  Whether the region is device memory.
 * @param reg Region of kernel-virtual memory. May wrap around.
 * @param first_untyped_slot First available untyped boot info slot.
 * @return true on success, false on failure.
 */
__attribute__((__section__(".boot.text"))) static bool_t create_untypeds_for_region(
    cap_t root_cnode_cap,
    bool_t device_memory,
    region_t reg,
    seL4_SlotPos first_untyped_slot
)
{
    /* This code works with regions that wrap (where end < start), because the loop cuts up the
       region into size-aligned chunks, one for each cap. Memory chunks that are size-aligned cannot
       themselves overflow, so they satisfy alignment, size, and overflow conditions. The region
       [0..end) is not necessarily part of the kernel window (depending on the value of PPTR_BASE).
       This is fine for device untypeds. For normal untypeds, the region is assumed to be fully in
       the kernel window. This is not checked here. */
    while (!is_reg_empty(reg)) {

        /* Calculate the bit size of the region. This is also correct for end < start: it will
           return the correct size of the set [start..-1] union [0..end). This will then be too
           large for alignment, so the code further down will reduce the size. */
        unsigned int size_bits = (sizeof(seL4_Word) * 8) - 1 - clzl(reg.end - reg.start);
        /* The size can't exceed the largest possible untyped size. */
        if (size_bits > 47) {
            size_bits = 47;
        }
        /* The start address 0 satisfies any alignment needs, otherwise ensure
         * the region's bit size does not exceed the alignment of the region.
         */
        if (0 != reg.start) {
            unsigned int align_bits = ctzl(reg.start);
            if (size_bits > align_bits) {
                size_bits = align_bits;
            }
        }
        /* Provide an untyped capability for the region only if it is large
         * enough to be retyped into objects later. Otherwise the region can't
         * be used anyway.
         */
        if (size_bits >= 4) {
            if (!provide_untyped_cap(root_cnode_cap, device_memory, reg.start, size_bits, first_untyped_slot)) {
                return false;
            }
        }
        reg.start += (1ul << (size_bits));
    }
    return true;
}

__attribute__((__section__(".boot.text"))) bool_t create_untypeds(cap_t root_cnode_cap)
{
    seL4_SlotPos first_untyped_slot = ndks_boot.slot_pos_cur;

    paddr_t start = 0;
    for (word_t i = 0; i < ndks_boot.resv_count; i++) {
        if (start < ndks_boot.reserved[i].start) {
            region_t reg = paddr_to_pptr_reg((p_region_t) {
                start, ndks_boot.reserved[i].start
            });
            if (!create_untypeds_for_region(root_cnode_cap, true, reg, first_untyped_slot)) {
                printf("ERROR: creation of untypeds for device region #%u at"
                       " [%""lx""..%""lx"") failed\n",
                       (unsigned int)i, reg.start, reg.end);
                return false;
            }
        }

        start = ndks_boot.reserved[i].end;
    }

    if (start < 17592186044416) {
        region_t reg = paddr_to_pptr_reg((p_region_t) {
            start, 17592186044416
        });

        if (!create_untypeds_for_region(root_cnode_cap, true, reg, first_untyped_slot)) {
            printf("ERROR: creation of untypeds for top device region"
                   " [%""lx""..%""lx"") failed\n",
                   reg.start, reg.end);
            return false;
        }
    }

    /* There is a part of the kernel (code/data) that is only needed for the
     * boot process. We can create UT objects for these frames, so the memory
     * can be reused.
     */
    region_t boot_mem_reuse_reg = paddr_to_pptr_reg(get_p_reg_kernel_img_boot());
    if (!create_untypeds_for_region(root_cnode_cap, false, boot_mem_reuse_reg, first_untyped_slot)) {
        printf("ERROR: creation of untypeds for recycled boot memory"
               " [%""lx""..%""lx""] failed\n",
               boot_mem_reuse_reg.start, boot_mem_reuse_reg.end);
        return false;
    }

    /* convert remaining freemem into UT objects and provide the caps */
    for (word_t i = 0; i < (sizeof(ndks_boot.freemem) / sizeof((ndks_boot.freemem)[0])); i++) {
        region_t reg = ndks_boot.freemem[i];
        ndks_boot.freemem[i] = (region_t){ .start = 0, .end = 0 };
        if (!create_untypeds_for_region(root_cnode_cap, false, reg, first_untyped_slot)) {
            printf("ERROR: creation of untypeds for free memory region #%u at"
                   " [%""lx""..%""lx"") failed\n",
                   (unsigned int)i, reg.start, reg.end);
            return false;
        }
    }

    ndks_boot.bi_frame->untyped = (seL4_SlotRegion) {
        .start = first_untyped_slot,
        .end = ndks_boot.slot_pos_cur
    };

    return true;
}

__attribute__((__section__(".boot.text"))) void bi_finalise(void)
{

    if (rootserver.paging.start != rootserver.paging.end) {
        printf("WARNING: internal book keeping error. Less pagetables allocated than predicted: "
               "%ld page tables allocated but not used.\n", (rootserver.paging.end - rootserver.paging.start) >> 12);
    }

    ndks_boot.bi_frame->empty = (seL4_SlotRegion) {
        .start = ndks_boot.slot_pos_cur,
        .end = (1ul << (13))
    };
}

__attribute__((__section__(".boot.text"))) static inline pptr_t ceiling_kernel_window(pptr_t p)
{
    /* Adjust address if it exceeds the kernel window
     * Note that we compare physical address in case of overflow.
     */
    if (addrFromPPtr((void *)p) > (0xffffffffc0000000ul - (0xffffff8000000000ul - 0x0ul))) {
        p = 0xffffffffc0000000ul;
    }
    return p;
}

__attribute__((__section__(".boot.text"))) static bool_t check_available_memory(word_t n_available,
                                               const p_region_t *available)
{
    /* The system configuration is broken if no region is available. */
    if (0 == n_available) {
        printf("ERROR: no memory regions available\n");
        return false;
    }

    printf("available phys memory regions: %""lu""\n", n_available);
    /* Force ordering and exclusivity of available regions. */
    for (word_t i = 0; i < n_available; i++) {
        const p_region_t *r = &available[i];
        printf("  [%""lx""..%""lx"")\n", r->start, r->end);

        /* Available regions must be sane */
        if (r->start > r->end) {
            printf("ERROR: memory region %""lu"" has start > end\n", i);
            return false;
        }

        /* Available regions can't be empty. */
        if (r->start == r->end) {
            printf("ERROR: memory region %""lu"" empty\n", i);
            return false;
        }

        /* Regions must be ordered and must not overlap. Regions are [start..end),
           so the == case is fine. Directly adjacent regions are allowed. */
        if ((i > 0) && (r->start < available[i - 1].end)) {
            printf("ERROR: memory region %d in wrong order\n", (int)i);
            return false;
        }
    }

    return true;
}


__attribute__((__section__(".boot.text"))) static bool_t check_reserved_memory(word_t n_reserved,
                                              const region_t *reserved)
{
    printf("reserved virt address space regions: %""lu""\n",
           n_reserved);
    /* Force ordering and exclusivity of reserved regions. */
    for (word_t i = 0; i < n_reserved; i++) {
        const region_t *r = &reserved[i];
        printf("  [%""lx""..%""lx"")\n", r->start, r->end);

        /* Reserved regions must be sane, the size is allowed to be zero. */
        if (r->start > r->end) {
            printf("ERROR: reserved region %""lu"" has start > end\n", i);
            return false;
        }

        /* Regions must be ordered and must not overlap. Regions are [start..end),
           so the == case is fine. Directly adjacent regions are allowed. */
        if ((i > 0) && (r->start < reserved[i - 1].end)) {
            printf("ERROR: reserved region %""lu"" in wrong order\n", i);
            return false;
        }
    }

    return true;
}

/* we can't declare arrays on the stack, so this is space for
 * the function below to use. */
__attribute__((__section__(".boot.bss"))) static region_t avail_reg[((sizeof(avail_p_regs) / sizeof((avail_p_regs)[0])) + 0 + 1 + 1)];
/**
 * Dynamically initialise the available memory on the platform.
 * A region represents an area of memory.
 */
__attribute__((__section__(".boot.text"))) bool_t init_freemem(word_t n_available, const p_region_t *available,
                              word_t n_reserved, const region_t *reserved,
                              v_region_t it_v_reg, word_t extra_bi_size_bits)
{

    if (!check_available_memory(n_available, available)) {
        return false;
    }

    if (!check_reserved_memory(n_reserved, reserved)) {
        return false;
    }

    for (word_t i = 0; i < (sizeof(ndks_boot.freemem) / sizeof((ndks_boot.freemem)[0])); i++) {
        ndks_boot.freemem[i] = (region_t){ .start = 0, .end = 0 };
    }

    /* convert the available regions to pptrs */
    for (word_t i = 0; i < n_available; i++) {
        avail_reg[i] = paddr_to_pptr_reg(available[i]);
        avail_reg[i].end = ceiling_kernel_window(avail_reg[i].end);
        avail_reg[i].start = ceiling_kernel_window(avail_reg[i].start);
    }

    word_t a = 0;
    word_t r = 0;
    /* Now iterate through the available regions, removing any reserved regions. */
    while (a < n_available && r < n_reserved) {
        if (reserved[r].start == reserved[r].end) {
            /* reserved region is empty - skip it */
            r++;
        } else if (avail_reg[a].start >= avail_reg[a].end) {
            /* skip the entire region - it's empty now after trimming */
            a++;
        } else if (reserved[r].end <= avail_reg[a].start) {
            /* the reserved region is below the available region - skip it */
            reserve_region(pptr_to_paddr_reg(reserved[r]));
            r++;
        } else if (reserved[r].start >= avail_reg[a].end) {
            /* the reserved region is above the available region - take the whole thing */
            insert_region(avail_reg[a]);
            a++;
        } else {
            /* the reserved region overlaps with the available region */
            if (reserved[r].start <= avail_reg[a].start) {
                /* the region overlaps with the start of the available region.
                 * trim start of the available region */
                avail_reg[a].start = (((avail_reg[a].end)<(reserved[r].end))?(avail_reg[a].end):(reserved[r].end));
                /* do not increment reserved index here - there could be more overlapping regions */
            } else {
                do { if (!(reserved[r].start < avail_reg[a].end)) { _assert_fail("reserved[r].start < avail_reg[a].end", "/home/b/test/sel4test/kernel/src/kernel/boot.c", 990, __func__); } } while(0);
                /* take the first chunk of the available region and move
                 * the start to the end of the reserved region */
                region_t m = avail_reg[a];
                m.end = reserved[r].start;
                insert_region(m);
                if (avail_reg[a].end > reserved[r].end) {
                    avail_reg[a].start = reserved[r].end;
                    /* we could increment reserved index here, but it's more consistent with the
                     * other overlapping case if we don't */
                } else {
                    a++;
                }
            }
        }
    }

    for (; r < n_reserved; r++) {
        if (reserved[r].start < reserved[r].end) {
            reserve_region(pptr_to_paddr_reg(reserved[r]));
        }
    }

    /* no more reserved regions - add the rest */
    for (; a < n_available; a++) {
        if (avail_reg[a].start < avail_reg[a].end) {
            insert_region(avail_reg[a]);
        }
    }

    /* now try to fit the root server objects into a region */
    int i = (sizeof(ndks_boot.freemem) / sizeof((ndks_boot.freemem)[0])) - 1;
    if (!is_reg_empty(ndks_boot.freemem[i])) {
        printf("ERROR: insufficient MAX_NUM_FREEMEM_REG (%u)\n",
               (unsigned int)((sizeof(avail_p_regs) / sizeof((avail_p_regs)[0])) + 0 + 1 + 1));
        return false;
    }
    /* skip any empty regions */
    for (; i >= 0 && is_reg_empty(ndks_boot.freemem[i]); i--);

    /* try to grab the last available p region to create the root server objects
     * from. If possible, retain any left over memory as an extra p region */
    word_t size = calculate_rootserver_size(it_v_reg, extra_bi_size_bits);
    word_t max = rootserver_max_size_bits(extra_bi_size_bits);
    for (; i >= 0; i--) {
        /* Invariant: both i and (i + 1) are valid indices in ndks_boot.freemem. */
        do { if (!(i < (sizeof(ndks_boot.freemem) / sizeof((ndks_boot.freemem)[0])) - 1)) { _assert_fail("i < ARRAY_SIZE(ndks_boot.freemem) - 1", "/home/b/test/sel4test/kernel/src/kernel/boot.c", 1036, __func__); } } while(0);
        /* Invariant; the region at index i is the current candidate.
         * Invariant: regions 0 up to (i - 1), if any, are additional candidates.
         * Invariant: region (i + 1) is empty. */
        do { if (!(is_reg_empty(ndks_boot.freemem[i + 1]))) { _assert_fail("is_reg_empty(ndks_boot.freemem[i + 1])", "/home/b/test/sel4test/kernel/src/kernel/boot.c", 1040, __func__); } } while(0);
        /* Invariant: regions above (i + 1), if any, are empty or too small to use.
         * Invariant: all non-empty regions are ordered, disjoint and unallocated. */

        /* We make a fresh variable to index the known-empty region, because the
         * SimplExportAndRefine verification test has poor support for array
         * indices that are sums of variables and small constants. */
        int empty_index = i + 1;

        /* Try to take the top-most suitably sized and aligned chunk. */
        pptr_t unaligned_start = ndks_boot.freemem[i].end - size;
        pptr_t start = (((unaligned_start) >> (max)) << (max));
        /* if unaligned_start didn't underflow, and start fits in the region,
         * then we've found a region that fits the root server objects. */
        if (unaligned_start <= ndks_boot.freemem[i].end
            && start >= ndks_boot.freemem[i].start) {
            create_rootserver_objects(start, it_v_reg, extra_bi_size_bits);
            /* There may be leftovers before and after the memory we used. */
            /* Shuffle the after leftover up to the empty slot (i + 1). */
            ndks_boot.freemem[empty_index] = (region_t) {
                .start = start + size,
                .end = ndks_boot.freemem[i].end
            };
            /* Leave the before leftover in current slot i. */
            ndks_boot.freemem[i].end = start;
            /* Regions i and (i + 1) are now well defined, ordered, disjoint,
             * and unallocated, so we can return successfully. */
            return true;
        }
        /* Region i isn't big enough, so shuffle it up to slot (i + 1),
         * which we know is unused. */
        ndks_boot.freemem[empty_index] = ndks_boot.freemem[i];
        /* Now region i is unused, so make it empty to reestablish the invariant
         * for the next iteration (when it will be slot i + 1). */
        ndks_boot.freemem[i] = (region_t){ .start = 0, .end = 0 };
    }

    /* We didn't find a big enough region. */
    printf("ERROR: no free memory region is big enough for root server "
           "objects, need size/alignment of 2^%""lu""\n", max);
    return false;
}
# 1 "/home/b/test/sel4test/kernel/src/kernel/cspace.c"
/*
 * Copyright 2014, General Dynamics C4 Systems
 *
 * SPDX-License-Identifier: GPL-2.0-only
 */
# 15 "/home/b/test/sel4test/kernel/src/kernel/cspace.c"
lookupCap_ret_t lookupCap(tcb_t *thread, cptr_t cPtr)
{
    lookupSlot_raw_ret_t lu_ret;
    lookupCap_ret_t ret;

    lu_ret = lookupSlot(thread, cPtr);
    if (__builtin_expect(!!(lu_ret.status != EXCEPTION_NONE), 0)) {
        ret.status = lu_ret.status;
        ret.cap = cap_null_cap_new();
        return ret;
    }

    ret.status = EXCEPTION_NONE;
    ret.cap = lu_ret.slot->cap;
    return ret;
}

lookupCapAndSlot_ret_t lookupCapAndSlot(tcb_t *thread, cptr_t cPtr)
{
    lookupSlot_raw_ret_t lu_ret;
    lookupCapAndSlot_ret_t ret;

    lu_ret = lookupSlot(thread, cPtr);
    if (__builtin_expect(!!(lu_ret.status != EXCEPTION_NONE), 0)) {
        ret.status = lu_ret.status;
        ret.slot = ((void *)0);
        ret.cap = cap_null_cap_new();
        return ret;
    }

    ret.status = EXCEPTION_NONE;
    ret.slot = lu_ret.slot;
    ret.cap = lu_ret.slot->cap;
    return ret;
}

lookupSlot_raw_ret_t lookupSlot(tcb_t *thread, cptr_t capptr)
{
    cap_t threadRoot;
    resolveAddressBits_ret_t res_ret;
    lookupSlot_raw_ret_t ret;

    threadRoot = (((cte_t *)((word_t)(thread)&~((1ul << (12)) - 1ul)))+(tcbCTable))->cap;
    res_ret = resolveAddressBits(threadRoot, capptr, (1 << 6));

    ret.status = res_ret.status;
    ret.slot = res_ret.slot;
    return ret;
}

lookupSlot_ret_t lookupSlotForCNodeOp(bool_t isSource, cap_t root, cptr_t capptr,
                                      word_t depth)
{
    resolveAddressBits_ret_t res_ret;
    lookupSlot_ret_t ret;

    ret.slot = ((void *)0);

    if (__builtin_expect(!!(cap_get_capType(root) != cap_cnode_cap), 0)) {
        current_syscall_error.type = seL4_FailedLookup;
        current_syscall_error.failedLookupWasSource = isSource;
        current_lookup_fault = lookup_fault_invalid_root_new();
        ret.status = EXCEPTION_SYSCALL_ERROR;
        return ret;
    }

    if (__builtin_expect(!!(depth < 1 || depth > (1 << 6)), 0)) {
        current_syscall_error.type = seL4_RangeError;
        current_syscall_error.rangeErrorMin = 1;
        current_syscall_error.rangeErrorMax = (1 << 6);
        ret.status = EXCEPTION_SYSCALL_ERROR;
        return ret;
    }
    res_ret = resolveAddressBits(root, capptr, depth);
    if (__builtin_expect(!!(res_ret.status != EXCEPTION_NONE), 0)) {
        current_syscall_error.type = seL4_FailedLookup;
        current_syscall_error.failedLookupWasSource = isSource;
        /* current_lookup_fault will have been set by resolveAddressBits */
        ret.status = EXCEPTION_SYSCALL_ERROR;
        return ret;
    }

    if (__builtin_expect(!!(res_ret.bitsRemaining != 0), 0)) {
        current_syscall_error.type = seL4_FailedLookup;
        current_syscall_error.failedLookupWasSource = isSource;
        current_lookup_fault =
            lookup_fault_depth_mismatch_new(0, res_ret.bitsRemaining);
        ret.status = EXCEPTION_SYSCALL_ERROR;
        return ret;
    }

    ret.slot = res_ret.slot;
    ret.status = EXCEPTION_NONE;
    return ret;
}

lookupSlot_ret_t lookupSourceSlot(cap_t root, cptr_t capptr, word_t depth)
{
    return lookupSlotForCNodeOp(true, root, capptr, depth);
}

lookupSlot_ret_t lookupTargetSlot(cap_t root, cptr_t capptr, word_t depth)
{
    return lookupSlotForCNodeOp(false, root, capptr, depth);
}

lookupSlot_ret_t lookupPivotSlot(cap_t root, cptr_t capptr, word_t depth)
{
    return lookupSlotForCNodeOp(true, root, capptr, depth);
}

resolveAddressBits_ret_t resolveAddressBits(cap_t nodeCap, cptr_t capptr, word_t n_bits)
{
    resolveAddressBits_ret_t ret;
    word_t radixBits, guardBits, levelBits, guard;
    word_t capGuard, offset;
    cte_t *slot;

    ret.bitsRemaining = n_bits;
    ret.slot = ((void *)0);

    if (__builtin_expect(!!(cap_get_capType(nodeCap) != cap_cnode_cap), 0)) {
        current_lookup_fault = lookup_fault_invalid_root_new();
        ret.status = EXCEPTION_LOOKUP_FAULT;
        return ret;
    }

    while (1) {
        radixBits = cap_cnode_cap_get_capCNodeRadix(nodeCap);
        guardBits = cap_cnode_cap_get_capCNodeGuardSize(nodeCap);
        levelBits = radixBits + guardBits;

        /* Haskell error: "All CNodes must resolve bits" */
        do { if (!(levelBits != 0)) { _assert_fail("levelBits != 0", "/home/b/test/sel4test/kernel/src/kernel/cspace.c", 148, __func__); } } while(0);

        capGuard = cap_cnode_cap_get_capCNodeGuard(nodeCap);

        /* The MASK(wordRadix) here is to avoid the case where
         * n_bits = wordBits (=2^wordRadix) and guardBits = 0, as it violates
         * the C spec to shift right by more than wordBits-1.
         */
        guard = (capptr >> ((n_bits - guardBits) & ((1ul << (6)) - 1ul))) & ((1ul << (guardBits)) - 1ul);
        if (__builtin_expect(!!(guardBits > n_bits || guard != capGuard), 0)) {
            current_lookup_fault =
                lookup_fault_guard_mismatch_new(capGuard, n_bits, guardBits);
            ret.status = EXCEPTION_LOOKUP_FAULT;
            return ret;
        }

        if (__builtin_expect(!!(levelBits > n_bits), 0)) {
            current_lookup_fault =
                lookup_fault_depth_mismatch_new(levelBits, n_bits);
            ret.status = EXCEPTION_LOOKUP_FAULT;
            return ret;
        }

        offset = (capptr >> (n_bits - levelBits)) & ((1ul << (radixBits)) - 1ul);
        slot = ((cte_t *)(cap_cnode_cap_get_capCNodePtr(nodeCap))) + offset;

        if (__builtin_expect(!!(n_bits == levelBits), 1)) {
            ret.status = EXCEPTION_NONE;
            ret.slot = slot;
            ret.bitsRemaining = 0;
            return ret;
        }

        /** GHOSTUPD: "(\<acute>levelBits > 0, id)" */

        n_bits -= levelBits;
        nodeCap = slot->cap;

        if (__builtin_expect(!!(cap_get_capType(nodeCap) != cap_cnode_cap), 0)) {
            ret.status = EXCEPTION_NONE;
            ret.slot = slot;
            ret.bitsRemaining = n_bits;
            return ret;
        }
    }
}
# 1 "/home/b/test/sel4test/kernel/src/kernel/faulthandler.c"
/*
 * Copyright 2014, General Dynamics C4 Systems
 *
 * SPDX-License-Identifier: GPL-2.0-only
 */
# 54 "/home/b/test/sel4test/kernel/src/kernel/faulthandler.c"
void handleFault(tcb_t *tptr)
{
    exception_t status;
    seL4_Fault_t fault = current_fault;

    status = sendFaultIPC(tptr);
    if (status != EXCEPTION_NONE) {
        handleDoubleFault(tptr, fault);
    }
}

exception_t sendFaultIPC(tcb_t *tptr)
{
    cptr_t handlerCPtr;
    cap_t handlerCap;
    lookupCap_ret_t lu_ret;
    lookup_fault_t original_lookup_fault;

    original_lookup_fault = current_lookup_fault;

    handlerCPtr = tptr->tcbFaultHandler;
    lu_ret = lookupCap(tptr, handlerCPtr);
    if (lu_ret.status != EXCEPTION_NONE) {
        current_fault = seL4_Fault_CapFault_new(handlerCPtr, false);
        return EXCEPTION_FAULT;
    }
    handlerCap = lu_ret.cap;

    if (cap_get_capType(handlerCap) == cap_endpoint_cap &&
        cap_endpoint_cap_get_capCanSend(handlerCap) &&
        (cap_endpoint_cap_get_capCanGrant(handlerCap) ||
         cap_endpoint_cap_get_capCanGrantReply(handlerCap))) {
        tptr->tcbFault = current_fault;
        if (seL4_Fault_get_seL4_FaultType(current_fault) == seL4_Fault_CapFault) {
            tptr->tcbLookupFailure = original_lookup_fault;
        }
        sendIPC(true, true,
                cap_endpoint_cap_get_capEPBadge(handlerCap),
                cap_endpoint_cap_get_capCanGrant(handlerCap), true, tptr,
                ((endpoint_t *)(cap_endpoint_cap_get_capEPPtr(handlerCap))));

        return EXCEPTION_NONE;
    } else {
        current_fault = seL4_Fault_CapFault_new(handlerCPtr, false);
        current_lookup_fault = lookup_fault_missing_capability_new(0);

        return EXCEPTION_FAULT;
    }
}



static void print_fault(seL4_Fault_t f)
{
    switch (seL4_Fault_get_seL4_FaultType(f)) {
    case seL4_Fault_NullFault:
        printf("null fault");
        break;
    case seL4_Fault_CapFault:
        printf("cap fault in %s phase at address %p",
               seL4_Fault_CapFault_get_inReceivePhase(f) ? "receive" : "send",
               (void *)seL4_Fault_CapFault_get_address(f));
        break;
    case seL4_Fault_VMFault:
        printf("vm fault on %s at address %p with status %p",
               seL4_Fault_VMFault_get_instructionFault(f) ? "code" : "data",
               (void *)seL4_Fault_VMFault_get_address(f),
               (void *)seL4_Fault_VMFault_get_FSR(f));
        break;
    case seL4_Fault_UnknownSyscall:
        printf("unknown syscall %p",
               (void *)seL4_Fault_UnknownSyscall_get_syscallNumber(f));
        break;
    case seL4_Fault_UserException:
        printf("user exception %p code %p",
               (void *)seL4_Fault_UserException_get_number(f),
               (void *)seL4_Fault_UserException_get_code(f));
        break;





    default:
        printf("unknown fault");
        break;
    }
}





/* The second fault, ex2, is stored in the global current_fault */
void handleDoubleFault(tcb_t *tptr, seL4_Fault_t ex1)

{





    seL4_Fault_t ex2 = current_fault;
    printf("Caught ");
    print_fault(ex2);
    printf("\nwhile trying to handle:\n");
    print_fault(ex1);


    printf("\nin thread %p \"%s\" ", tptr, ((debug_tcb_t *)(((cte_t *)((word_t)(tptr)&~((1ul << (12)) - 1ul)))+(tcbCNodeEntries)))->tcbName);


    printf("at address %p\n", (void *)getRestartPC(tptr));
    printf("With stack:\n");
    Arch_userStackTrace(tptr);


    setThreadState(tptr, ThreadState_Inactive);
}
# 1 "/home/b/test/sel4test/kernel/src/kernel/stack.c"
/*
 * Copyright 2020, Data61, CSIRO (ABN 41 687 119 230)
 *
 * SPDX-License-Identifier: GPL-2.0-only
 */



__attribute__((externally_visible)) __attribute__((__aligned__(4096)))
char kernel_stack_alloc[1][(1ul << (12))];
# 1 "/home/b/test/sel4test/kernel/src/kernel/thread.c"
/*
 * Copyright 2014, General Dynamics C4 Systems
 *
 * SPDX-License-Identifier: GPL-2.0-only
 */
# 25 "/home/b/test/sel4test/kernel/src/kernel/thread.c"
static seL4_MessageInfo_t
transferCaps(seL4_MessageInfo_t info,
             endpoint_t *endpoint, tcb_t *receiver,
             word_t *receiveBuffer);

__attribute__((__section__(".boot.text"))) void configureIdleThread(tcb_t *tcb)
{
    tcb->tcbFlags = seL4_TCBFlag_fpuDisabled;
    Arch_configureIdleThread(tcb);
    setThreadState(tcb, ThreadState_IdleThreadState);
}

void activateThread(void)
{







    switch (thread_state_get_tsType(ksCurThread->tcbState)) {
    case ThreadState_Running:



        break;

    case ThreadState_Restart: {
        word_t pc;

        pc = getRestartPC(ksCurThread);
        setNextPC(ksCurThread, pc);
        setThreadState(ksCurThread, ThreadState_Running);
        break;
    }

    case ThreadState_IdleThreadState:
        Arch_activateIdleThread(ksCurThread);
        break;

    default:
        _fail("Current thread is blocked", "/home/b/test/sel4test/kernel/src/kernel/thread.c", 67, __func__);
    }
}

void suspend(tcb_t *target)
{
    cancelIPC(target);
    if (thread_state_get_tsType(target->tcbState) == ThreadState_Running) {
        /* whilst in the running state it is possible that restart pc of a thread is
         * incorrect. As we do not know what state this thread will transition to
         * after we make it inactive we update its restart pc so that the thread next
         * runs at the correct address whether it is restarted or moved directly to
         * running */
        updateRestartPC(target);
    }
    setThreadState(target, ThreadState_Inactive);
    tcbSchedDequeue(target);




}

void restart(tcb_t *target)
{
    if (isStopped(target)) {
        cancelIPC(target);
# 105 "/home/b/test/sel4test/kernel/src/kernel/thread.c"
        setupReplyMaster(target);
        setThreadState(target, ThreadState_Restart);
        tcbSchedEnqueue(target);
        possibleSwitchTo(target);

    }
}

void doIPCTransfer(tcb_t *sender, endpoint_t *endpoint, word_t badge,
                   bool_t grant, tcb_t *receiver)
{
    void *receiveBuffer, *sendBuffer;

    receiveBuffer = lookupIPCBuffer(true, receiver);

    if (__builtin_expect(!!(seL4_Fault_get_seL4_FaultType(sender->tcbFault) == seL4_Fault_NullFault), 1)) {
        sendBuffer = lookupIPCBuffer(false, sender);
        doNormalTransfer(sender, sendBuffer, endpoint, badge, grant,
                         receiver, receiveBuffer);
    } else {
        doFaultTransfer(badge, sender, receiver, receiveBuffer);
    }
}




void doReplyTransfer(tcb_t *sender, tcb_t *receiver, cte_t *slot, bool_t grant)

{
# 152 "/home/b/test/sel4test/kernel/src/kernel/thread.c"
    do { if (!(thread_state_get_tsType(receiver->tcbState) == ThreadState_BlockedOnReply)) { _assert_fail("thread_state_get_tsType(receiver->tcbState) == ThreadState_BlockedOnReply", "/home/b/test/sel4test/kernel/src/kernel/thread.c", 152, __func__); } } while(0)
                                      ;


    word_t fault_type = seL4_Fault_get_seL4_FaultType(receiver->tcbFault);
    if (__builtin_expect(!!(fault_type == seL4_Fault_NullFault), 1)) {
        doIPCTransfer(sender, ((void *)0), 0, grant, receiver);



        /** GHOSTUPD: "(True, gs_set_assn cteDeleteOne_'proc (ucast cap_reply_cap))" */
        cteDeleteOne(slot);
        setThreadState(receiver, ThreadState_Running);
        possibleSwitchTo(receiver);

    } else {

        /** GHOSTUPD: "(True, gs_set_assn cteDeleteOne_'proc (ucast cap_reply_cap))" */
        cteDeleteOne(slot);

        bool_t restart = handleFaultReply(receiver, sender);
        receiver->tcbFault = seL4_Fault_NullFault_new();
        if (restart) {
            setThreadState(receiver, ThreadState_Restart);

            possibleSwitchTo(receiver);

        } else {
            setThreadState(receiver, ThreadState_Inactive);
        }
    }
# 199 "/home/b/test/sel4test/kernel/src/kernel/thread.c"
}

void doNormalTransfer(tcb_t *sender, word_t *sendBuffer, endpoint_t *endpoint,
                      word_t badge, bool_t canGrant, tcb_t *receiver,
                      word_t *receiveBuffer)
{
    word_t msgTransferred;
    seL4_MessageInfo_t tag;
    exception_t status;

    tag = messageInfoFromWord(getRegister(sender, msgInfoRegister));

    if (canGrant) {
        status = lookupExtraCaps(sender, sendBuffer, tag);
        if (__builtin_expect(!!(status != EXCEPTION_NONE), 0)) {
            current_extra_caps.excaprefs[0] = ((void *)0);
        }
    } else {
        current_extra_caps.excaprefs[0] = ((void *)0);
    }

    msgTransferred = copyMRs(sender, sendBuffer, receiver, receiveBuffer,
                             seL4_MessageInfo_get_length(tag));

    tag = transferCaps(tag, endpoint, receiver, receiveBuffer);

    tag = seL4_MessageInfo_set_length(tag, msgTransferred);
    setRegister(receiver, msgInfoRegister, wordFromMessageInfo(tag));
    setRegister(receiver, badgeRegister, badge);
}

void doFaultTransfer(word_t badge, tcb_t *sender, tcb_t *receiver,
                     word_t *receiverIPCBuffer)
{
    word_t sent;
    seL4_MessageInfo_t msgInfo;

    sent = setMRs_fault(sender, receiver, receiverIPCBuffer);
    msgInfo = seL4_MessageInfo_new(
                  seL4_Fault_get_seL4_FaultType(sender->tcbFault), 0, 0, sent);
    setRegister(receiver, msgInfoRegister, wordFromMessageInfo(msgInfo));
    setRegister(receiver, badgeRegister, badge);
}

/* Like getReceiveSlots, this is specialised for single-cap transfer. */
static seL4_MessageInfo_t transferCaps(seL4_MessageInfo_t info,
                                       endpoint_t *endpoint, tcb_t *receiver,
                                       word_t *receiveBuffer)
{
    word_t i;
    cte_t *destSlot;

    info = seL4_MessageInfo_set_extraCaps(info, 0);
    info = seL4_MessageInfo_set_capsUnwrapped(info, 0);

    if (__builtin_expect(!!(!current_extra_caps.excaprefs[0] || !receiveBuffer), 1)) {
        return info;
    }

    destSlot = getReceiveSlots(receiver, receiveBuffer);

    for (i = 0; i < ((1ul<<(seL4_MsgExtraCapBits))-1) && current_extra_caps.excaprefs[i] != ((void *)0); i++) {
        cte_t *slot = current_extra_caps.excaprefs[i];
        cap_t cap = slot->cap;

        if (cap_get_capType(cap) == cap_endpoint_cap &&
            ((endpoint_t *)(cap_endpoint_cap_get_capEPPtr(cap))) == endpoint) {
            /* If this is a cap to the endpoint on which the message was sent,
             * only transfer the badge, not the cap. */
            setExtraBadge(receiveBuffer,
                          cap_endpoint_cap_get_capEPBadge(cap), i);

            info = seL4_MessageInfo_set_capsUnwrapped(info,
                                                      seL4_MessageInfo_get_capsUnwrapped(info) | (1 << i));

        } else {
            deriveCap_ret_t dc_ret;

            if (!destSlot) {
                break;
            }

            dc_ret = deriveCap(slot, cap);

            if (dc_ret.status != EXCEPTION_NONE) {
                break;
            }
            if (cap_get_capType(dc_ret.cap) == cap_null_cap) {
                break;
            }

            cteInsert(dc_ret.cap, slot, destSlot);

            destSlot = ((void *)0);
        }
    }

    return seL4_MessageInfo_set_extraCaps(info, i);
}

void doNBRecvFailedTransfer(tcb_t *thread)
{
    /* Set the badge register to 0 to indicate there was no message */
    setRegister(thread, badgeRegister, 0);
}

void prepareSetDomain(tcb_t *tptr, dom_t dom)
{

    if (ksCurDomain != dom) {
        /* Save FPU state now to avoid touching cross-domain state later */
        fpuRelease(tptr);
    }

}

static void prepareNextDomain(void)
{

    /* Save FPU state now to avoid touching cross-domain state later */
    switchLocalFpuOwner(((void *)0));

}

static void nextDomain(void)
{
    ksDomScheduleIdx++;
    if (ksDomScheduleIdx >= ksDomScheduleLength) {
        ksDomScheduleIdx = 0;
    }



    ksWorkUnitsCompleted = 0;
    ksCurDomain = ksDomSchedule[ksDomScheduleIdx].domain;



    ksDomainTime = ksDomSchedule[ksDomScheduleIdx].length;

}
# 364 "/home/b/test/sel4test/kernel/src/kernel/thread.c"
static void scheduleChooseNewThread(void)
{
    if (ksDomainTime == 0) {
        prepareNextDomain();
        nextDomain();
    }
    chooseThread();
}

void schedule(void)
{





    if (ksSchedulerAction != ((tcb_t*)0)) {
        bool_t was_runnable;
        if (isRunnable(ksCurThread)) {
            was_runnable = true;
            tcbSchedEnqueue(ksCurThread);
        } else {
            was_runnable = false;
        }

        if (ksSchedulerAction == ((tcb_t*) 1)) {
            scheduleChooseNewThread();
        } else {
            tcb_t *candidate = ksSchedulerAction;
            do { if (!(isRunnable(candidate))) { _assert_fail("isSchedulable(candidate)", "/home/b/test/sel4test/kernel/src/kernel/thread.c", 393, __func__); } } while(0);
            /* Avoid checking bitmap when ksCurThread is higher prio, to
             * match fast path.
             * Don't look at ksCurThread prio when it's idle, to respect
             * information flow in non-fastpath cases. */
            bool_t fastfail =
                ksCurThread == ksIdleThread
                || (candidate->tcbPriority < ksCurThread->tcbPriority);
            if (fastfail &&
                !isHighestPrio(ksCurDomain, candidate->tcbPriority)) {
                tcbSchedEnqueue(candidate);
                /* we can't, need to reschedule */
                ksSchedulerAction = ((tcb_t*) 1);
                scheduleChooseNewThread();
            } else if (was_runnable && candidate->tcbPriority == ksCurThread->tcbPriority) {
                /* We append the candidate at the end of the scheduling queue, that way the
                 * current thread, that was enqueued at the start of the scheduling queue
                 * will get picked during chooseNewThread */
                tcbSchedAppend(candidate);
                ksSchedulerAction = ((tcb_t*) 1);
                scheduleChooseNewThread();
            } else {
                do { if (!(candidate != ksCurThread)) { _assert_fail("candidate != NODE_STATE(ksCurThread)", "/home/b/test/sel4test/kernel/src/kernel/thread.c", 415, __func__); } } while(0);
                switchToThread(candidate);
            }
        }
    }
    ksSchedulerAction = ((tcb_t*)0);
# 434 "/home/b/test/sel4test/kernel/src/kernel/thread.c"
}

void chooseThread(void)
{
    word_t prio;
    word_t dom;
    tcb_t *thread;

    if (numDomains > 1) {
        dom = ksCurDomain;
    } else {
        dom = 0;
    }

    if (__builtin_expect(!!(ksReadyQueuesL1Bitmap[dom]), 1)) {
        prio = getHighestPrio(dom);
        thread = ksReadyQueues[ready_queues_index(dom, prio)].head;
        do { if (!(thread)) { _assert_fail("thread", "/home/b/test/sel4test/kernel/src/kernel/thread.c", 451, __func__); } } while(0);
        do { if (!(isRunnable(thread))) { _assert_fail("isSchedulable(thread)", "/home/b/test/sel4test/kernel/src/kernel/thread.c", 452, __func__); } } while(0);




        switchToThread(thread);
    } else {
        switchToIdleThread();
    }
}

void switchToThread(tcb_t *thread)
{
# 475 "/home/b/test/sel4test/kernel/src/kernel/thread.c"
    Arch_switchToThread(thread);


    lazyFPURestore(thread);


    tcbSchedDequeue(thread);
    ksCurThread = thread;
}

void switchToIdleThread(void)
{



    Arch_switchToIdleThread();
    ksCurThread = ksIdleThread;
}

void setDomain(tcb_t *tptr, dom_t dom)
{
    tcbSchedDequeue(tptr);
    tptr->tcbDomain = dom;
    if (isRunnable(tptr)) {
        tcbSchedEnqueue(tptr);
    }
    if (tptr == ksCurThread) {
        rescheduleRequired();
    }
}

void setMCPriority(tcb_t *tptr, prio_t mcp)
{
    tptr->tcbMCP = mcp;
}
# 540 "/home/b/test/sel4test/kernel/src/kernel/thread.c"
void setPriority(tcb_t *tptr, prio_t prio)
{
    tcbSchedDequeue(tptr);
    tptr->tcbPriority = prio;
    if (isRunnable(tptr)) {
        if (tptr == ksCurThread) {
            rescheduleRequired();
        } else {
            possibleSwitchTo(tptr);
        }
    }
}


/* Note that this thread will possibly continue at the end of this kernel
 * entry. Do not queue it yet, since a queue+unqueue operation is wasteful
 * if it will be picked. Instead, it waits in the 'ksSchedulerAction' site
 * on which the scheduler will take action. */
void possibleSwitchTo(tcb_t *target)
{



        if (ksCurDomain != target->tcbDomain
            ) {
            tcbSchedEnqueue(target);
        } else if (ksSchedulerAction != ((tcb_t*)0)) {
            /* Too many threads want special treatment, use regular queues. */
            rescheduleRequired();
            tcbSchedEnqueue(target);
        } else {
            ksSchedulerAction = target;
        }




}

void setThreadState(tcb_t *tptr, _thread_state_t ts)
{
    thread_state_ptr_set_tsType(&tptr->tcbState, ts);
    scheduleTCB(tptr);
}

void scheduleTCB(tcb_t *tptr)
{
    if (tptr == ksCurThread &&
        ksSchedulerAction == ((tcb_t*)0) &&
        !isRunnable(tptr)) {
        rescheduleRequired();
    }
}
# 676 "/home/b/test/sel4test/kernel/src/kernel/thread.c"
void timerTick(void)
{
    if (__builtin_expect(!!(thread_state_get_tsType(ksCurThread->tcbState) == ThreadState_Running), 1)





       ) {
        if (ksCurThread->tcbTimeSlice > 1) {
            ksCurThread->tcbTimeSlice--;
        } else {
            ksCurThread->tcbTimeSlice = 5;
            tcbSchedAppend(ksCurThread);
            rescheduleRequired();
        }
    }

    if (numDomains > 1) {
        ksDomainTime--;
        if (ksDomainTime == 0) {
            rescheduleRequired();
        }
    }
}


void rescheduleRequired(void)
{
    if (ksSchedulerAction != ((tcb_t*)0)
        && ksSchedulerAction != ((tcb_t*) 1)



       ) {




        tcbSchedEnqueue(ksSchedulerAction);
    }
    ksSchedulerAction = ((tcb_t*) 1);
}
# 1 "/home/b/test/sel4test/kernel/src/machine/capdl.c"
/*
 * Copyright 2020, Data61, CSIRO (ABN 41 687 119 230)
 *
 * SPDX-License-Identifier: GPL-2.0-only
 */







# 1 "/home/b/test/sel4test/kernel/include/machine/timer.h" 1
/*
 * Copyright 2020, Data61, CSIRO (ABN 41 687 119 230)
 *
 * SPDX-License-Identifier: GPL-2.0-only
 */

       
# 34 "/home/b/test/sel4test/kernel/include/machine/timer.h"
static inline void resetTimer(void);
# 14 "/home/b/test/sel4test/kernel/src/machine/capdl.c" 2
# 22 "/home/b/test/sel4test/kernel/src/machine/capdl.c"
/* seen list - check this array before we print cnode and vspace */
/* TBD: This is to avoid traversing the same cnode. It should be applied to object
 * as well since the extractor might comes across multiple caps to the same object.
 */
cap_t seen_list[256];
int watermark = 0;

void add_to_seen(cap_t c)
{
    /* Won't work well if there're more than SEEN_SZ cnode */
    if (watermark < 256) {
        seen_list[watermark] = c;
        watermark++;
    }
}

void reset_seen_list(void)
{
    memset(seen_list, 0, 256 * sizeof(seen_list[0]));
    watermark = 0;
}

bool_t seen(cap_t c)
{
    for (int i = 0; i < watermark; i++) {
        if (same_cap(seen_list[i], c)) {
            return true;
        }
    }
    return false;
}

bool_t same_cap(cap_t a, cap_t b)
{
    return (a.words[0] == b.words[0] && a.words[1] == b.words[1]);
}

/* Return true if strings are the same */
static inline bool_t strings_equal(const char *str1, const char *str2)
{
    while (*str1 && *str2 && (*str1 == *str2)) {
        str1++;
        str2++;
    }
    return !(*(const unsigned char *)str1 - * (const unsigned char *)str2);
}

/* Return true if the tcb is for rootserver or idle thread */
bool_t root_or_idle_tcb(tcb_t *tcb)
{
    return (strings_equal(((debug_tcb_t *)(((cte_t *)((word_t)(tcb)&~((1ul << (12)) - 1ul)))+(tcbCNodeEntries)))->tcbName, "rootserver")
            || strings_equal(((debug_tcb_t *)(((cte_t *)((word_t)(tcb)&~((1ul << (12)) - 1ul)))+(tcbCNodeEntries)))->tcbName, "idle_thread"));
}

/*
 * Print objects
 */



void obj_tcb_print_attrs(tcb_t *tcb)
{
    printf("(addr: 0x%lx, ip: 0x%lx, sp: 0x%lx, prio: %lu, max_prio: %lu",
           (long unsigned int)tcb->tcbIPCBuffer,
           (long unsigned int)getRestartPC(tcb),
           (long unsigned int)get_tcb_sp(tcb),
           (long unsigned int)tcb->tcbPriority,
           (long unsigned int)tcb->tcbMCP);





    /* init */
# 104 "/home/b/test/sel4test/kernel/src/machine/capdl.c"
    printf(", dom: %ld)\n", tcb->tcbDomain);
}
# 135 "/home/b/test/sel4test/kernel/src/machine/capdl.c"
void obj_ut_print_attrs(cte_t *slot, tcb_t *tcb)
{
    /* might have two untypeds with the same address but different size */
    printf("%p_%lu_untyped = ut (%lu bits, paddr: %p) {",
           (void *)cap_untyped_cap_get_capPtr(slot->cap),
           (long unsigned int)cap_untyped_cap_get_capBlockSize(slot->cap),
           (long unsigned int)cap_untyped_cap_get_capBlockSize(slot->cap),
           ((word_t *)(cap_untyped_cap_get_capPtr(slot->cap))));

    /* there is no need to check for a NullCap as NullCaps are
    always accompanied by null mdb pointers */
    for (cte_t *nextPtr = ((cte_t *)(mdb_node_get_mdbNext(slot->cteMDBNode)));
         nextPtr && isMDBParentOf(slot, nextPtr);
         nextPtr = ((cte_t *)(mdb_node_get_mdbNext(slot->cteMDBNode)))) {
        if (!sameRegionAs(slot->cap, nextPtr->cap)) {
            /* TBD:
             * - this will print out the attributes of the cap, which it shouldn't
             *
             * - might be a pathological case where an untyped has a child cap that
             *   isn't reachable from any of the non root threads. This would result
             *   in an object being mentioned but never properly defined
             */
            print_cap(nextPtr->cap);
        }
    }
    printf("}\n");
}

void obj_cnode_print_attrs(cap_t cnode)
{
    printf("(%lu bits)\n", (long unsigned int)cap_cnode_cap_get_capCNodeRadix(cnode));
}

void obj_tcb_print_cnodes(cap_t cnode, tcb_t *tcb)
{
    if (seen(cnode)) {
        return;
    }
    add_to_seen(cnode);
    printf("%p_cnode = cnode ", (void *)cap_cnode_cap_get_capCNodePtr(cnode));
    obj_cnode_print_attrs(cnode);
    word_t radix = cap_cnode_cap_get_capCNodeRadix(cnode);

    for (uint32_t i = 0; i < (1 << radix); i++) {
        lookupCapAndSlot_ret_t c = lookupCapAndSlot(tcb, i);
        if (cap_get_capType(c.cap) == cap_untyped_cap) {
            /* we need `cte_t *` to print out the slots of an untyped object */
            obj_ut_print_attrs(c.slot, tcb);

        } else if (cap_get_capType(c.cap) == cap_cnode_cap) {
            /* TBD: deal with nested cnodes */

        } else if (cap_get_capType(c.cap) != cap_null_cap) {
            print_object(c.cap);
        }
    }
}

/*
 * Caps
 */

void cap_cnode_print_attrs(cap_t cnode)
{
    printf("(guard: %lu, guard_size: %lu)\n",
           (long unsigned int)cap_cnode_cap_get_capCNodeGuard(cnode),
           (long unsigned int)cap_cnode_cap_get_capCNodeGuardSize(cnode));
}

void cap_ep_print_attrs(cap_t ep)
{
    printf("(");
    cap_endpoint_cap_get_capCanReceive(ep) ? putchar('R') : 0;
    cap_endpoint_cap_get_capCanSend(ep) ? putchar('W') : 0;
    cap_endpoint_cap_get_capCanGrant(ep) ? putchar('G') : 0;
    cap_endpoint_cap_get_capCanGrantReply(ep) ? putchar('P') : 0;
    long unsigned int badge = cap_endpoint_cap_get_capEPBadge(ep);
    badge ? printf(", badge: %lu)\n", badge) : printf(")\n");
}

void cap_ntfn_print_attrs(cap_t ntfn)
{
    printf("(");
    cap_notification_cap_get_capNtfnCanReceive(ntfn) ? putchar('R') : 0;
    cap_notification_cap_get_capNtfnCanSend(ntfn) ? putchar('W') : 0;
    long unsigned int badge = cap_notification_cap_get_capNtfnBadge(ntfn);
    badge ? printf(", badge: %lu)\n", badge) : printf(")\n");
}

/*
 * print object slots
 */

void obj_tcb_print_slots(tcb_t *tcb)
{
    printf("%p_tcb {\n", tcb);

    /* CSpace root */
    if (cap_get_capType((((cte_t *)((word_t)(tcb)&~((1ul << (12)) - 1ul)))+(tcbCTable))->cap) != cap_null_cap) {
        printf("cspace: %p_cnode ",
               (void *)cap_cnode_cap_get_capCNodePtr((((cte_t *)((word_t)(tcb)&~((1ul << (12)) - 1ul)))+(tcbCTable))->cap));
        cap_cnode_print_attrs((((cte_t *)((word_t)(tcb)&~((1ul << (12)) - 1ul)))+(tcbCTable))->cap);
    }

    /* VSpace root */
    if (cap_get_capType((((cte_t *)((word_t)(tcb)&~((1ul << (12)) - 1ul)))+(tcbVTable))->cap) != cap_null_cap) {
        printf("vspace: %p_pd\n",
               ((vspace_root_t *)(cap_vspace_cap_get_capVSBasePtr((((cte_t *)((word_t)(tcb)&~((1ul << (12)) - 1ul)))+(tcbVTable))->cap))));

    }

    /* IPC buffer cap slot */
    if (cap_get_capType((((cte_t *)((word_t)(tcb)&~((1ul << (12)) - 1ul)))+(tcbBuffer))->cap) != cap_null_cap) {
        /* TBD: print out the bound vcpu */
        print_ipc_buffer_slot(tcb);
    }
# 274 "/home/b/test/sel4test/kernel/src/machine/capdl.c"
    /* Reply cap slot */
    if (cap_get_capType((((cte_t *)((word_t)(tcb)&~((1ul << (12)) - 1ul)))+(tcbReply))->cap) != cap_null_cap) {
        printf("reply_slot: %p_reply\n",
               (void *)cap_reply_cap_get_capTCBPtr((((cte_t *)((word_t)(tcb)&~((1ul << (12)) - 1ul)))+(tcbReply))->cap));
    }

    /* TCB of most recent IPC sender */
    if (cap_get_capType((((cte_t *)((word_t)(tcb)&~((1ul << (12)) - 1ul)))+(tcbCaller))->cap) != cap_null_cap) {
        tcb_t *caller = ((tcb_t *)(cap_thread_cap_get_capTCBPtr((((cte_t *)((word_t)(tcb)&~((1ul << (12)) - 1ul)))+(tcbCaller))->cap)));
        printf("caller_slot: %p_tcb\n", caller);
    }

    printf("}\n");
}

/* TBD: deal with nested cnodes */
void obj_cnode_print_slots(tcb_t *tcb)
{
    cap_t root = (((cte_t *)((word_t)(tcb)&~((1ul << (12)) - 1ul)))+(tcbCTable))->cap;
    if (cap_get_capType(root) != cap_cnode_cap) {
        return;
    }

    word_t radix = cap_cnode_cap_get_capCNodeRadix(root);
    if (seen(root)) {
        return;
    }
    add_to_seen(root);

    printf("%p_cnode {\n", (void *)cap_cnode_cap_get_capCNodePtr(root));

    for (uint32_t i = 0; i < (1 << radix); i++) {
        lookupCapAndSlot_ret_t c = lookupCapAndSlot(tcb, i);
        if (cap_get_capType(c.cap) != cap_null_cap) {
            printf("0x%x: ", i);
            print_cap(c.cap);
        }
    }
    printf("}\n");

    for (uint32_t i = 0; i < (1 << radix); i++) {
        lookupCapAndSlot_ret_t c = lookupCapAndSlot(tcb, i);
        if (cap_get_capType(c.cap) == cap_irq_handler_cap) {
            /* TBD: should instead print it from IRQNode */
            obj_irq_print_slots(c.cap);
        }
    }
}

void obj_irq_print_maps(void)
{
    printf("irq maps {\n");

    for (seL4_Word target = 0; target < 1; target++) {
        for (unsigned i = 0; i <= maxIRQ; i++) {
            irq_t irq = (i);
            if (isIRQActive(irq)) {
                cap_t cap = intStateIRQNode[(irq)].cap;
                if (cap_get_capType(cap) != cap_null_cap) {
                    printf("%d: 0x%lx_%lu_irq\n",
                           i,



                           (long unsigned int)irq,

                           (long unsigned int)target);
                }
            }
        }
    }
    printf("}\n");
}

void obj_irq_print_slots(cap_t irq_cap)
{
    irq_t irq = (cap_irq_handler_cap_get_capIRQ(irq_cap));
    if (isIRQActive(irq)) {
        printf("0x%lx_%lu_irq {\n",



               (long unsigned int)irq,

               (long unsigned int)0);
        cap_t ntfn_cap = intStateIRQNode[(irq)].cap;
        if (cap_get_capType(ntfn_cap) != cap_null_cap) {
            printf("0x0: ");
            print_cap(ntfn_cap);
        }
        printf("}\n");
    }
}

void print_objects(void)
{
    for (tcb_t *curr = ksDebugTCBs; curr != ((void *)0); curr = ((debug_tcb_t *)(((cte_t *)((word_t)(curr)&~((1ul << (12)) - 1ul)))+(tcbCNodeEntries)))->tcbDebugNext) {
        if (root_or_idle_tcb(curr)) {
            continue;
        }
        /* print the contains of the tcb's vtable as objects */
        obj_tcb_print_vtable(curr);
    }

    for (tcb_t *curr = ksDebugTCBs; curr != ((void *)0); curr = ((debug_tcb_t *)(((cte_t *)((word_t)(curr)&~((1ul << (12)) - 1ul)))+(tcbCNodeEntries)))->tcbDebugNext) {
        if (root_or_idle_tcb(curr)) {
            continue;
        }

        /* print the tcb as objects */
        printf("%p_tcb = tcb ", curr);
        obj_tcb_print_attrs(curr);

        /* print the contains of the tcb's ctable as objects */
        if (cap_get_capType((((cte_t *)((word_t)(curr)&~((1ul << (12)) - 1ul)))+(tcbCTable))->cap) == cap_cnode_cap) {
            obj_tcb_print_cnodes((((cte_t *)((word_t)(curr)&~((1ul << (12)) - 1ul)))+(tcbCTable))->cap, curr);
        }
    }
}

void print_caps(void)
{
    for (tcb_t *curr = ksDebugTCBs; curr != ((void *)0); curr = ((debug_tcb_t *)(((cte_t *)((word_t)(curr)&~((1ul << (12)) - 1ul)))+(tcbCNodeEntries)))->tcbDebugNext) {
        if (root_or_idle_tcb(curr)) {
            continue;
        }
        obj_cnode_print_slots(curr);
        obj_vtable_print_slots(curr);
        obj_tcb_print_slots(curr);
    }
}

void print_cap(cap_t cap)
{
    switch (cap_get_capType(cap)) {
    case cap_endpoint_cap: {
        printf("%p_ep ",
               (void *)cap_endpoint_cap_get_capEPPtr(cap));
        cap_ep_print_attrs(cap);
        break;
    }
    case cap_notification_cap: {
        printf("%p_notification ",
               (void *)cap_notification_cap_get_capNtfnPtr(cap));
        cap_ntfn_print_attrs(cap);
        break;
    }
    case cap_untyped_cap: {
        printf("%p_untyped\n",
               (void *)cap_untyped_cap_get_capPtr(cap));
        break;
    }
    case cap_thread_cap: {
        printf("%p_tcb\n",
               (void *)cap_thread_cap_get_capTCBPtr(cap));
        break;
    }
    case cap_cnode_cap: {
        printf("%p_cnode ",
               (void *)cap_cnode_cap_get_capCNodePtr(cap));
        cap_cnode_print_attrs(cap);
        break;
    }
# 454 "/home/b/test/sel4test/kernel/src/machine/capdl.c"
    case cap_irq_control_cap: {
        printf("irq_control\n"); /* only one in the system */
        break;
    }
    case cap_irq_handler_cap: {
        printf("%p_%lu_irq\n",
               (void *)cap_irq_handler_cap_get_capIRQ(cap),
               (long unsigned int)0);
        break;
    }
    default: {
        print_cap_arch(cap);
        break;
    }
    }
}

void print_object(cap_t cap)
{
    switch (cap_get_capType(cap)) {
    case cap_endpoint_cap: {
        printf("%p_ep = ep\n",
               (void *)cap_endpoint_cap_get_capEPPtr(cap));
        break;
    }
    case cap_notification_cap: {
        printf("%p_notification = notification\n",
               (void *)cap_notification_cap_get_capNtfnPtr(cap));
        break;
    }
    case cap_thread_cap: {
        /* this object has already got handle by `print_objects` */
        break;
    }
    case cap_cnode_cap: {
        do { if (!(!"should not happend")) { _assert_fail("!\"should not happend\"", "/home/b/test/sel4test/kernel/src/machine/capdl.c", 489, __func__); } } while(0);
    }
# 504 "/home/b/test/sel4test/kernel/src/machine/capdl.c"
    case cap_irq_handler_cap: {
        printf("%p_%lu_irq = irq\n",
               (void *)cap_irq_handler_cap_get_capIRQ(cap),
               (long unsigned int)0);
        break;
    }
    default:
        print_object_arch(cap);
        break;
    }
}
# 1 "/home/b/test/sel4test/kernel/src/machine/fpu.c"
/*
 * Copyright 2020, Data61, CSIRO (ABN 41 687 119 230)
 *
 * SPDX-License-Identifier: GPL-2.0-only
 */
# 14 "/home/b/test/sel4test/kernel/src/machine/fpu.c"
/* Switch the owner of the FPU to the given thread on local core. */
void switchLocalFpuOwner(tcb_t *new_owner)
{
    enableFpu();
    if (ksCurFPUOwner) {
        saveFpuState(ksCurFPUOwner);
    }
    if (new_owner) {
        loadFpuState(new_owner);
    } else {
        disableFpu();
    }
    ksCurFPUOwner = new_owner;
}

void switchFpuOwner(tcb_t *new_owner, word_t cpu)
{





    {
        switchLocalFpuOwner(new_owner);
    }
}

/* Prepare for the deletion of the given thread. */
void fpuRelease(tcb_t *thread)
{
    /* If the thread being deleted currently owns the FPU, switch away from it
     * so that 'ksCurFPUOwner' doesn't point to invalid memory. */
    if (nativeThreadUsingFPU(thread)) {
        switchFpuOwner(((void *)0), 0);
    }
}
# 1 "/home/b/test/sel4test/kernel/src/machine/io.c"
/*
 * Copyright 2014, General Dynamics C4 Systems
 *
 * SPDX-License-Identifier: GPL-2.0-only
 *
 * Portions derived from musl:
 *
 * Copyright (c) 2005-2020 Rich Felker, et al.
 *
 * SPDX-License-Identifier: MIT
 */
# 21 "/home/b/test/sel4test/kernel/src/machine/io.c"
/*
 *------------------------------------------------------------------------------
 * printf() core output channel management
 *------------------------------------------------------------------------------
 */

typedef struct _out_wrap_t out_wrap_t;

/* handler defining how/where to actually output a buffer */
typedef void (*out_write_fn)(out_wrap_t *out, const char *buf, word_t len);

struct _out_wrap_t {
    const out_write_fn write;
    char *const buf;
    const word_t maxlen;
    word_t used;
};

/* printf_core() and its helpers call this to actually output something. The
 * parameter 'out_wrap' cam be NULL, e.g. when printf_core() is just caller to
 * validate the format string. In this case we do nothing.
 */
static void out(out_wrap_t *out_wrap, const char *buf, word_t len)
{
    if (out_wrap) {
        out_wrap->write(out_wrap, buf, len);
    }
}

/* An out_write_fn implementation to print the characters via putchar(). It is
 * guaranteed here that 'out' is not NULL. The current implementation also never
 * passes NULL for 'buf'. */
static void do_output_to_putchar(
    __attribute__((unused)) out_wrap_t *out,
    const char *buf,
    word_t len)
{
    if (buf) {
        while (len-- > 0) {
            putchar(*buf++);
        }
    }
}

/* An out_write_fn implementation to copy the buffer into the out buffer. It is
 * guaranteed here that 'out' is not NULL. The current implementation also never
 * passes NULL for 'buf'. */
static void do_output_to_buffer(
    out_wrap_t *out,
    const char *buf,
    word_t len)
{
    /* It's guaranteed here that 'out' is not NULL. The current implementation
     * also never passes NULL for 'buf'. */
    if (buf && (out->used < out->maxlen)) {
        /* there is still space in the buffer*/
        word_t free = out->maxlen - out->used;
        if (len > free) {
            len = free;
        }
        memcpy(&out->buf[out->used], buf, len);
        out->used += len;
    }
}

/*
 *------------------------------------------------------------------------------
 * printf() core implementation
 *------------------------------------------------------------------------------
 */

static inline bool_t isdigit(char c)
{
    return c >= '0' &&
           c <= '9';
}

/* Convenient bit representation for modifier flags, which all fall within 31
 * codepoints of the space character.
 */
# 112 "/home/b/test/sel4test/kernel/src/machine/io.c"
/* State machine to accept length modifiers + conversion specifiers.
 * Result is 0 on failure, or an argument type to pop on success.
 */

enum {
    BARE, LPRE, LLPRE, HPRE, HHPRE, BIGLPRE,
    ZTPRE, JPRE,
    STOP,
    PTR, INT, UINT, ULLONG,
    LONG, ULONG,
    SHORT, USHORT, CHAR, UCHAR,
    WORDT, LLONG,




    NOARG,
    MAXSTATE
};



static const unsigned char states[]['z' - 'A' + 1] = {
    { /* 0: bare types */
        [('d')-'A'] = INT, [('i')-'A'] = INT,
        [('o')-'A'] = UINT, [('u')-'A'] = UINT, [('x')-'A'] = UINT, [('X')-'A'] = UINT,
        [('c')-'A'] = CHAR,
        [('s')-'A'] = PTR, [('p')-'A'] = ULONG, [('n')-'A'] = PTR,
        [('l')-'A'] = LPRE, [('h')-'A'] = HPRE,
        [('z')-'A'] = ZTPRE, [('j')-'A'] = JPRE, [('t')-'A'] = ZTPRE,
    }, { /* 1: l-prefixed */
        [('d')-'A'] = LONG, [('i')-'A'] = LONG,
        [('o')-'A'] = ULONG, [('u')-'A'] = ULONG, [('x')-'A'] = ULONG, [('X')-'A'] = ULONG,
        [('n')-'A'] = PTR,
        [('l')-'A'] = LLPRE,
    }, { /* 2: ll-prefixed */
        [('d')-'A'] = LLONG, [('i')-'A'] = LLONG,
        [('o')-'A'] = ULLONG, [('u')-'A'] = ULLONG,
        [('x')-'A'] = ULLONG, [('X')-'A'] = ULLONG,
        [('n')-'A'] = PTR,
    }, { /* 3: h-prefixed */
        [('d')-'A'] = SHORT, [('i')-'A'] = SHORT,
        [('o')-'A'] = USHORT, [('u')-'A'] = USHORT,
        [('x')-'A'] = USHORT, [('X')-'A'] = USHORT,
        [('n')-'A'] = PTR,
        [('h')-'A'] = HHPRE,
    }, { /* 4: hh-prefixed */
        [('d')-'A'] = CHAR, [('i')-'A'] = CHAR,
        [('o')-'A'] = UCHAR, [('u')-'A'] = UCHAR,
        [('x')-'A'] = UCHAR, [('X')-'A'] = UCHAR,
        [('n')-'A'] = PTR,
    }, { /* 5: L-prefixed not supported */
    }, { /* 6: z- or t-prefixed (assumed to be same size) */
        [('d')-'A'] = LONG, [('i')-'A'] = LONG,
        [('o')-'A'] = WORDT, [('u')-'A'] = WORDT,
        [('x')-'A'] = WORDT, [('X')-'A'] = WORDT,
        [('n')-'A'] = PTR,
    }, { /* 7: j-prefixed */
        [('d')-'A'] = LLONG, [('i')-'A'] = LLONG,
        [('o')-'A'] = ULLONG, [('u')-'A'] = ULLONG,
        [('x')-'A'] = ULLONG, [('X')-'A'] = ULLONG,
        [('n')-'A'] = PTR,
    }
};




union arg {
    uintmax_t i;
    long double f;
    void *p;
};

static void pop_arg(union arg *arg, int type, va_list *ap)
{
    switch (type) {
    case PTR:
        arg->p = __builtin_va_arg(*ap,void *);
        break;
    case INT:
        arg->i = __builtin_va_arg(*ap,int);
        break;
    case UINT:
        arg->i = __builtin_va_arg(*ap,unsigned int);
        break;
    case LONG:
        arg->i = __builtin_va_arg(*ap,long);
        break;
    case ULONG:
        arg->i = __builtin_va_arg(*ap,unsigned long);
        break;
    case LLONG:
        arg->i = __builtin_va_arg(*ap,long long);
        break;
    case ULLONG:
        arg->i = __builtin_va_arg(*ap,unsigned long long);
        break;
    case SHORT:
        arg->i = (short)__builtin_va_arg(*ap,int);
        break;
    case USHORT:
        arg->i = (unsigned short)__builtin_va_arg(*ap,int);
        break;
    case CHAR:
        arg->i = (signed char)__builtin_va_arg(*ap,int);
        break;
    case UCHAR:
        arg->i = (unsigned char)__builtin_va_arg(*ap,int);
        break;
    case WORDT:
        arg->i = __builtin_va_arg(*ap,word_t);
    }
}


static void pad(out_wrap_t *f, char c, int w, int l, int fl)
{
    char pad[32]; /* good enough for what the kernel prints */
    if (fl & ((1U<<('-'-' ')) | (1U<<('0'-' '))) || l >= w) {
        return;
    }
    l = w - l;
    memset(pad, c, l > sizeof(pad) ? sizeof(pad) : l);
    for (; l >= sizeof(pad); l -= sizeof(pad)) {
        out(f, pad, sizeof(pad));
    }
    out(f, pad, l);
}

static const char xdigits[16] = {
    "0123456789ABCDEF"
};

static char *fmt_x(uintmax_t x, char *s, int lower)
{
    for (; x; x >>= 4) {
        *--s = xdigits[(x & 15)] | lower;
    }
    return s;
}

static char *fmt_o(uintmax_t x, char *s)
{
    for (; x; x >>= 3) {
        *--s = '0' + (x & 7);
    }
    return s;
}

static char *fmt_u(uintmax_t x, char *s)
{
    while (0 != x) {
# 287 "/home/b/test/sel4test/kernel/src/machine/io.c"
        uintmax_t q = x / 10;
        unsigned int rem = x % 10;

        *--s = '0' + rem;
        x = q;
    }
    return s;
}

/* Maximum buffer size taken to ensure correct adaptation. However, it could be
 * reduced/removed if we could measure the buf length under all code paths
 */




static int getint(char **s)
{
    int i;
    for (i = 0; isdigit(**s); (*s)++) {
        if (i > (0xFFFFFFFFFFFFFFFF) / 10U || (**s - '0') > (0xFFFFFFFFFFFFFFFF) - 10 * i) {
            i = -1;
        } else {
            i = 10 * i + (**s - '0');
        }
    }
    return i;
}

static int printf_core(out_wrap_t *f, const char *fmt, va_list *ap, union arg *nl_arg, int *nl_type)
{
    char *a, *z, *s = (char *)fmt;
    unsigned l10n = 0, fl;
    int w, p, xp;
    union arg arg;
    int argpos;
    unsigned st, ps;
    int cnt = 0, l = 0;
    char buf[sizeof(uintmax_t) * 3 + 3 + 113 / 4];
    const char *prefix;
    int t, pl;

    for (;;) {
        if (l > (0xFFFFFFFFFFFFFFFF) - cnt) {
            /* This error is only specified for snprintf, for other function
             * from the printf() family the behavior is unspecified. Stopping
             * immediately here also seems sane, otherwise %n could produce
             * wrong results.
             */
            return -1; /* overflow */
        }

        /* Update output count, end loop when fmt is exhausted */
        cnt += l;
        if (!*s) {
            break;
        }

        /* Handle literal text and %% format specifiers */
        for (a = s; *s && *s != '%'; s++);
        for (z = s; s[0] == '%' && s[1] == '%'; z++, s += 2);
        if (z - a > (0xFFFFFFFFFFFFFFFF) - cnt) {
            return -1; /* overflow */
        }
        l = z - a;
        out(f, a, l);
        if (l) {
            continue;
        }

        if (isdigit(s[1]) && s[2] == '$') {
            l10n = 1;
            argpos = (s[1] - '0');
            s += 3;
        } else {
            argpos = -1;
            s++;
        }

        /* Read modifier flags */
        for (fl = 0; (unsigned)*s - ' ' < 32 && (((1U<<('#'-' '))|(1U<<('0'-' '))|(1U<<('-'-' '))|(1U<<(' '-' '))|(1U<<('+'-' '))|(1U<<('\''-' '))) & (1U<<( *s -' '))); s++) {
            fl |= (1U<<( *s -' '));
        }

        /* Read field width */
        if (*s == '*') {
            if (isdigit(s[1]) && s[2] == '$') {
                l10n = 1;
                nl_type[(s[1] - '0')] = INT;
                w = nl_arg[(s[1] - '0')].i;
                s += 3;
            } else if (!l10n) {
                w = f ? __builtin_va_arg(*ap,int) : 0;
                s++;
            } else {
                return -1; /* invalid */
            }
            if (w < 0) {
                fl |= (1U<<('-'-' '));
                w = -w;
            }
        } else if ((w = getint(&s)) < 0) {
            return -1; /* overflow */
        }

        /* Read precision */
        if (*s == '.' && s[1] == '*') {
            if (isdigit(s[2]) && s[3] == '$') {
                nl_type[(s[2] - '0')] = INT;
                p = nl_arg[(s[2] - '0')].i;
                s += 4;
            } else if (!l10n) {
                p = f ? __builtin_va_arg(*ap,int) : 0;
                s += 2;
            } else {
                return -1;/* invalid */
            }
            xp = (p >= 0);
        } else if (*s == '.') {
            s++;
            p = getint(&s);
            xp = 1;
        } else {
            p = -1;
            xp = 0;
        }

        /* Format specifier state machine */
        st = 0;
        do {
            if (((unsigned)(*s)-'A' > 'z'-'A')) {
                return -1; /* invalid */
            }
            ps = st;
            st = states[st][(*s++)-'A'];
        } while (st - 1 < STOP);
        if (!st) {
            return -1; /* invalid */
        }

        /* Check validity of argument type (nl/normal) */
        if (st == NOARG) {
            if (argpos >= 0) {
                return -1; /* invalid */
            }
        } else {
            if (argpos >= 0) {
                nl_type[argpos] = st;
                arg = nl_arg[argpos];
            } else if (f) {
                pop_arg(&arg, st, ap);
            } else {
                return 0;
            }
        }

        if (!f) {
            continue;
        }

        z = buf + sizeof(buf);
        prefix = "-+   0X0x";
        pl = 0;
        t = s[-1];

        /* - and 0 flags are mutually exclusive */
        if (fl & (1U<<('-'-' '))) {
            fl &= ~(1U<<('0'-' '));
        }

        if (t == 'n') {
            if (!arg.p) {
                continue;
            }
            switch (ps) {
            case BARE:
                *(int *)arg.p = cnt;
                break;
            case LPRE:
                *(long *)arg.p = cnt;
                break;
            case LLPRE:
                *(long long *)arg.p = cnt;
                break;
            case HPRE:
                *(unsigned short *)arg.p = cnt;
                break;
            case HHPRE:
                *(unsigned char *)arg.p = cnt;
                break;
            case ZTPRE:
                *(word_t *)arg.p = cnt;
                break;
            case JPRE:
                *(word_t *)arg.p = cnt;
                break;
            }
            continue;
        } else if (t == 'c') {
            p = 1;
            a = z - p;
            *a = arg.i;
            fl &= ~(1U<<('0'-' '));
        } else if (t == 's') {
            a = arg.p ? arg.p : "(null)";
            z = a + strnlen(a, p < 0 ? (0xFFFFFFFFFFFFFFFF) : p);
            if (p < 0 && *z) {
                return -1; /* overflow */
            }
            p = z - a;
            fl &= ~(1U<<('0'-' '));
        } else {
            switch (t) {
            case 'p':
                p = (((p)>(2 * sizeof(void *)))?(p):(2 * sizeof(void *)));
                t = 'x';
                fl |= (1U<<('#'-' '));
            case 'x':
            case 'X':
                a = fmt_x(arg.i, z, t & 32);
                if (arg.i && (fl & (1U<<('#'-' ')))) {
                    prefix += (t >> 4);
                    pl = 2;
                }
                break;
            case 'o':
                a = fmt_o(arg.i, z);
                if ((fl & (1U<<('#'-' '))) && p < (z - a + 1)) {
                    p = z - a + 1;
                }
                break;
            case 'd':
            case 'i':
                pl = 1;
                if (arg.i > (0xFFFFFFFFFFFFFFFF)) {
                    arg.i = -arg.i;
                } else if (fl & (1U<<('+'-' '))) {
                    prefix++;
                } else if (fl & (1U<<(' '-' '))) {
                    prefix += 2;
                } else {
                    pl = 0;
                }
            case 'u':
                a = fmt_u(arg.i, z);
                break;
            }
            if (xp && p < 0) {
                return -1; /* overflow */
            }
            if (xp) {
                fl &= ~(1U<<('0'-' '));
            }
            if (!arg.i && !p) {
                a = z;
            } else {
                p = (((p)>(z - a + !arg.i))?(p):(z - a + !arg.i));
            }
        }

        if (p < z - a) {
            p = z - a;
        }
        if (p > (0xFFFFFFFFFFFFFFFF) - pl) {
            return -1; /* overflow */
        }
        if (w < pl + p) {
            w = pl + p;
        }
        if (w > (0xFFFFFFFFFFFFFFFF) - cnt) {
            return -1; /* overflow */
        }

        pad(f, ' ', w, pl + p, fl);
        out(f, prefix, pl);
        pad(f, '0', w, pl + p, fl ^ (1U<<('0'-' ')));
        pad(f, '0', p, z - a, 0);
        out(f, a, z - a);
        pad(f, ' ', w, pl + p, fl ^ (1U<<('-'-' ')));

        l = w;
    }

    if (f) {
        return cnt;
    }
    if (!l10n) {
        return 0;
    }

    int i;
    for (i = 1; i <= 9 && nl_type[i]; i++) {
        pop_arg(nl_arg + i, nl_type[i], ap);
    }
    for (; i <= 9 && !nl_type[i]; i++);
    if (i <= 9) {
        return -1; /* invalid */
    }

    return 1;
}

static int vprintf(out_wrap_t *out, const char *fmt, va_list ap)
{
    va_list ap2;
    int nl_type[9 + 1] = {0};
    union arg nl_arg[9 + 1];
    int ret;

    /* validate format string */
    __builtin_va_copy(ap2,ap);
    if (printf_core(((void *)0), fmt, &ap2, nl_arg, nl_type) < 0) {
        __builtin_va_end(ap2);
        return -1;
    }

    ret = printf_core(out, fmt, &ap2, nl_arg, nl_type);
    __builtin_va_end(ap2);
    return ret;
}

/*
 *------------------------------------------------------------------------------
 * Kernel printing API
 *------------------------------------------------------------------------------
 */

int impl_kvprintf(const char *format, va_list ap)
{
    out_wrap_t out_wrap = {
        .write = do_output_to_putchar,
        .buf = ((void *)0),
        .maxlen = 0,
        .used = 0
    };

    return vprintf(&out_wrap, format, ap);
}

int impl_ksnvprintf(char *str, word_t size, const char *format, va_list ap)
{
    if (!str) {
        size = 0;
    }

    out_wrap_t out_wrap = {
        .write = do_output_to_buffer,
        .buf = str,
        .maxlen = size,
        .used = 0
    };

    int ret = vprintf(&out_wrap, format, ap);

    /* We return the number of characters written into the buffer, excluding the
     * terminating null char. However, we do never write more than 'size' bytes,
     * that includes the terminating null char. If the output was truncated due
     * to this limit, then the return value is the number of chars excluding the
     * terminating null byte, which would have been written to the buffer, if
     * enough space had been available. Thus, a return value of 'size' or more
     * means that the output was truncated.
     */
    if ((ret > 0) && (size > 0)) {
        str[(ret < size) ? ret : size - 1] = '\0';
    }

    return ret;
}
# 1 "/home/b/test/sel4test/kernel/src/machine/registerset.c"
/*
 * Copyright 2020, Data61, CSIRO (ABN 41 687 119 230)
 *
 * SPDX-License-Identifier: GPL-2.0-only
 */



const register_t fault_messages[][(((n_syscallMessage)>(n_exceptionMessage))?(n_syscallMessage):(n_exceptionMessage))] = {
    [MessageID_Syscall] = { [seL4_UnknownSyscall_X0] = X0, [seL4_UnknownSyscall_X1] = X1, [seL4_UnknownSyscall_X2] = X2, [seL4_UnknownSyscall_X3] = X3, [seL4_UnknownSyscall_X4] = X4, [seL4_UnknownSyscall_X5] = X5, [seL4_UnknownSyscall_X6] = X6, [seL4_UnknownSyscall_X7] = X7, [seL4_UnknownSyscall_FaultIP] = FaultIP, [seL4_UnknownSyscall_SP] = SP_EL0, [seL4_UnknownSyscall_LR] = ELR_EL1, [seL4_UnknownSyscall_SPSR] = SPSR_EL1},
    [MessageID_Exception] = { [seL4_UserException_FaultIP] = FaultIP, [seL4_UserException_SP] = SP_EL0, [seL4_UserException_SPSR] = SPSR_EL1 },



};
# 1 "/home/b/test/sel4test/kernel/src/model/preemption.c"
/*
 * Copyright 2014, General Dynamics C4 Systems
 *
 * SPDX-License-Identifier: GPL-2.0-only
 */


# 1 "/home/b/test/sel4test/kernel/include/model/preemption.h" 1
/*
 * Copyright 2014, General Dynamics C4 Systems
 *
 * SPDX-License-Identifier: GPL-2.0-only
 */

       



exception_t preemptionPoint(void);
# 9 "/home/b/test/sel4test/kernel/src/model/preemption.c" 2




/*
 * Possibly preempt the current thread to allow an interrupt to be handled.
 */
exception_t preemptionPoint(void)
{
    /* Record that we have performed some work. */
    ksWorkUnitsCompleted++;

    /*
     * If we have performed a non-trivial amount of work since last time we
     * checked for preemption, and there is an interrupt pending, handle the
     * interrupt.
     *
     * We avoid checking for pending IRQs every call, as our callers tend to
     * call us in a tight loop and checking for pending IRQs can be quite slow.
     */
    if (ksWorkUnitsCompleted >= 100) {
        ksWorkUnitsCompleted = 0;





        if (isIRQPending()) {

            return EXCEPTION_PREEMPTED;
        }
    }

    return EXCEPTION_NONE;
}
# 1 "/home/b/test/sel4test/kernel/src/model/smp.c"
/*
 * Copyright 2020, Data61, CSIRO (ABN 41 687 119 230)
 *
 * SPDX-License-Identifier: GPL-2.0-only
 */
# 1 "/home/b/test/sel4test/kernel/src/model/statedata.c"
/*
 * Copyright 2014, General Dynamics C4 Systems
 *
 * SPDX-License-Identifier: GPL-2.0-only
 */




# 1 "/home/b/test/sel4test/kernel/include/plat/default/plat/machine.h" 1
/*
 * Copyright 2020, Data61, CSIRO (ABN 41 687 119 230)
 *
 * SPDX-License-Identifier: GPL-2.0-only
 */
# 11 "/home/b/test/sel4test/kernel/src/model/statedata.c" 2






/* Collective cpu states, including both pre core architecture dependant and independent data */
;

/* Global count of how many cpus there are */
word_t ksNumCPUs;

/* Pointer to the head of the scheduler queue for each priority */
tcb_queue_t ksReadyQueues[(1 * 256)];
word_t ksReadyQueuesL1Bitmap[1];
word_t ksReadyQueuesL2Bitmap[1][((256 + (1 << 6) - 1) / (1 << 6))];
_Static_assert((((256 + (1 << 6) - 1) / (1 << 6)) - 1) <= (1 << 6), "ksReadyQueuesL1BitmapBigEnough");





/* Current thread TCB pointer */
tcb_t * ksCurThread;

/* Idle thread TCB pointer */
tcb_t * ksIdleThread;

/* Values of 0 and ~0 encode ResumeCurrentThread and ChooseNewThread
 * respectively; other values encode SwitchToThread and must be valid
 * tcb pointers */
tcb_t * ksSchedulerAction;


/* The thread using the FPU, or NULL if FPU state is invalid */
tcb_t * ksCurFPUOwner;
# 62 "/home/b/test/sel4test/kernel/src/model/statedata.c"
tcb_t * ksDebugTCBs;
# 73 "/home/b/test/sel4test/kernel/src/model/statedata.c"
/* Units of work we have completed since the last time we checked for
 * pending interrupts */
word_t ksWorkUnitsCompleted;

irq_state_t intStateIRQTable[(maxIRQ + 1)];
/* CNode containing interrupt handler endpoints - like all seL4 objects, this CNode needs to be
 * of a size that is a power of 2 and aligned to its size. */
cte_t intStateIRQNode[(1ul << ((9)))] __attribute__((__aligned__((1ul << ((9) + 5)))));
_Static_assert(sizeof(intStateIRQNode) >= (((maxIRQ + 1)) *sizeof(cte_t)), "irqCNodeSize");;

/* Currently active domain */
dom_t ksCurDomain;

/* Domain timeslice remaining */



word_t ksDomainTime;


/* An index into ksDomSchedule for active domain and length. */
word_t ksDomScheduleIdx;

/* Idle thread. */
__attribute__((__section__("._idle_thread"))) char ksIdleThreadTCB[1][(1ul << (12))] __attribute__((__aligned__((1ul << (12)))));







kernel_entry_t ksKernelEntry;
# 1 "/home/b/test/sel4test/kernel/src/object/cnode.c"
/*
 * Copyright 2014, General Dynamics C4 Systems
 *
 * SPDX-License-Identifier: GPL-2.0-only
 */
# 25 "/home/b/test/sel4test/kernel/src/object/cnode.c"
struct finaliseSlot_ret {
    exception_t status;
    bool_t success;
    cap_t cleanupInfo;
};
typedef struct finaliseSlot_ret finaliseSlot_ret_t;

static finaliseSlot_ret_t finaliseSlot(cte_t *slot, bool_t exposed);
static void emptySlot(cte_t *slot, cap_t cleanupInfo);
static exception_t reduceZombie(cte_t *slot, bool_t exposed);







exception_t decodeCNodeInvocation(word_t invLabel, word_t length, cap_t cap,
                                  word_t *buffer)
{
    lookupSlot_ret_t lu_ret;
    cte_t *destSlot;
    word_t index, w_bits;
    exception_t status;

    /* Haskell error: "decodeCNodeInvocation: invalid cap" */
    do { if (!(cap_get_capType(cap) == cap_cnode_cap)) { _assert_fail("cap_get_capType(cap) == cap_cnode_cap", "/home/b/test/sel4test/kernel/src/object/cnode.c", 51, __func__); } } while(0);

    if (invLabel < CNodeRevoke || invLabel > CNodeSaveCaller) {
        do { printf("\033[0m" "\033[1m" "<<" "\033[0m" "\033[32m" "seL4(CPU %" "lu" ")" "\033[0m" "\033[1m" " [%s/%d T%p \"%s\" @%lx]: " "CNodeCap: Illegal Operation attempted." ">>" "\033[0m" "\n", 0lu, __func__, 54, ksCurThread, ((debug_tcb_t *)(((cte_t *)((word_t)(ksCurThread)&~((1ul << (12)) - 1ul)))+(tcbCNodeEntries)))->tcbName, (word_t)getRestartPC(ksCurThread)); } while (0);
        current_syscall_error.type = seL4_IllegalOperation;
        return EXCEPTION_SYSCALL_ERROR;
    }

    if (length < 2) {
        do { printf("\033[0m" "\033[1m" "<<" "\033[0m" "\033[32m" "seL4(CPU %" "lu" ")" "\033[0m" "\033[1m" " [%s/%d T%p \"%s\" @%lx]: " "CNode operation: Truncated message." ">>" "\033[0m" "\n", 0lu, __func__, 60, ksCurThread, ((debug_tcb_t *)(((cte_t *)((word_t)(ksCurThread)&~((1ul << (12)) - 1ul)))+(tcbCNodeEntries)))->tcbName, (word_t)getRestartPC(ksCurThread)); } while (0);
        current_syscall_error.type = seL4_TruncatedMessage;
        return EXCEPTION_SYSCALL_ERROR;
    }
    index = getSyscallArg(0, buffer);
    w_bits = getSyscallArg(1, buffer);

    lu_ret = lookupTargetSlot(cap, index, w_bits);
    if (lu_ret.status != EXCEPTION_NONE) {
        do { printf("\033[0m" "\033[1m" "<<" "\033[0m" "\033[32m" "seL4(CPU %" "lu" ")" "\033[0m" "\033[1m" " [%s/%d T%p \"%s\" @%lx]: " "CNode operation: Target slot invalid." ">>" "\033[0m" "\n", 0lu, __func__, 69, ksCurThread, ((debug_tcb_t *)(((cte_t *)((word_t)(ksCurThread)&~((1ul << (12)) - 1ul)))+(tcbCNodeEntries)))->tcbName, (word_t)getRestartPC(ksCurThread)); } while (0);
        return lu_ret.status;
    }
    destSlot = lu_ret.slot;

    if (invLabel >= CNodeCopy && invLabel <= CNodeMutate) {
        cte_t *srcSlot;
        word_t srcIndex, srcDepth, capData;
        bool_t isMove;
        seL4_CapRights_t cap_rights;
        cap_t srcRoot, newCap;
        deriveCap_ret_t dc_ret;
        cap_t srcCap;

        if (length < 4 || current_extra_caps.excaprefs[0] == ((void *)0)) {
            do { printf("\033[0m" "\033[1m" "<<" "\033[0m" "\033[32m" "seL4(CPU %" "lu" ")" "\033[0m" "\033[1m" " [%s/%d T%p \"%s\" @%lx]: " "CNode Copy/Mint/Move/Mutate: Truncated message." ">>" "\033[0m" "\n", 0lu, __func__, 84, ksCurThread, ((debug_tcb_t *)(((cte_t *)((word_t)(ksCurThread)&~((1ul << (12)) - 1ul)))+(tcbCNodeEntries)))->tcbName, (word_t)getRestartPC(ksCurThread)); } while (0);
            current_syscall_error.type = seL4_TruncatedMessage;
            return EXCEPTION_SYSCALL_ERROR;
        }
        srcIndex = getSyscallArg(2, buffer);
        srcDepth = getSyscallArg(3, buffer);

        srcRoot = current_extra_caps.excaprefs[0]->cap;

        status = ensureEmptySlot(destSlot);
        if (status != EXCEPTION_NONE) {
            do { printf("\033[0m" "\033[1m" "<<" "\033[0m" "\033[32m" "seL4(CPU %" "lu" ")" "\033[0m" "\033[1m" " [%s/%d T%p \"%s\" @%lx]: " "CNode Copy/Mint/Move/Mutate: Destination not empty." ">>" "\033[0m" "\n", 0lu, __func__, 95, ksCurThread, ((debug_tcb_t *)(((cte_t *)((word_t)(ksCurThread)&~((1ul << (12)) - 1ul)))+(tcbCNodeEntries)))->tcbName, (word_t)getRestartPC(ksCurThread)); } while (0);
            return status;
        }

        lu_ret = lookupSourceSlot(srcRoot, srcIndex, srcDepth);
        if (lu_ret.status != EXCEPTION_NONE) {
            do { printf("\033[0m" "\033[1m" "<<" "\033[0m" "\033[32m" "seL4(CPU %" "lu" ")" "\033[0m" "\033[1m" " [%s/%d T%p \"%s\" @%lx]: " "CNode Copy/Mint/Move/Mutate: Invalid source slot." ">>" "\033[0m" "\n", 0lu, __func__, 101, ksCurThread, ((debug_tcb_t *)(((cte_t *)((word_t)(ksCurThread)&~((1ul << (12)) - 1ul)))+(tcbCNodeEntries)))->tcbName, (word_t)getRestartPC(ksCurThread)); } while (0);
            return lu_ret.status;
        }
        srcSlot = lu_ret.slot;

        if (cap_get_capType(srcSlot->cap) == cap_null_cap) {
            do { printf("\033[0m" "\033[1m" "<<" "\033[0m" "\033[32m" "seL4(CPU %" "lu" ")" "\033[0m" "\033[1m" " [%s/%d T%p \"%s\" @%lx]: " "CNode Copy/Mint/Move/Mutate: Source slot invalid or empty." ">>" "\033[0m" "\n", 0lu, __func__, 107, ksCurThread, ((debug_tcb_t *)(((cte_t *)((word_t)(ksCurThread)&~((1ul << (12)) - 1ul)))+(tcbCNodeEntries)))->tcbName, (word_t)getRestartPC(ksCurThread)); } while (0);
            current_syscall_error.type = seL4_FailedLookup;
            current_syscall_error.failedLookupWasSource = 1;
            current_lookup_fault =
                lookup_fault_missing_capability_new(srcDepth);
            return EXCEPTION_SYSCALL_ERROR;
        }

        switch (invLabel) {
        case CNodeCopy:

            if (length < 5) {
                do { printf("\033[0m" "\033[1m" "<<" "\033[0m" "\033[32m" "seL4(CPU %" "lu" ")" "\033[0m" "\033[1m" " [%s/%d T%p \"%s\" @%lx]: " "Truncated message for CNode Copy operation." ">>" "\033[0m" "\n", 0lu, __func__, 119, ksCurThread, ((debug_tcb_t *)(((cte_t *)((word_t)(ksCurThread)&~((1ul << (12)) - 1ul)))+(tcbCNodeEntries)))->tcbName, (word_t)getRestartPC(ksCurThread)); } while (0);
                current_syscall_error.type = seL4_TruncatedMessage;
                return EXCEPTION_SYSCALL_ERROR;
            }

            cap_rights = rightsFromWord(getSyscallArg(4, buffer));
            srcCap = maskCapRights(cap_rights, srcSlot->cap);
            dc_ret = deriveCap(srcSlot, srcCap);
            if (dc_ret.status != EXCEPTION_NONE) {
                do { printf("\033[0m" "\033[1m" "<<" "\033[0m" "\033[32m" "seL4(CPU %" "lu" ")" "\033[0m" "\033[1m" " [%s/%d T%p \"%s\" @%lx]: " "Error deriving cap for CNode Copy operation." ">>" "\033[0m" "\n", 0lu, __func__, 128, ksCurThread, ((debug_tcb_t *)(((cte_t *)((word_t)(ksCurThread)&~((1ul << (12)) - 1ul)))+(tcbCNodeEntries)))->tcbName, (word_t)getRestartPC(ksCurThread)); } while (0);
                return dc_ret.status;
            }
            newCap = dc_ret.cap;
            isMove = false;

            break;

        case CNodeMint:
            if (length < 6) {
                do { printf("\033[0m" "\033[1m" "<<" "\033[0m" "\033[32m" "seL4(CPU %" "lu" ")" "\033[0m" "\033[1m" " [%s/%d T%p \"%s\" @%lx]: " "CNode Mint: Truncated message." ">>" "\033[0m" "\n", 0lu, __func__, 138, ksCurThread, ((debug_tcb_t *)(((cte_t *)((word_t)(ksCurThread)&~((1ul << (12)) - 1ul)))+(tcbCNodeEntries)))->tcbName, (word_t)getRestartPC(ksCurThread)); } while (0);
                current_syscall_error.type = seL4_TruncatedMessage;
                return EXCEPTION_SYSCALL_ERROR;
            }

            cap_rights = rightsFromWord(getSyscallArg(4, buffer));
            capData = getSyscallArg(5, buffer);
            srcCap = maskCapRights(cap_rights, srcSlot->cap);
            dc_ret = deriveCap(srcSlot,
                               updateCapData(false, capData, srcCap));
            if (dc_ret.status != EXCEPTION_NONE) {
                do { printf("\033[0m" "\033[1m" "<<" "\033[0m" "\033[32m" "seL4(CPU %" "lu" ")" "\033[0m" "\033[1m" " [%s/%d T%p \"%s\" @%lx]: " "Error deriving cap for CNode Mint operation." ">>" "\033[0m" "\n", 0lu, __func__, 149, ksCurThread, ((debug_tcb_t *)(((cte_t *)((word_t)(ksCurThread)&~((1ul << (12)) - 1ul)))+(tcbCNodeEntries)))->tcbName, (word_t)getRestartPC(ksCurThread)); } while (0);
                return dc_ret.status;
            }
            newCap = dc_ret.cap;
            isMove = false;

            break;

        case CNodeMove:
            newCap = srcSlot->cap;
            isMove = true;

            break;

        case CNodeMutate:
            if (length < 5) {
                do { printf("\033[0m" "\033[1m" "<<" "\033[0m" "\033[32m" "seL4(CPU %" "lu" ")" "\033[0m" "\033[1m" " [%s/%d T%p \"%s\" @%lx]: " "CNode Mutate: Truncated message." ">>" "\033[0m" "\n", 0lu, __func__, 165, ksCurThread, ((debug_tcb_t *)(((cte_t *)((word_t)(ksCurThread)&~((1ul << (12)) - 1ul)))+(tcbCNodeEntries)))->tcbName, (word_t)getRestartPC(ksCurThread)); } while (0);
                current_syscall_error.type = seL4_TruncatedMessage;
                return EXCEPTION_SYSCALL_ERROR;
            }

            capData = getSyscallArg(4, buffer);
            newCap = updateCapData(true, capData, srcSlot->cap);
            isMove = true;

            break;

        default:
            do { if (!(0)) { _assert_fail("0", "/home/b/test/sel4test/kernel/src/object/cnode.c", 177, __func__); } } while(0);
            return EXCEPTION_NONE;
        }

        if (cap_get_capType(newCap) == cap_null_cap) {
            do { printf("\033[0m" "\033[1m" "<<" "\033[0m" "\033[32m" "seL4(CPU %" "lu" ")" "\033[0m" "\033[1m" " [%s/%d T%p \"%s\" @%lx]: " "CNode Copy/Mint/Move/Mutate: Mutated cap would be invalid." ">>" "\033[0m" "\n", 0lu, __func__, 182, ksCurThread, ((debug_tcb_t *)(((cte_t *)((word_t)(ksCurThread)&~((1ul << (12)) - 1ul)))+(tcbCNodeEntries)))->tcbName, (word_t)getRestartPC(ksCurThread)); } while (0);
            current_syscall_error.type = seL4_IllegalOperation;
            return EXCEPTION_SYSCALL_ERROR;
        }

        setThreadState(ksCurThread, ThreadState_Restart);
        if (isMove) {
            return invokeCNodeMove(newCap, srcSlot, destSlot);
        } else {
            return invokeCNodeInsert(newCap, srcSlot, destSlot);
        }
    }

    if (invLabel == CNodeRevoke) {
        setThreadState(ksCurThread, ThreadState_Restart);
        return invokeCNodeRevoke(destSlot);
    }

    if (invLabel == CNodeDelete) {
        setThreadState(ksCurThread, ThreadState_Restart);
        return invokeCNodeDelete(destSlot);
    }


    if (invLabel == CNodeSaveCaller) {
        status = ensureEmptySlot(destSlot);
        if (status != EXCEPTION_NONE) {
            do { printf("\033[0m" "\033[1m" "<<" "\033[0m" "\033[32m" "seL4(CPU %" "lu" ")" "\033[0m" "\033[1m" " [%s/%d T%p \"%s\" @%lx]: " "CNode SaveCaller: Destination slot not empty." ">>" "\033[0m" "\n", 0lu, __func__, 209, ksCurThread, ((debug_tcb_t *)(((cte_t *)((word_t)(ksCurThread)&~((1ul << (12)) - 1ul)))+(tcbCNodeEntries)))->tcbName, (word_t)getRestartPC(ksCurThread)); } while (0);
            return status;
        }

        setThreadState(ksCurThread, ThreadState_Restart);
        return invokeCNodeSaveCaller(destSlot);
    }


    if (invLabel == CNodeCancelBadgedSends) {
        cap_t destCap;

        destCap = destSlot->cap;

        if (!hasCancelSendRights(destCap)) {
            do { printf("\033[0m" "\033[1m" "<<" "\033[0m" "\033[32m" "seL4(CPU %" "lu" ")" "\033[0m" "\033[1m" " [%s/%d T%p \"%s\" @%lx]: " "CNode CancelBadgedSends: Target cap invalid." ">>" "\033[0m" "\n", 0lu, __func__, 224, ksCurThread, ((debug_tcb_t *)(((cte_t *)((word_t)(ksCurThread)&~((1ul << (12)) - 1ul)))+(tcbCNodeEntries)))->tcbName, (word_t)getRestartPC(ksCurThread)); } while (0);
            current_syscall_error.type = seL4_IllegalOperation;
            return EXCEPTION_SYSCALL_ERROR;
        }
        setThreadState(ksCurThread, ThreadState_Restart);
        return invokeCNodeCancelBadgedSends(destCap);
    }

    if (invLabel == CNodeRotate) {
        word_t pivotNewData, pivotIndex, pivotDepth;
        word_t srcNewData, srcIndex, srcDepth;
        cte_t *pivotSlot, *srcSlot;
        cap_t pivotRoot, srcRoot, newSrcCap, newPivotCap;

        if (length < 8 || current_extra_caps.excaprefs[0] == ((void *)0)
            || current_extra_caps.excaprefs[1] == ((void *)0)) {
            current_syscall_error.type = seL4_TruncatedMessage;
            return EXCEPTION_SYSCALL_ERROR;
        }
        pivotNewData = getSyscallArg(2, buffer);
        pivotIndex = getSyscallArg(3, buffer);
        pivotDepth = getSyscallArg(4, buffer);
        srcNewData = getSyscallArg(5, buffer);
        srcIndex = getSyscallArg(6, buffer);
        srcDepth = getSyscallArg(7, buffer);

        pivotRoot = current_extra_caps.excaprefs[0]->cap;
        srcRoot = current_extra_caps.excaprefs[1]->cap;

        lu_ret = lookupSourceSlot(srcRoot, srcIndex, srcDepth);
        if (lu_ret.status != EXCEPTION_NONE) {
            return lu_ret.status;
        }
        srcSlot = lu_ret.slot;

        lu_ret = lookupPivotSlot(pivotRoot, pivotIndex, pivotDepth);
        if (lu_ret.status != EXCEPTION_NONE) {
            return lu_ret.status;
        }
        pivotSlot = lu_ret.slot;

        if (pivotSlot == srcSlot || pivotSlot == destSlot) {
            do { printf("\033[0m" "\033[1m" "<<" "\033[0m" "\033[32m" "seL4(CPU %" "lu" ")" "\033[0m" "\033[1m" " [%s/%d T%p \"%s\" @%lx]: " "CNode Rotate: Pivot slot the same as source or dest slot." ">>" "\033[0m" "\n", 0lu, __func__, 266, ksCurThread, ((debug_tcb_t *)(((cte_t *)((word_t)(ksCurThread)&~((1ul << (12)) - 1ul)))+(tcbCNodeEntries)))->tcbName, (word_t)getRestartPC(ksCurThread)); } while (0);
            current_syscall_error.type = seL4_IllegalOperation;
            return EXCEPTION_SYSCALL_ERROR;
        }

        if (srcSlot != destSlot) {
            status = ensureEmptySlot(destSlot);
            if (status != EXCEPTION_NONE) {
                return status;
            }
        }

        if (cap_get_capType(srcSlot->cap) == cap_null_cap) {
            current_syscall_error.type = seL4_FailedLookup;
            current_syscall_error.failedLookupWasSource = 1;
            current_lookup_fault = lookup_fault_missing_capability_new(srcDepth);
            return EXCEPTION_SYSCALL_ERROR;
        }

        if (cap_get_capType(pivotSlot->cap) == cap_null_cap) {
            current_syscall_error.type = seL4_FailedLookup;
            current_syscall_error.failedLookupWasSource = 0;
            current_lookup_fault = lookup_fault_missing_capability_new(pivotDepth);
            return EXCEPTION_SYSCALL_ERROR;
        }

        newSrcCap = updateCapData(true, srcNewData, srcSlot->cap);
        newPivotCap = updateCapData(true, pivotNewData, pivotSlot->cap);

        if (cap_get_capType(newSrcCap) == cap_null_cap) {
            do { printf("\033[0m" "\033[1m" "<<" "\033[0m" "\033[32m" "seL4(CPU %" "lu" ")" "\033[0m" "\033[1m" " [%s/%d T%p \"%s\" @%lx]: " "CNode Rotate: Source cap invalid." ">>" "\033[0m" "\n", 0lu, __func__, 296, ksCurThread, ((debug_tcb_t *)(((cte_t *)((word_t)(ksCurThread)&~((1ul << (12)) - 1ul)))+(tcbCNodeEntries)))->tcbName, (word_t)getRestartPC(ksCurThread)); } while (0);
            current_syscall_error.type = seL4_IllegalOperation;
            return EXCEPTION_SYSCALL_ERROR;
        }

        if (cap_get_capType(newPivotCap) == cap_null_cap) {
            do { printf("\033[0m" "\033[1m" "<<" "\033[0m" "\033[32m" "seL4(CPU %" "lu" ")" "\033[0m" "\033[1m" " [%s/%d T%p \"%s\" @%lx]: " "CNode Rotate: Pivot cap invalid." ">>" "\033[0m" "\n", 0lu, __func__, 302, ksCurThread, ((debug_tcb_t *)(((cte_t *)((word_t)(ksCurThread)&~((1ul << (12)) - 1ul)))+(tcbCNodeEntries)))->tcbName, (word_t)getRestartPC(ksCurThread)); } while (0);
            current_syscall_error.type = seL4_IllegalOperation;
            return EXCEPTION_SYSCALL_ERROR;
        }

        setThreadState(ksCurThread, ThreadState_Restart);
        return invokeCNodeRotate(newSrcCap, newPivotCap,
                                 srcSlot, pivotSlot, destSlot);
    }

    return EXCEPTION_NONE;
}

exception_t invokeCNodeRevoke(cte_t *destSlot)
{
    return cteRevoke(destSlot);
}

exception_t invokeCNodeDelete(cte_t *destSlot)
{
    return cteDelete(destSlot, true);
}

exception_t invokeCNodeCancelBadgedSends(cap_t cap)
{
    word_t badge = cap_endpoint_cap_get_capEPBadge(cap);
    if (badge) {
        endpoint_t *ep = (endpoint_t *)
                         cap_endpoint_cap_get_capEPPtr(cap);
        cancelBadgedSends(ep, badge);
    }
    return EXCEPTION_NONE;
}

exception_t invokeCNodeInsert(cap_t cap, cte_t *srcSlot, cte_t *destSlot)
{
    cteInsert(cap, srcSlot, destSlot);

    return EXCEPTION_NONE;
}

exception_t invokeCNodeMove(cap_t cap, cte_t *srcSlot, cte_t *destSlot)
{
    cteMove(cap, srcSlot, destSlot);

    return EXCEPTION_NONE;
}

exception_t invokeCNodeRotate(cap_t cap1, cap_t cap2, cte_t *slot1,
                              cte_t *slot2, cte_t *slot3)
{
    if (slot1 == slot3) {
        cteSwap(cap1, slot1, cap2, slot2);
    } else {
        cteMove(cap2, slot2, slot3);
        cteMove(cap1, slot1, slot2);
    }

    return EXCEPTION_NONE;
}


exception_t invokeCNodeSaveCaller(cte_t *destSlot)
{
    cap_t cap;
    cte_t *srcSlot;

    srcSlot = (((cte_t *)((word_t)(ksCurThread)&~((1ul << (12)) - 1ul)))+(tcbCaller));
    cap = srcSlot->cap;

    switch (cap_get_capType(cap)) {
    case cap_null_cap:
        do { printf("\033[0m" "\033[1m" "<<" "\033[0m" "\033[32m" "seL4(CPU %" "lu" ")" "\033[0m" "\033[1m" " [%s/%d T%p \"%s\" @%lx]: " "CNode SaveCaller: Reply cap not present." ">>" "\033[0m" "\n", 0lu, __func__, 374, ksCurThread, ((debug_tcb_t *)(((cte_t *)((word_t)(ksCurThread)&~((1ul << (12)) - 1ul)))+(tcbCNodeEntries)))->tcbName, (word_t)getRestartPC(ksCurThread)); } while (0);
        break;

    case cap_reply_cap:
        if (!cap_reply_cap_get_capReplyMaster(cap)) {
            cteMove(cap, srcSlot, destSlot);
        }
        break;

    default:
        _fail("caller capability must be null or reply", "/home/b/test/sel4test/kernel/src/object/cnode.c", 384, __func__);
        break;
    }

    return EXCEPTION_NONE;
}


/*
 * If creating a child UntypedCap, don't allow new objects to be created in the
 * parent.
 */
static void setUntypedCapAsFull(cap_t srcCap, cap_t newCap, cte_t *srcSlot)
{
    if ((cap_get_capType(srcCap) == cap_untyped_cap)
        && (cap_get_capType(newCap) == cap_untyped_cap)) {
        if ((cap_untyped_cap_get_capPtr(srcCap)
             == cap_untyped_cap_get_capPtr(newCap))
            && (cap_untyped_cap_get_capBlockSize(newCap)
                == cap_untyped_cap_get_capBlockSize(srcCap))) {
            cap_untyped_cap_ptr_set_capFreeIndex(&(srcSlot->cap),
                                                 ((1ul << ((cap_untyped_cap_get_capBlockSize(srcCap)) - 4))));
        }
    }
}

void cteInsert(cap_t newCap, cte_t *srcSlot, cte_t *destSlot)
{
    mdb_node_t srcMDB, newMDB;
    cap_t srcCap;
    bool_t newCapIsRevocable;

    srcMDB = srcSlot->cteMDBNode;
    srcCap = srcSlot->cap;

    newCapIsRevocable = isCapRevocable(newCap, srcCap);

    newMDB = mdb_node_set_mdbPrev(srcMDB, ((word_t)(srcSlot)));
    newMDB = mdb_node_set_mdbRevocable(newMDB, newCapIsRevocable);
    newMDB = mdb_node_set_mdbFirstBadged(newMDB, newCapIsRevocable);

    /* Haskell error: "cteInsert to non-empty destination" */
    do { if (!(cap_get_capType(destSlot->cap) == cap_null_cap)) { _assert_fail("cap_get_capType(destSlot->cap) == cap_null_cap", "/home/b/test/sel4test/kernel/src/object/cnode.c", 426, __func__); } } while(0);
    /* Haskell error: "cteInsert: mdb entry must be empty" */
    do { if (!((cte_t *)mdb_node_get_mdbNext(destSlot->cteMDBNode) == ((void *)0) && (cte_t *)mdb_node_get_mdbPrev(destSlot->cteMDBNode) == ((void *)0))) { _assert_fail("(cte_t *)mdb_node_get_mdbNext(destSlot->cteMDBNode) == NULL && (cte_t *)mdb_node_get_mdbPrev(destSlot->cteMDBNode) == NULL", "/home/b/test/sel4test/kernel/src/object/cnode.c", 428, __func__); } } while(0)
                                                                       ;

    /* Prevent parent untyped cap from being used again if creating a child
     * untyped from it. */
    setUntypedCapAsFull(srcCap, newCap, srcSlot);

    destSlot->cap = newCap;
    destSlot->cteMDBNode = newMDB;
    mdb_node_ptr_set_mdbNext(&srcSlot->cteMDBNode, ((word_t)(destSlot)));
    if (mdb_node_get_mdbNext(newMDB)) {
        mdb_node_ptr_set_mdbPrev(
            &((cte_t *)(mdb_node_get_mdbNext(newMDB)))->cteMDBNode,
            ((word_t)(destSlot)));
    }
}

void cteMove(cap_t newCap, cte_t *srcSlot, cte_t *destSlot)
{
    mdb_node_t mdb;
    word_t prev_ptr, next_ptr;

    /* Haskell error: "cteMove to non-empty destination" */
    do { if (!(cap_get_capType(destSlot->cap) == cap_null_cap)) { _assert_fail("cap_get_capType(destSlot->cap) == cap_null_cap", "/home/b/test/sel4test/kernel/src/object/cnode.c", 451, __func__); } } while(0);
    /* Haskell error: "cteMove: mdb entry must be empty" */
    do { if (!((cte_t *)mdb_node_get_mdbNext(destSlot->cteMDBNode) == ((void *)0) && (cte_t *)mdb_node_get_mdbPrev(destSlot->cteMDBNode) == ((void *)0))) { _assert_fail("(cte_t *)mdb_node_get_mdbNext(destSlot->cteMDBNode) == NULL && (cte_t *)mdb_node_get_mdbPrev(destSlot->cteMDBNode) == NULL", "/home/b/test/sel4test/kernel/src/object/cnode.c", 453, __func__); } } while(0)
                                                                       ;

    mdb = srcSlot->cteMDBNode;
    destSlot->cap = newCap;
    srcSlot->cap = cap_null_cap_new();
    destSlot->cteMDBNode = mdb;
    srcSlot->cteMDBNode = mdb_node_new(0, false, false, 0);

    prev_ptr = mdb_node_get_mdbPrev(mdb);
    if (prev_ptr)
        mdb_node_ptr_set_mdbNext(
            &((cte_t *)(prev_ptr))->cteMDBNode,
            ((word_t)(destSlot)));

    next_ptr = mdb_node_get_mdbNext(mdb);
    if (next_ptr)
        mdb_node_ptr_set_mdbPrev(
            &((cte_t *)(next_ptr))->cteMDBNode,
            ((word_t)(destSlot)));
}

void capSwapForDelete(cte_t *slot1, cte_t *slot2)
{
    cap_t cap1, cap2;

    if (slot1 == slot2) {
        return;
    }

    cap1 = slot1->cap;
    cap2 = slot2->cap;

    cteSwap(cap1, slot1, cap2, slot2);
}

void cteSwap(cap_t cap1, cte_t *slot1, cap_t cap2, cte_t *slot2)
{
    mdb_node_t mdb1, mdb2;
    word_t next_ptr, prev_ptr;

    slot1->cap = cap2;
    slot2->cap = cap1;

    mdb1 = slot1->cteMDBNode;

    prev_ptr = mdb_node_get_mdbPrev(mdb1);
    if (prev_ptr)
        mdb_node_ptr_set_mdbNext(
            &((cte_t *)(prev_ptr))->cteMDBNode,
            ((word_t)(slot2)));

    next_ptr = mdb_node_get_mdbNext(mdb1);
    if (next_ptr)
        mdb_node_ptr_set_mdbPrev(
            &((cte_t *)(next_ptr))->cteMDBNode,
            ((word_t)(slot2)));

    mdb2 = slot2->cteMDBNode;
    slot1->cteMDBNode = mdb2;
    slot2->cteMDBNode = mdb1;

    prev_ptr = mdb_node_get_mdbPrev(mdb2);
    if (prev_ptr)
        mdb_node_ptr_set_mdbNext(
            &((cte_t *)(prev_ptr))->cteMDBNode,
            ((word_t)(slot1)));

    next_ptr = mdb_node_get_mdbNext(mdb2);
    if (next_ptr)
        mdb_node_ptr_set_mdbPrev(
            &((cte_t *)(next_ptr))->cteMDBNode,
            ((word_t)(slot1)));
}

exception_t cteRevoke(cte_t *slot)
{
    cte_t *nextPtr;
    exception_t status;

    /* there is no need to check for a NullCap as NullCaps are
       always accompanied by null mdb pointers */
    for (nextPtr = ((cte_t *)(mdb_node_get_mdbNext(slot->cteMDBNode)));
         nextPtr && isMDBParentOf(slot, nextPtr);
         nextPtr = ((cte_t *)(mdb_node_get_mdbNext(slot->cteMDBNode)))) {
        status = cteDelete(nextPtr, true);
        if (status != EXCEPTION_NONE) {
            return status;
        }

        status = preemptionPoint();
        if (status != EXCEPTION_NONE) {
            return status;
        }
    }

    return EXCEPTION_NONE;
}

exception_t cteDelete(cte_t *slot, bool_t exposed)
{
    finaliseSlot_ret_t fs_ret;

    fs_ret = finaliseSlot(slot, exposed);
    if (fs_ret.status != EXCEPTION_NONE) {
        return fs_ret.status;
    }

    if (exposed || fs_ret.success) {
        emptySlot(slot, fs_ret.cleanupInfo);
    }
    return EXCEPTION_NONE;
}

static void emptySlot(cte_t *slot, cap_t cleanupInfo)
{
    if (cap_get_capType(slot->cap) != cap_null_cap) {
        mdb_node_t mdbNode;
        cte_t *prev, *next;

        mdbNode = slot->cteMDBNode;
        prev = ((cte_t *)(mdb_node_get_mdbPrev(mdbNode)));
        next = ((cte_t *)(mdb_node_get_mdbNext(mdbNode)));

        if (prev) {
            mdb_node_ptr_set_mdbNext(&prev->cteMDBNode, ((word_t)(next)));
        }
        if (next) {
            mdb_node_ptr_set_mdbPrev(&next->cteMDBNode, ((word_t)(prev)));
        }
        if (next)
            mdb_node_ptr_set_mdbFirstBadged(&next->cteMDBNode,
                                            mdb_node_get_mdbFirstBadged(next->cteMDBNode) ||
                                            mdb_node_get_mdbFirstBadged(mdbNode));
        slot->cap = cap_null_cap_new();
        slot->cteMDBNode = mdb_node_new(0, false, false, 0);

        postCapDeletion(cleanupInfo);
    }
}

static inline bool_t __attribute__((__const__)) capRemovable(cap_t cap, cte_t *slot)
{
    switch (cap_get_capType(cap)) {
    case cap_null_cap:
        return true;
    case cap_zombie_cap: {
        word_t n = cap_zombie_cap_get_capZombieNumber(cap);
        cte_t *z_slot = (cte_t *)cap_zombie_cap_get_capZombiePtr(cap);
        return (n == 0 || (n == 1 && slot == z_slot));
    }
    default:
        _fail("finaliseCap should only return Zombie or NullCap", "/home/b/test/sel4test/kernel/src/object/cnode.c", 605, __func__);
    }
}

static inline bool_t __attribute__((__const__)) capCyclicZombie(cap_t cap, cte_t *slot)
{
    return cap_get_capType(cap) == cap_zombie_cap &&
           ((cte_t *)(cap_zombie_cap_get_capZombiePtr(cap))) == slot;
}

static finaliseSlot_ret_t finaliseSlot(cte_t *slot, bool_t immediate)
{
    bool_t final;
    finaliseCap_ret_t fc_ret;
    exception_t status;
    finaliseSlot_ret_t ret;

    while (cap_get_capType(slot->cap) != cap_null_cap) {
        final = isFinalCapability(slot);
        fc_ret = finaliseCap(slot->cap, final, false);

        if (capRemovable(fc_ret.remainder, slot)) {
            ret.status = EXCEPTION_NONE;
            ret.success = true;
            ret.cleanupInfo = fc_ret.cleanupInfo;
            return ret;
        }

        slot->cap = fc_ret.remainder;

        if (!immediate && capCyclicZombie(fc_ret.remainder, slot)) {
            ret.status = EXCEPTION_NONE;
            ret.success = false;
            ret.cleanupInfo = fc_ret.cleanupInfo;
            return ret;
        }

        status = reduceZombie(slot, immediate);
        if (status != EXCEPTION_NONE) {
            ret.status = status;
            ret.success = false;
            ret.cleanupInfo = cap_null_cap_new();
            return ret;
        }

        status = preemptionPoint();
        if (status != EXCEPTION_NONE) {
            ret.status = status;
            ret.success = false;
            ret.cleanupInfo = cap_null_cap_new();
            return ret;
        }
    }
    ret.status = EXCEPTION_NONE;
    ret.success = true;
    ret.cleanupInfo = cap_null_cap_new();
    return ret;
}

static exception_t reduceZombie(cte_t *slot, bool_t immediate)
{
    cte_t *ptr;
    word_t n, type;
    exception_t status;

    do { if (!(cap_get_capType(slot->cap) == cap_zombie_cap)) { _assert_fail("cap_get_capType(slot->cap) == cap_zombie_cap", "/home/b/test/sel4test/kernel/src/object/cnode.c", 670, __func__); } } while(0);
    ptr = (cte_t *)cap_zombie_cap_get_capZombiePtr(slot->cap);
    n = cap_zombie_cap_get_capZombieNumber(slot->cap);
    type = cap_zombie_cap_get_capZombieType(slot->cap);

    /* Haskell error: "reduceZombie: expected unremovable zombie" */
    do { if (!(n > 0)) { _assert_fail("n > 0", "/home/b/test/sel4test/kernel/src/object/cnode.c", 676, __func__); } } while(0);

    if (immediate) {
        cte_t *endSlot = &ptr[n - 1];

        status = cteDelete(endSlot, false);
        if (status != EXCEPTION_NONE) {
            return status;
        }

        switch (cap_get_capType(slot->cap)) {
        case cap_null_cap:
            break;

        case cap_zombie_cap: {
            cte_t *ptr2 =
                (cte_t *)cap_zombie_cap_get_capZombiePtr(slot->cap);

            if (ptr == ptr2 &&
                cap_zombie_cap_get_capZombieNumber(slot->cap) == n &&
                cap_zombie_cap_get_capZombieType(slot->cap) == type) {
                do { if (!(cap_get_capType(endSlot->cap) == cap_null_cap)) { _assert_fail("cap_get_capType(endSlot->cap) == cap_null_cap", "/home/b/test/sel4test/kernel/src/object/cnode.c", 697, __func__); } } while(0);
                slot->cap =
                    cap_zombie_cap_set_capZombieNumber(slot->cap, n - 1);
            } else {
                /* Haskell error:
                 * "Expected new Zombie to be self-referential."
                 */
                do { if (!(ptr2 == slot && ptr != slot)) { _assert_fail("ptr2 == slot && ptr != slot", "/home/b/test/sel4test/kernel/src/object/cnode.c", 704, __func__); } } while(0);
            }
            break;
        }

        default:
            _fail("Expected recursion to result in Zombie.", "/home/b/test/sel4test/kernel/src/object/cnode.c", 710, __func__);
        }
    } else {
        /* Haskell error: "Cyclic zombie passed to unexposed reduceZombie" */
        do { if (!(ptr != slot)) { _assert_fail("ptr != slot", "/home/b/test/sel4test/kernel/src/object/cnode.c", 714, __func__); } } while(0);

        if (cap_get_capType(ptr->cap) == cap_zombie_cap) {
            /* Haskell error: "Moving self-referential Zombie aside." */
            do { if (!(ptr != ((cte_t *)(cap_zombie_cap_get_capZombiePtr(ptr->cap))))) { _assert_fail("ptr != CTE_PTR(cap_zombie_cap_get_capZombiePtr(ptr->cap))", "/home/b/test/sel4test/kernel/src/object/cnode.c", 718, __func__); } } while(0);
        }

        capSwapForDelete(ptr, slot);
    }
    return EXCEPTION_NONE;
}

void cteDeleteOne(cte_t *slot)
{
    word_t cap_type = cap_get_capType(slot->cap);
    if (cap_type != cap_null_cap) {
        bool_t final;
        finaliseCap_ret_t fc_ret __attribute__((unused));

        /** GHOSTUPD: "(gs_get_assn cteDeleteOne_'proc \<acute>ghost'state = (-1)
            \<or> gs_get_assn cteDeleteOne_'proc \<acute>ghost'state = \<acute>cap_type, id)" */

        final = isFinalCapability(slot);
        fc_ret = finaliseCap(slot->cap, final, true);
        /* Haskell error: "cteDeleteOne: cap should be removable" */
        do { if (!(capRemovable(fc_ret.remainder, slot) && cap_get_capType(fc_ret.cleanupInfo) == cap_null_cap)) { _assert_fail("capRemovable(fc_ret.remainder, slot) && cap_get_capType(fc_ret.cleanupInfo) == cap_null_cap", "/home/b/test/sel4test/kernel/src/object/cnode.c", 739, __func__); } } while(0)
                                                                   ;
        emptySlot(slot, cap_null_cap_new());
    }
}

void insertNewCap(cte_t *parent, cte_t *slot, cap_t cap)
{
    cte_t *next;

    next = ((cte_t *)(mdb_node_get_mdbNext(parent->cteMDBNode)));
    slot->cap = cap;
    slot->cteMDBNode = mdb_node_new(((word_t)(next)), true, true, ((word_t)(parent)));
    if (next) {
        mdb_node_ptr_set_mdbPrev(&next->cteMDBNode, ((word_t)(slot)));
    }
    mdb_node_ptr_set_mdbNext(&parent->cteMDBNode, ((word_t)(slot)));
}


void setupReplyMaster(tcb_t *thread)
{
    cte_t *slot;

    slot = (((cte_t *)((word_t)(thread)&~((1ul << (12)) - 1ul)))+(tcbReply));
    if (cap_get_capType(slot->cap) == cap_null_cap) {
        /* Haskell asserts that no reply caps exist for this thread here. This
         * cannot be translated. */
        slot->cap = cap_reply_cap_new(true, true, ((word_t)(thread)));
        slot->cteMDBNode = mdb_node_new(0, false, false, 0);
        mdb_node_ptr_set_mdbRevocable(&slot->cteMDBNode, true);
        mdb_node_ptr_set_mdbFirstBadged(&slot->cteMDBNode, true);
    }
}


bool_t __attribute__((__pure__)) isMDBParentOf(cte_t *cte_a, cte_t *cte_b)
{
    if (!mdb_node_get_mdbRevocable(cte_a->cteMDBNode)) {
        return false;
    }
    if (!sameRegionAs(cte_a->cap, cte_b->cap)) {
        return false;
    }
    /* We want to take care of arch caps first, because that is easier in the proofs.
       We expect Arch_isMDBParentOf to return true if both caps are not architecture
       specific. */
    if (!Arch_isMDBParentOf(cte_a->cap, cte_b->cap,
                            mdb_node_get_mdbFirstBadged(cte_b->cteMDBNode))) {
        return false;
    }
    switch (cap_get_capType(cte_a->cap)) {
    case cap_endpoint_cap: {
        word_t badge;

        badge = cap_endpoint_cap_get_capEPBadge(cte_a->cap);
        if (badge == 0) {
            return true;
        }
        return (badge == cap_endpoint_cap_get_capEPBadge(cte_b->cap)) &&
               !mdb_node_get_mdbFirstBadged(cte_b->cteMDBNode);
        break;
    }

    case cap_notification_cap: {
        word_t badge;

        badge = cap_notification_cap_get_capNtfnBadge(cte_a->cap);
        if (badge == 0) {
            return true;
        }
        return (badge == cap_notification_cap_get_capNtfnBadge(cte_b->cap)) &&
               !mdb_node_get_mdbFirstBadged(cte_b->cteMDBNode);
        break;
    }

    default:
        return true;
        break;
    }
}

exception_t ensureNoChildren(cte_t *slot)
{
    if (mdb_node_get_mdbNext(slot->cteMDBNode) != 0) {
        cte_t *next;

        next = ((cte_t *)(mdb_node_get_mdbNext(slot->cteMDBNode)));
        if (isMDBParentOf(slot, next)) {
            current_syscall_error.type = seL4_RevokeFirst;
            return EXCEPTION_SYSCALL_ERROR;
        }
    }

    return EXCEPTION_NONE;
}

exception_t ensureEmptySlot(cte_t *slot)
{
    if (cap_get_capType(slot->cap) != cap_null_cap) {
        current_syscall_error.type = seL4_DeleteFirst;
        return EXCEPTION_SYSCALL_ERROR;
    }

    return EXCEPTION_NONE;
}

bool_t __attribute__((__pure__)) isFinalCapability(cte_t *cte)
{
    mdb_node_t mdb;
    bool_t prevIsSameObject;

    mdb = cte->cteMDBNode;

    if (mdb_node_get_mdbPrev(mdb) == 0) {
        prevIsSameObject = false;
    } else {
        cte_t *prev;

        prev = ((cte_t *)(mdb_node_get_mdbPrev(mdb)));
        prevIsSameObject = sameObjectAs(prev->cap, cte->cap);
    }

    if (prevIsSameObject) {
        return false;
    } else {
        if (mdb_node_get_mdbNext(mdb) == 0) {
            return true;
        } else {
            cte_t *next;

            next = ((cte_t *)(mdb_node_get_mdbNext(mdb)));
            return !sameObjectAs(cte->cap, next->cap);
        }
    }
}

bool_t __attribute__((__pure__)) slotCapLongRunningDelete(cte_t *slot)
{
    if (cap_get_capType(slot->cap) == cap_null_cap) {
        return false;
    } else if (! isFinalCapability(slot)) {
        return false;
    }
    switch (cap_get_capType(slot->cap)) {
    case cap_thread_cap:
    case cap_zombie_cap:
    case cap_cnode_cap:
        return true;
    default:
        return false;
    }
}

/* This implementation is specialised to the (current) limit
 * of one cap receive slot. */
cte_t *getReceiveSlots(tcb_t *thread, word_t *buffer)
{
    cap_transfer_t ct;
    cptr_t cptr;
    lookupCap_ret_t luc_ret;
    lookupSlot_ret_t lus_ret;
    cte_t *slot;
    cap_t cnode;

    if (!buffer) {
        return ((void *)0);
    }

    ct = loadCapTransfer(buffer);
    cptr = ct.ctReceiveRoot;

    luc_ret = lookupCap(thread, cptr);
    if (luc_ret.status != EXCEPTION_NONE) {
        return ((void *)0);
    }
    cnode = luc_ret.cap;

    lus_ret = lookupTargetSlot(cnode, ct.ctReceiveIndex, ct.ctReceiveDepth);
    if (lus_ret.status != EXCEPTION_NONE) {
        return ((void *)0);
    }
    slot = lus_ret.slot;

    if (cap_get_capType(slot->cap) != cap_null_cap) {
        return ((void *)0);
    }

    return slot;
}

cap_transfer_t __attribute__((__pure__)) loadCapTransfer(word_t *buffer)
{
    const int offset = seL4_MsgMaxLength + ((1ul<<(seL4_MsgExtraCapBits))-1) + 2;
    return capTransferFromWords(buffer + offset);
}
# 1 "/home/b/test/sel4test/kernel/src/object/endpoint.c"
/*
 * Copyright 2014, General Dynamics C4 Systems
 *
 * SPDX-License-Identifier: GPL-2.0-only
 */
# 23 "/home/b/test/sel4test/kernel/src/object/endpoint.c"
void sendIPC(bool_t blocking, bool_t do_call, word_t badge,
             bool_t canGrant, bool_t canGrantReply, tcb_t *thread, endpoint_t *epptr)

{
    switch (endpoint_ptr_get_state(epptr)) {
    case EPState_Idle:
    case EPState_Send:
        if (blocking) {
            tcb_queue_t queue;

            /* Set thread state to BlockedOnSend */
            thread_state_ptr_set_tsType(&thread->tcbState,
                                        ThreadState_BlockedOnSend);
            thread_state_ptr_set_blockingObject(
                &thread->tcbState, ((word_t)(epptr)));
            thread_state_ptr_set_blockingIPCBadge(
                &thread->tcbState, badge);
            thread_state_ptr_set_blockingIPCCanGrant(
                &thread->tcbState, canGrant);
            thread_state_ptr_set_blockingIPCCanGrantReply(
                &thread->tcbState, canGrantReply);
            thread_state_ptr_set_blockingIPCIsCall(
                &thread->tcbState, do_call);

            scheduleTCB(thread);

            /* Place calling thread in endpoint queue */
            queue = ep_ptr_get_queue(epptr);
            queue = tcbEPAppend(thread, queue);
            endpoint_ptr_set_state(epptr, EPState_Send);
            ep_ptr_set_queue(epptr, queue);
        }
        break;

    case EPState_Recv: {
        tcb_queue_t queue;
        tcb_t *dest;

        /* Get the head of the endpoint queue. */
        queue = ep_ptr_get_queue(epptr);
        dest = queue.head;

        /* Haskell error "Receive endpoint queue must not be empty" */
        do { if (!(dest)) { _assert_fail("dest", "/home/b/test/sel4test/kernel/src/object/endpoint.c", 66, __func__); } } while(0);

        /* Dequeue the first TCB */
        queue = tcbEPDequeue(dest, queue);
        ep_ptr_set_queue(epptr, queue);

        if (!queue.head) {
            endpoint_ptr_set_state(epptr, EPState_Idle);
        }

        /* Do the transfer */
        doIPCTransfer(thread, epptr, badge, canGrant, dest);
# 105 "/home/b/test/sel4test/kernel/src/object/endpoint.c"
        bool_t replyCanGrant = thread_state_ptr_get_blockingIPCCanGrant(&dest->tcbState);;

        setThreadState(dest, ThreadState_Running);
        possibleSwitchTo(dest);

        if (do_call) {
            if (canGrant || canGrantReply) {
                setupCallerCap(thread, dest, replyCanGrant);
            } else {
                setThreadState(thread, ThreadState_Inactive);
            }
        }

        break;
    }
    }
}




void receiveIPC(tcb_t *thread, cap_t cap, bool_t isBlocking)

{
    endpoint_t *epptr;
    notification_t *ntfnPtr;

    /* Haskell error "receiveIPC: invalid cap" */
    do { if (!(cap_get_capType(cap) == cap_endpoint_cap)) { _assert_fail("cap_get_capType(cap) == cap_endpoint_cap", "/home/b/test/sel4test/kernel/src/object/endpoint.c", 133, __func__); } } while(0);

    epptr = ((endpoint_t *)(cap_endpoint_cap_get_capEPPtr(cap)));
# 148 "/home/b/test/sel4test/kernel/src/object/endpoint.c"
    /* Check for anything waiting in the notification */
    ntfnPtr = thread->tcbBoundNotification;
    if (ntfnPtr && notification_ptr_get_state(ntfnPtr) == NtfnState_Active) {
        completeSignal(ntfnPtr, thread);
    } else {
# 163 "/home/b/test/sel4test/kernel/src/object/endpoint.c"
        switch (endpoint_ptr_get_state(epptr)) {
        case EPState_Idle:
        case EPState_Recv: {
            tcb_queue_t queue;

            if (isBlocking) {
                /* Set thread state to BlockedOnReceive */
                thread_state_ptr_set_tsType(&thread->tcbState,
                                            ThreadState_BlockedOnReceive);
                thread_state_ptr_set_blockingObject(
                    &thread->tcbState, ((word_t)(epptr)));






                thread_state_ptr_set_blockingIPCCanGrant(
                    &thread->tcbState, cap_endpoint_cap_get_capCanGrant(cap));

                scheduleTCB(thread);

                /* Place calling thread in endpoint queue */
                queue = ep_ptr_get_queue(epptr);
                queue = tcbEPAppend(thread, queue);
                endpoint_ptr_set_state(epptr, EPState_Recv);
                ep_ptr_set_queue(epptr, queue);
            } else {
                doNBRecvFailedTransfer(thread);
            }
            break;
        }

        case EPState_Send: {
            tcb_queue_t queue;
            tcb_t *sender;
            word_t badge;
            bool_t canGrant;
            bool_t canGrantReply;
            bool_t do_call;

            /* Get the head of the endpoint queue. */
            queue = ep_ptr_get_queue(epptr);
            sender = queue.head;

            /* Haskell error "Send endpoint queue must not be empty" */
            do { if (!(sender)) { _assert_fail("sender", "/home/b/test/sel4test/kernel/src/object/endpoint.c", 209, __func__); } } while(0);

            /* Dequeue the first TCB */
            queue = tcbEPDequeue(sender, queue);
            ep_ptr_set_queue(epptr, queue);

            if (!queue.head) {
                endpoint_ptr_set_state(epptr, EPState_Idle);
            }

            /* Get sender IPC details */
            badge = thread_state_ptr_get_blockingIPCBadge(&sender->tcbState);
            canGrant =
                thread_state_ptr_get_blockingIPCCanGrant(&sender->tcbState);
            canGrantReply =
                thread_state_ptr_get_blockingIPCCanGrantReply(&sender->tcbState);

            /* Do the transfer */
            doIPCTransfer(sender, epptr, badge,
                          canGrant, thread);

            do_call = thread_state_ptr_get_blockingIPCIsCall(&sender->tcbState);
# 261 "/home/b/test/sel4test/kernel/src/object/endpoint.c"
            if (do_call) {
                if (canGrant || canGrantReply) {
                    setupCallerCap(sender, thread, cap_endpoint_cap_get_capCanGrant(cap));
                } else {
                    setThreadState(sender, ThreadState_Inactive);
                }
            } else {
                setThreadState(sender, ThreadState_Running);
                possibleSwitchTo(sender);
            }

            break;
        }
        }
    }
}

void replyFromKernel_error(tcb_t *thread)
{
    word_t len;
    word_t *ipcBuffer;

    ipcBuffer = lookupIPCBuffer(true, thread);
    setRegister(thread, badgeRegister, 0);
    len = setMRs_syscall_error(thread, ipcBuffer);
# 295 "/home/b/test/sel4test/kernel/src/object/endpoint.c"
    setRegister(thread, msgInfoRegister, wordFromMessageInfo(
                    seL4_MessageInfo_new(current_syscall_error.type, 0, 0, len)));
}

void replyFromKernel_success_empty(tcb_t *thread)
{
    setRegister(thread, badgeRegister, 0);
    setRegister(thread, msgInfoRegister, wordFromMessageInfo(
                    seL4_MessageInfo_new(0, 0, 0, 0)));
}

void cancelIPC(tcb_t *tptr)
{
    thread_state_t *state = &tptr->tcbState;






    switch (thread_state_ptr_get_tsType(state)) {
    case ThreadState_BlockedOnSend:
    case ThreadState_BlockedOnReceive: {
        /* blockedIPCCancel state */
        endpoint_t *epptr;
        tcb_queue_t queue;

        epptr = ((endpoint_t *)(thread_state_ptr_get_blockingObject(state)));

        /* Haskell error "blockedIPCCancel: endpoint must not be idle" */
        do { if (!(endpoint_ptr_get_state(epptr) != EPState_Idle)) { _assert_fail("endpoint_ptr_get_state(epptr) != EPState_Idle", "/home/b/test/sel4test/kernel/src/object/endpoint.c", 325, __func__); } } while(0);

        /* Dequeue TCB */
        queue = ep_ptr_get_queue(epptr);
        queue = tcbEPDequeue(tptr, queue);
        ep_ptr_set_queue(epptr, queue);

        if (!queue.head) {
            endpoint_ptr_set_state(epptr, EPState_Idle);
        }
# 344 "/home/b/test/sel4test/kernel/src/object/endpoint.c"
        setThreadState(tptr, ThreadState_Inactive);
        break;
    }

    case ThreadState_BlockedOnNotification:
        cancelSignal(tptr,
                     ((notification_t *)(thread_state_ptr_get_blockingObject(state))));
        break;

    case ThreadState_BlockedOnReply: {



        cte_t *slot, *callerCap;

        tptr->tcbFault = seL4_Fault_NullFault_new();

        /* Get the reply cap slot */
        slot = (((cte_t *)((word_t)(tptr)&~((1ul << (12)) - 1ul)))+(tcbReply));

        callerCap = ((cte_t *)(mdb_node_get_mdbNext(slot->cteMDBNode)));
        if (callerCap) {
            /** GHOSTUPD: "(True,
                gs_set_assn cteDeleteOne_'proc (ucast cap_reply_cap))" */
            cteDeleteOne(callerCap);
        }


        break;
    }
    }
}
# 400 "/home/b/test/sel4test/kernel/src/object/endpoint.c"
void cancelAllIPC(endpoint_t *epptr)
{
    switch (endpoint_ptr_get_state(epptr)) {
    case EPState_Idle:
        break;

    default: {
        tcb_t *thread = ((tcb_t *)(endpoint_ptr_get_epQueue_head(epptr)));

        /* Make endpoint idle */
        endpoint_ptr_set_state(epptr, EPState_Idle);
        endpoint_ptr_set_epQueue_head(epptr, 0);
        endpoint_ptr_set_epQueue_tail(epptr, 0);

        /* Set all blocked threads to restart */
        for (; thread; thread = thread->tcbEPNext) {
# 425 "/home/b/test/sel4test/kernel/src/object/endpoint.c"
            setThreadState(thread, ThreadState_Restart);
            tcbSchedEnqueue(thread);

        }

        rescheduleRequired();
        break;
    }
    }
}

void cancelBadgedSends(endpoint_t *epptr, word_t badge)
{
    switch (endpoint_ptr_get_state(epptr)) {
    case EPState_Idle:
    case EPState_Recv:
        break;

    case EPState_Send: {
        tcb_t *thread, *next;
        tcb_queue_t queue = ep_ptr_get_queue(epptr);

        /* this is a de-optimisation for verification
         * reasons. it allows the contents of the endpoint
         * queue to be ignored during the for loop. */
        endpoint_ptr_set_state(epptr, EPState_Idle);
        endpoint_ptr_set_epQueue_head(epptr, 0);
        endpoint_ptr_set_epQueue_tail(epptr, 0);

        for (thread = queue.head; thread; thread = next) {
            word_t b = thread_state_ptr_get_blockingIPCBadge(
                           &thread->tcbState);
            next = thread->tcbEPNext;
# 466 "/home/b/test/sel4test/kernel/src/object/endpoint.c"
            if (b == badge) {
                setThreadState(thread, ThreadState_Restart);
                tcbSchedEnqueue(thread);
                queue = tcbEPDequeue(thread, queue);
            }

        }
        ep_ptr_set_queue(epptr, queue);

        if (queue.head) {
            endpoint_ptr_set_state(epptr, EPState_Send);
        }

        rescheduleRequired();

        break;
    }

    default:
        _fail("invalid EP state", "/home/b/test/sel4test/kernel/src/object/endpoint.c", 485, __func__);
    }
}
# 1 "/home/b/test/sel4test/kernel/src/object/interrupt.c"
/*
 * Copyright 2014, General Dynamics C4 Systems
 *
 * SPDX-License-Identifier: GPL-2.0-only
 */
# 23 "/home/b/test/sel4test/kernel/src/object/interrupt.c"
exception_t decodeIRQControlInvocation(word_t invLabel, word_t length,
                                       cte_t *srcSlot, word_t *buffer)
{
    if (invLabel == IRQIssueIRQHandler) {
        word_t index, depth, irq_w;
        irq_t irq;
        cte_t *destSlot;
        cap_t cnodeCap;
        lookupSlot_ret_t lu_ret;
        exception_t status;

        if (length < 3 || current_extra_caps.excaprefs[0] == ((void *)0)) {
            current_syscall_error.type = seL4_TruncatedMessage;
            return EXCEPTION_SYSCALL_ERROR;
        }
        irq_w = getSyscallArg(0, buffer);
        irq = (irq_w);
        index = getSyscallArg(1, buffer);
        depth = getSyscallArg(2, buffer);

        cnodeCap = current_extra_caps.excaprefs[0]->cap;

        status = Arch_checkIRQ(irq_w);
        if (status != EXCEPTION_NONE) {
            return status;
        }

        if (isIRQActive(irq)) {
            current_syscall_error.type = seL4_RevokeFirst;
            do { printf("\033[0m" "\033[1m" "<<" "\033[0m" "\033[32m" "seL4(CPU %" "lu" ")" "\033[0m" "\033[1m" " [%s/%d T%p \"%s\" @%lx]: " "Rejecting request for IRQ %u. Already active." ">>" "\033[0m" "\n", 0lu, __func__, 52, ksCurThread, ((debug_tcb_t *)(((cte_t *)((word_t)(ksCurThread)&~((1ul << (12)) - 1ul)))+(tcbCNodeEntries)))->tcbName, (word_t)getRestartPC(ksCurThread), (int)(irq)); } while (0);
            return EXCEPTION_SYSCALL_ERROR;
        }

        lu_ret = lookupTargetSlot(cnodeCap, index, depth);
        if (lu_ret.status != EXCEPTION_NONE) {
            do { printf("\033[0m" "\033[1m" "<<" "\033[0m" "\033[32m" "seL4(CPU %" "lu" ")" "\033[0m" "\033[1m" " [%s/%d T%p \"%s\" @%lx]: " "Target slot for new IRQ Handler cap invalid: cap %lu, IRQ %u." ">>" "\033[0m" "\n", 0lu, __func__, 58, ksCurThread, ((debug_tcb_t *)(((cte_t *)((word_t)(ksCurThread)&~((1ul << (12)) - 1ul)))+(tcbCNodeEntries)))->tcbName, (word_t)getRestartPC(ksCurThread), getExtraCPtr(buffer, 0), (int)(irq)); } while (0)
                                                                     ;
            return lu_ret.status;
        }
        destSlot = lu_ret.slot;

        status = ensureEmptySlot(destSlot);
        if (status != EXCEPTION_NONE) {
            do { printf("\033[0m" "\033[1m" "<<" "\033[0m" "\033[32m" "seL4(CPU %" "lu" ")" "\033[0m" "\033[1m" " [%s/%d T%p \"%s\" @%lx]: " "Target slot for new IRQ Handler cap not empty: cap %lu, IRQ %u." ">>" "\033[0m" "\n", 0lu, __func__, 66, ksCurThread, ((debug_tcb_t *)(((cte_t *)((word_t)(ksCurThread)&~((1ul << (12)) - 1ul)))+(tcbCNodeEntries)))->tcbName, (word_t)getRestartPC(ksCurThread), getExtraCPtr(buffer, 0), (int)(irq)); } while (0)
                                                                     ;
            return status;
        }

        setThreadState(ksCurThread, ThreadState_Restart);
        return invokeIRQControl(irq, destSlot, srcSlot);
    } else {
        return Arch_decodeIRQControlInvocation(invLabel, length, srcSlot, buffer);
    }
}

exception_t invokeIRQControl(irq_t irq, cte_t *handlerSlot, cte_t *controlSlot)
{
    setIRQState(IRQSignal, irq);
    cteInsert(cap_irq_handler_cap_new((irq)), controlSlot, handlerSlot);

    return EXCEPTION_NONE;
}

exception_t decodeIRQHandlerInvocation(word_t invLabel, irq_t irq)
{
    switch (invLabel) {
    case IRQAckIRQ:
        setThreadState(ksCurThread, ThreadState_Restart);
        invokeIRQHandler_AckIRQ(irq);
        return EXCEPTION_NONE;

    case IRQSetIRQHandler: {
        cap_t ntfnCap;
        cte_t *slot;

        if (current_extra_caps.excaprefs[0] == ((void *)0)) {
            current_syscall_error.type = seL4_TruncatedMessage;
            return EXCEPTION_SYSCALL_ERROR;
        }
        ntfnCap = current_extra_caps.excaprefs[0]->cap;
        slot = current_extra_caps.excaprefs[0];

        if (cap_get_capType(ntfnCap) != cap_notification_cap ||
            !cap_notification_cap_get_capNtfnCanSend(ntfnCap)) {
            if (cap_get_capType(ntfnCap) != cap_notification_cap) {
                do { printf("\033[0m" "\033[1m" "<<" "\033[0m" "\033[32m" "seL4(CPU %" "lu" ")" "\033[0m" "\033[1m" " [%s/%d T%p \"%s\" @%lx]: " "IRQSetHandler: provided cap is not an notification capability." ">>" "\033[0m" "\n", 0lu, __func__, 108, ksCurThread, ((debug_tcb_t *)(((cte_t *)((word_t)(ksCurThread)&~((1ul << (12)) - 1ul)))+(tcbCNodeEntries)))->tcbName, (word_t)getRestartPC(ksCurThread)); } while (0);
            } else {
                do { printf("\033[0m" "\033[1m" "<<" "\033[0m" "\033[32m" "seL4(CPU %" "lu" ")" "\033[0m" "\033[1m" " [%s/%d T%p \"%s\" @%lx]: " "IRQSetHandler: caller does not have send rights on the endpoint." ">>" "\033[0m" "\n", 0lu, __func__, 110, ksCurThread, ((debug_tcb_t *)(((cte_t *)((word_t)(ksCurThread)&~((1ul << (12)) - 1ul)))+(tcbCNodeEntries)))->tcbName, (word_t)getRestartPC(ksCurThread)); } while (0);
            }
            current_syscall_error.type = seL4_InvalidCapability;
            current_syscall_error.invalidCapNumber = 0;
            return EXCEPTION_SYSCALL_ERROR;
        }

        setThreadState(ksCurThread, ThreadState_Restart);
        invokeIRQHandler_SetIRQHandler(irq, ntfnCap, slot);
        return EXCEPTION_NONE;
    }

    case IRQClearIRQHandler:
        setThreadState(ksCurThread, ThreadState_Restart);
        invokeIRQHandler_ClearIRQHandler(irq);
        return EXCEPTION_NONE;

    default:
        do { printf("\033[0m" "\033[1m" "<<" "\033[0m" "\033[32m" "seL4(CPU %" "lu" ")" "\033[0m" "\033[1m" " [%s/%d T%p \"%s\" @%lx]: " "IRQHandler: Illegal operation." ">>" "\033[0m" "\n", 0lu, __func__, 128, ksCurThread, ((debug_tcb_t *)(((cte_t *)((word_t)(ksCurThread)&~((1ul << (12)) - 1ul)))+(tcbCNodeEntries)))->tcbName, (word_t)getRestartPC(ksCurThread)); } while (0);
        current_syscall_error.type = seL4_IllegalOperation;
        return EXCEPTION_SYSCALL_ERROR;
    }
}

void invokeIRQHandler_AckIRQ(irq_t irq)
{
# 159 "/home/b/test/sel4test/kernel/src/object/interrupt.c"
    if (wrap_config_set(1)) {
        deactivateInterrupt(irq);
    } else {
        maskInterrupt(false, irq);
    }

}

void invokeIRQHandler_SetIRQHandler(irq_t irq, cap_t cap, cte_t *slot)
{
    cte_t *irqSlot;

    irqSlot = intStateIRQNode + (irq);
    /** GHOSTUPD: "(True, gs_set_assn cteDeleteOne_'proc (-1))" */
    cteDeleteOne(irqSlot);
    cteInsert(cap, slot, irqSlot);
}

void invokeIRQHandler_ClearIRQHandler(irq_t irq)
{
    cte_t *irqSlot;

    irqSlot = intStateIRQNode + (irq);
    /** GHOSTUPD: "(True, gs_set_assn cteDeleteOne_'proc (-1))" */
    cteDeleteOne(irqSlot);
}

void deletingIRQHandler(irq_t irq)
{
    cte_t *slot;

    slot = intStateIRQNode + (irq);
    /** GHOSTUPD: "(True, gs_set_assn cteDeleteOne_'proc (ucast cap_notification_cap))" */
    cteDeleteOne(slot);
}

void deletedIRQHandler(irq_t irq)
{
    setIRQState(IRQInactive, irq);
}

void handleInterrupt(irq_t irq)
{
    if (__builtin_expect(!!((irq) > maxIRQ), 0)) {
        /* The interrupt number is out of range. Pretend it did not happen by
         * handling it like an inactive interrupt (mask and ack). We assume this
         * is acceptable, because the platform specific interrupt controller
         * driver reported this interrupt. Maybe the value maxIRQ is just wrong
         * or set to a lower value because the interrupts are unused.
         */
        printf("Received IRQ %d, which is above the platforms maxIRQ of %d\n", (int)(irq), (int)maxIRQ);
        maskInterrupt(true, irq);
        ackInterrupt(irq);
        return;
    }

    switch (intStateIRQTable[(irq)]) {
    case IRQSignal: {
        /* Merging the variable declaration and initialization into one line
         * requires an update in the proofs first. Might be a c89 legacy.
         */
        cap_t cap;
        cap = intStateIRQNode[(irq)].cap;
        if (cap_get_capType(cap) == cap_notification_cap &&
            cap_notification_cap_get_capNtfnCanSend(cap)) {
            sendSignal(((notification_t *)(cap_notification_cap_get_capNtfnPtr(cap))),
                       cap_notification_cap_get_capNtfnBadge(cap));
        } else {

            printf("Undelivered IRQ: %d\n", (int)(irq));

        }

        if (!wrap_config_set(1)) {
            maskInterrupt(true, irq);
        }


        break;
    }

    case IRQTimer:




        timerTick();
        resetTimer();

        break;







    case IRQReserved:
        handleReservedIRQ(irq);
        break;

    case IRQInactive:
        /* This case shouldn't happen anyway unless the hardware or platform
         * code is broken. Hopefully masking it again should make the interrupt
         * go away.
         */
        maskInterrupt(true, irq);

        printf("Received disabled IRQ: %d\n", (int)(irq));

        break;

    default:
        /* No corresponding haskell error */
        _fail("Invalid IRQ state", "/home/b/test/sel4test/kernel/src/object/interrupt.c", 273, __func__);
    }

    /* Every interrupt is ack'd, even if it is an inactive one. Rationale is,
     * that for any interrupt reported by the platform specific code the generic
     * kernel code does report here that it is done with handling it. */
    ackInterrupt(irq);
}

bool_t isIRQActive(irq_t irq)
{
    return intStateIRQTable[(irq)] != IRQInactive;
}

void setIRQState(irq_state_t irqState, irq_t irq)
{
    intStateIRQTable[(irq)] = irqState;






    maskInterrupt(irqState == IRQInactive, irq);
}
# 1 "/home/b/test/sel4test/kernel/src/object/notification.c"
/*
 * Copyright 2014, General Dynamics C4 Systems
 *
 * SPDX-License-Identifier: GPL-2.0-only
 */
# 19 "/home/b/test/sel4test/kernel/src/object/notification.c"
static inline tcb_queue_t __attribute__((__pure__)) ntfn_ptr_get_queue(notification_t *ntfnPtr)
{
    tcb_queue_t ntfn_queue;

    ntfn_queue.head = (tcb_t *)notification_ptr_get_ntfnQueue_head(ntfnPtr);
    ntfn_queue.end = (tcb_t *)notification_ptr_get_ntfnQueue_tail(ntfnPtr);

    return ntfn_queue;
}

static inline void ntfn_ptr_set_queue(notification_t *ntfnPtr, tcb_queue_t ntfn_queue)
{
    notification_ptr_set_ntfnQueue_head(ntfnPtr, (word_t)ntfn_queue.head);
    notification_ptr_set_ntfnQueue_tail(ntfnPtr, (word_t)ntfn_queue.end);
}
# 62 "/home/b/test/sel4test/kernel/src/object/notification.c"
void sendSignal(notification_t *ntfnPtr, word_t badge)
{
    switch (notification_ptr_get_state(ntfnPtr)) {
    case NtfnState_Idle: {
        tcb_t *tcb = (tcb_t *)notification_ptr_get_ntfnBoundTCB(ntfnPtr);
        /* Check if we are bound and that thread is waiting for a message */
        if (tcb) {
            if (thread_state_ptr_get_tsType(&tcb->tcbState) == ThreadState_BlockedOnReceive) {
                /* Send and start thread running */
                cancelIPC(tcb);
                setThreadState(tcb, ThreadState_Running);
                setRegister(tcb, badgeRegister, badge);
                { { possibleSwitchTo(tcb); } }
# 122 "/home/b/test/sel4test/kernel/src/object/notification.c"
            } else {
                /* In particular, this path is taken when a thread
                 * is waiting on a reply cap since BlockedOnReply
                 * would also trigger this path. I.e, a thread
                 * with a bound notification will not be awakened
                 * by signals on that bound notification if it is
                 * in the middle of an seL4_Call.
                 */
                ntfn_set_active(ntfnPtr, badge);
            }
        } else {
            ntfn_set_active(ntfnPtr, badge);
        }
        break;
    }
    case NtfnState_Waiting: {
        tcb_queue_t ntfn_queue;
        tcb_t *dest;

        ntfn_queue = ntfn_ptr_get_queue(ntfnPtr);
        dest = ntfn_queue.head;

        /* Haskell error "WaitingNtfn Notification must have non-empty queue" */
        do { if (!(dest)) { _assert_fail("dest", "/home/b/test/sel4test/kernel/src/object/notification.c", 145, __func__); } } while(0);

        /* Dequeue TCB */
        ntfn_queue = tcbEPDequeue(dest, ntfn_queue);
        ntfn_ptr_set_queue(ntfnPtr, ntfn_queue);

        /* set the thread state to idle if the queue is empty */
        if (!ntfn_queue.head) {
            notification_ptr_set_state(ntfnPtr, NtfnState_Idle);
        }

        setThreadState(dest, ThreadState_Running);
        setRegister(dest, badgeRegister, badge);
        { { possibleSwitchTo(dest); } }
# 175 "/home/b/test/sel4test/kernel/src/object/notification.c"
        break;
    }

    case NtfnState_Active: {
        word_t badge2;

        badge2 = notification_ptr_get_ntfnMsgIdentifier(ntfnPtr);
        badge2 |= badge;

        notification_ptr_set_ntfnMsgIdentifier(ntfnPtr, badge2);
        break;
    }
    }
}

void receiveSignal(tcb_t *thread, cap_t cap, bool_t isBlocking)
{
    notification_t *ntfnPtr;

    ntfnPtr = ((notification_t *)(cap_notification_cap_get_capNtfnPtr(cap)));

    switch (notification_ptr_get_state(ntfnPtr)) {
    case NtfnState_Idle:
    case NtfnState_Waiting: {
        tcb_queue_t ntfn_queue;

        if (isBlocking) {
            /* Block thread on notification object */
            thread_state_ptr_set_tsType(&thread->tcbState,
                                        ThreadState_BlockedOnNotification);
            thread_state_ptr_set_blockingObject(&thread->tcbState,
                                                ((word_t)(ntfnPtr)));
            scheduleTCB(thread);

            /* Enqueue TCB */
            ntfn_queue = ntfn_ptr_get_queue(ntfnPtr);
            ntfn_queue = tcbEPAppend(thread, ntfn_queue);

            notification_ptr_set_state(ntfnPtr, NtfnState_Waiting);
            ntfn_ptr_set_queue(ntfnPtr, ntfn_queue);




        } else {
            doNBRecvFailedTransfer(thread);
        }

        break;
    }

    case NtfnState_Active:
        setRegister(
            thread, badgeRegister,
            notification_ptr_get_ntfnMsgIdentifier(ntfnPtr));
        notification_ptr_set_state(ntfnPtr, NtfnState_Idle);
# 239 "/home/b/test/sel4test/kernel/src/object/notification.c"
        break;
    }
}

void cancelAllSignals(notification_t *ntfnPtr)
{
    if (notification_ptr_get_state(ntfnPtr) == NtfnState_Waiting) {
        tcb_t *thread = ((tcb_t *)(notification_ptr_get_ntfnQueue_head(ntfnPtr)));

        notification_ptr_set_state(ntfnPtr, NtfnState_Idle);
        notification_ptr_set_ntfnQueue_head(ntfnPtr, 0);
        notification_ptr_set_ntfnQueue_tail(ntfnPtr, 0);

        /* Set all waiting threads to Restart */
        for (; thread; thread = thread->tcbEPNext) {
            setThreadState(thread, ThreadState_Restart);
# 269 "/home/b/test/sel4test/kernel/src/object/notification.c"
            tcbSchedEnqueue(thread);

        }
        rescheduleRequired();
    }
}

void cancelSignal(tcb_t *threadPtr, notification_t *ntfnPtr)
{
    tcb_queue_t ntfn_queue;

    /* Haskell error "cancelSignal: notification object must be in a waiting" state */
    do { if (!(notification_ptr_get_state(ntfnPtr) == NtfnState_Waiting)) { _assert_fail("notification_ptr_get_state(ntfnPtr) == NtfnState_Waiting", "/home/b/test/sel4test/kernel/src/object/notification.c", 281, __func__); } } while(0);

    /* Dequeue TCB */
    ntfn_queue = ntfn_ptr_get_queue(ntfnPtr);
    ntfn_queue = tcbEPDequeue(threadPtr, ntfn_queue);
    ntfn_ptr_set_queue(ntfnPtr, ntfn_queue);

    /* Make notification object idle */
    if (!ntfn_queue.head) {
        notification_ptr_set_state(ntfnPtr, NtfnState_Idle);
    }

    /* Make thread inactive */
    setThreadState(threadPtr, ThreadState_Inactive);
}

void completeSignal(notification_t *ntfnPtr, tcb_t *tcb)
{
    word_t badge;

    if (__builtin_expect(!!(tcb && notification_ptr_get_state(ntfnPtr) == NtfnState_Active), 1)) {
        badge = notification_ptr_get_ntfnMsgIdentifier(ntfnPtr);
        setRegister(tcb, badgeRegister, badge);
        notification_ptr_set_state(ntfnPtr, NtfnState_Idle);
# 321 "/home/b/test/sel4test/kernel/src/object/notification.c"
    } else {
        _fail("tried to complete signal with inactive notification object", "/home/b/test/sel4test/kernel/src/object/notification.c", 322, __func__);
    }
}

static inline void doUnbindNotification(notification_t *ntfnPtr, tcb_t *tcbptr)
{
    notification_ptr_set_ntfnBoundTCB(ntfnPtr, (word_t) 0);
    tcbptr->tcbBoundNotification = ((void *)0);
}

void unbindMaybeNotification(notification_t *ntfnPtr)
{
    tcb_t *boundTCB;
    boundTCB = (tcb_t *)notification_ptr_get_ntfnBoundTCB(ntfnPtr);

    if (boundTCB) {
        doUnbindNotification(ntfnPtr, boundTCB);
    }
}

void unbindNotification(tcb_t *tcb)
{
    notification_t *ntfnPtr;
    ntfnPtr = tcb->tcbBoundNotification;

    if (ntfnPtr) {
        doUnbindNotification(ntfnPtr, tcb);
    }
}

void bindNotification(tcb_t *tcb, notification_t *ntfnPtr)
{
    notification_ptr_set_ntfnBoundTCB(ntfnPtr, (word_t)tcb);
    tcb->tcbBoundNotification = ntfnPtr;
}
# 1 "/home/b/test/sel4test/kernel/src/object/objecttype.c"
/*
 * Copyright 2014, General Dynamics C4 Systems
 *
 * SPDX-License-Identifier: GPL-2.0-only
 */
# 33 "/home/b/test/sel4test/kernel/src/object/objecttype.c"
word_t getObjectSize(word_t t, word_t userObjSize)
{
    if (t >= seL4_NonArchObjectTypeCount) {
        return Arch_getObjectSize(t);
    } else {
        switch (t) {
        case seL4_TCBObject:
            return 12;
        case seL4_EndpointObject:
            return 4;
        case seL4_NotificationObject:
            return 5;
        case seL4_CapTableObject:
            return 5 + userObjSize;
        case seL4_UntypedObject:
            return userObjSize;






        default:
            _fail("Invalid object type", "/home/b/test/sel4test/kernel/src/object/objecttype.c", 56, __func__);
            return 0;
        }
    }
}

deriveCap_ret_t deriveCap(cte_t *slot, cap_t cap)
{
    deriveCap_ret_t ret;

    if (isArchCap(cap)) {
        return Arch_deriveCap(slot, cap);
    }

    switch (cap_get_capType(cap)) {
    case cap_zombie_cap:
        ret.status = EXCEPTION_NONE;
        ret.cap = cap_null_cap_new();
        break;

    case cap_irq_control_cap:
        ret.status = EXCEPTION_NONE;
        ret.cap = cap_null_cap_new();
        break;

    case cap_untyped_cap:
        ret.status = ensureNoChildren(slot);
        if (ret.status != EXCEPTION_NONE) {
            ret.cap = cap_null_cap_new();
        } else {
            ret.cap = cap;
        }
        break;


    case cap_reply_cap:
        ret.status = EXCEPTION_NONE;
        ret.cap = cap_null_cap_new();
        break;

    default:
        ret.status = EXCEPTION_NONE;
        ret.cap = cap;
    }

    return ret;
}

finaliseCap_ret_t finaliseCap(cap_t cap, bool_t final, bool_t exposed)
{
    finaliseCap_ret_t fc_ret;

    if (isArchCap(cap)) {
        return Arch_finaliseCap(cap, final);
    }

    switch (cap_get_capType(cap)) {
    case cap_endpoint_cap:
        if (final) {
            cancelAllIPC(((endpoint_t *)(cap_endpoint_cap_get_capEPPtr(cap))));
        }

        fc_ret.remainder = cap_null_cap_new();
        fc_ret.cleanupInfo = cap_null_cap_new();
        return fc_ret;

    case cap_notification_cap:
        if (final) {
            notification_t *ntfn = ((notification_t *)(cap_notification_cap_get_capNtfnPtr(cap)));



            unbindMaybeNotification(ntfn);
            cancelAllSignals(ntfn);
        }
        fc_ret.remainder = cap_null_cap_new();
        fc_ret.cleanupInfo = cap_null_cap_new();
        return fc_ret;

    case cap_reply_cap:
# 156 "/home/b/test/sel4test/kernel/src/object/objecttype.c"
    case cap_null_cap:
    case cap_domain_cap:
        fc_ret.remainder = cap_null_cap_new();
        fc_ret.cleanupInfo = cap_null_cap_new();
        return fc_ret;
    }

    if (exposed) {
        _fail("finaliseCap: failed to finalise immediately.", "/home/b/test/sel4test/kernel/src/object/objecttype.c", 164, __func__);
    }

    switch (cap_get_capType(cap)) {
    case cap_cnode_cap: {
        if (final) {
            fc_ret.remainder =
                Zombie_new(
                    1ul << cap_cnode_cap_get_capCNodeRadix(cap),
                    cap_cnode_cap_get_capCNodeRadix(cap),
                    cap_cnode_cap_get_capCNodePtr(cap)
                );
            fc_ret.cleanupInfo = cap_null_cap_new();
            return fc_ret;
        }
        break;
    }

    case cap_thread_cap: {
        if (final) {
            tcb_t *tcb;
            cte_t *cte_ptr;

            tcb = ((tcb_t *)(cap_thread_cap_get_capTCBPtr(cap)));
           
            cte_ptr = (((cte_t *)((word_t)(tcb)&~((1ul << (12)) - 1ul)))+(tcbCTable));
            unbindNotification(tcb);
# 200 "/home/b/test/sel4test/kernel/src/object/objecttype.c"
            suspend(tcb);

            tcbDebugRemove(tcb);

            Arch_prepareThreadDelete(tcb);
            fc_ret.remainder =
                Zombie_new(
                    tcbCNodeEntries,
                    (1ul << (6)),
                    ((word_t)(cte_ptr))
                );
            fc_ret.cleanupInfo = cap_null_cap_new();
            return fc_ret;
        }
        break;
    }
# 241 "/home/b/test/sel4test/kernel/src/object/objecttype.c"
    case cap_zombie_cap:
        fc_ret.remainder = cap;
        fc_ret.cleanupInfo = cap_null_cap_new();
        return fc_ret;

    case cap_irq_handler_cap:
        if (final) {
            irq_t irq = (cap_irq_handler_cap_get_capIRQ(cap));

            deletingIRQHandler(irq);

            fc_ret.remainder = cap_null_cap_new();
            fc_ret.cleanupInfo = cap;
            return fc_ret;
        }
        break;
    }

    fc_ret.remainder = cap_null_cap_new();
    fc_ret.cleanupInfo = cap_null_cap_new();
    return fc_ret;
}

bool_t __attribute__((__const__)) hasCancelSendRights(cap_t cap)
{
    switch (cap_get_capType(cap)) {
    case cap_endpoint_cap:
        return cap_endpoint_cap_get_capCanSend(cap) &&
               cap_endpoint_cap_get_capCanReceive(cap) &&
               cap_endpoint_cap_get_capCanGrantReply(cap) &&
               cap_endpoint_cap_get_capCanGrant(cap);

    default:
        return false;
    }
}

bool_t __attribute__((__const__)) sameRegionAs(cap_t cap_a, cap_t cap_b)
{
    switch (cap_get_capType(cap_a)) {
    case cap_untyped_cap:
        if (cap_get_capIsPhysical(cap_b)) {
            word_t aBase, bBase, aTop, bTop;

            aBase = (word_t)((word_t *)(cap_untyped_cap_get_capPtr(cap_a)));
            bBase = (word_t)cap_get_capPtr(cap_b);

            aTop = aBase + ((1ul << (cap_untyped_cap_get_capBlockSize(cap_a))) - 1ul);
            bTop = bBase + ((1ul << (cap_get_capSizeBits(cap_b))) - 1ul);

            return (aBase <= bBase) && (bTop <= aTop) && (bBase <= bTop);
        }
        break;

    case cap_endpoint_cap:
        if (cap_get_capType(cap_b) == cap_endpoint_cap) {
            return cap_endpoint_cap_get_capEPPtr(cap_a) ==
                   cap_endpoint_cap_get_capEPPtr(cap_b);
        }
        break;

    case cap_notification_cap:
        if (cap_get_capType(cap_b) == cap_notification_cap) {
            return cap_notification_cap_get_capNtfnPtr(cap_a) ==
                   cap_notification_cap_get_capNtfnPtr(cap_b);
        }
        break;

    case cap_cnode_cap:
        if (cap_get_capType(cap_b) == cap_cnode_cap) {
            return (cap_cnode_cap_get_capCNodePtr(cap_a) ==
                    cap_cnode_cap_get_capCNodePtr(cap_b)) &&
                   (cap_cnode_cap_get_capCNodeRadix(cap_a) ==
                    cap_cnode_cap_get_capCNodeRadix(cap_b));
        }
        break;

    case cap_thread_cap:
        if (cap_get_capType(cap_b) == cap_thread_cap) {
            return cap_thread_cap_get_capTCBPtr(cap_a) ==
                   cap_thread_cap_get_capTCBPtr(cap_b);
        }
        break;

    case cap_reply_cap:
        if (cap_get_capType(cap_b) == cap_reply_cap) {




            return cap_reply_cap_get_capTCBPtr(cap_a) ==
                   cap_reply_cap_get_capTCBPtr(cap_b);

        }
        break;

    case cap_domain_cap:
        if (cap_get_capType(cap_b) == cap_domain_cap) {
            return true;
        }
        break;

    case cap_irq_control_cap:
        if (cap_get_capType(cap_b) == cap_irq_control_cap ||
            cap_get_capType(cap_b) == cap_irq_handler_cap ||
            Arch_isIRQControlDescendant(cap_b)) {
            return true;
        }
        break;

    case cap_irq_handler_cap:
        if (cap_get_capType(cap_b) == cap_irq_handler_cap) {
            return (word_t)cap_irq_handler_cap_get_capIRQ(cap_a) ==
                   (word_t)cap_irq_handler_cap_get_capIRQ(cap_b);
        }
        break;
# 373 "/home/b/test/sel4test/kernel/src/object/objecttype.c"
    default:
        if (isArchCap(cap_a) &&
            isArchCap(cap_b)) {
            return Arch_sameRegionAs(cap_a, cap_b);
        }
        break;
    }

    return false;
}

bool_t __attribute__((__const__)) sameObjectAs(cap_t cap_a, cap_t cap_b)
{
    if (cap_get_capType(cap_a) == cap_untyped_cap) {
        return false;
    }
    if (cap_get_capType(cap_a) == cap_irq_control_cap) {
        return false;
    }
    if (isArchCap(cap_a) && isArchCap(cap_b)) {
        return Arch_sameObjectAs(cap_a, cap_b);
    }
    return sameRegionAs(cap_a, cap_b);
}

cap_t __attribute__((__const__)) updateCapData(bool_t preserve, word_t newData, cap_t cap)
{
    if (isArchCap(cap)) {
        return Arch_updateCapData(preserve, newData, cap);
    }

    switch (cap_get_capType(cap)) {
    case cap_endpoint_cap:
        if (!preserve && cap_endpoint_cap_get_capEPBadge(cap) == 0) {
            return cap_endpoint_cap_set_capEPBadge(cap, newData);
        } else {
            return cap_null_cap_new();
        }

    case cap_notification_cap:
        if (!preserve && cap_notification_cap_get_capNtfnBadge(cap) == 0) {
            return cap_notification_cap_set_capNtfnBadge(cap, newData);
        } else {
            return cap_null_cap_new();
        }

    case cap_cnode_cap: {
        word_t guard, guardSize;
        seL4_CNode_CapData_t w = { .words = { newData } };

        guardSize = seL4_CNode_CapData_get_guardSize(w);

        if (guardSize + cap_cnode_cap_get_capCNodeRadix(cap) > (1 << 6)) {
            return cap_null_cap_new();
        } else {
            cap_t new_cap;

            guard = seL4_CNode_CapData_get_guard(w) & ((1ul << (guardSize)) - 1ul);
            new_cap = cap_cnode_cap_set_capCNodeGuard(cap, guard);
            new_cap = cap_cnode_cap_set_capCNodeGuardSize(new_cap,
                                                          guardSize);

            return new_cap;
        }
    }

    default:
        return cap;
    }
}

cap_t __attribute__((__const__)) maskCapRights(seL4_CapRights_t cap_rights, cap_t cap)
{
    if (isArchCap(cap)) {
        return Arch_maskCapRights(cap_rights, cap);
    }

    switch (cap_get_capType(cap)) {
    case cap_null_cap:
    case cap_domain_cap:
    case cap_cnode_cap:
    case cap_untyped_cap:
    case cap_irq_control_cap:
    case cap_irq_handler_cap:
    case cap_zombie_cap:
    case cap_thread_cap:




        return cap;

    case cap_endpoint_cap: {
        cap_t new_cap;

        new_cap = cap_endpoint_cap_set_capCanSend(
                      cap, cap_endpoint_cap_get_capCanSend(cap) &
                      seL4_CapRights_get_capAllowWrite(cap_rights));
        new_cap = cap_endpoint_cap_set_capCanReceive(
                      new_cap, cap_endpoint_cap_get_capCanReceive(cap) &
                      seL4_CapRights_get_capAllowRead(cap_rights));
        new_cap = cap_endpoint_cap_set_capCanGrant(
                      new_cap, cap_endpoint_cap_get_capCanGrant(cap) &
                      seL4_CapRights_get_capAllowGrant(cap_rights));
        new_cap = cap_endpoint_cap_set_capCanGrantReply(
                      new_cap, cap_endpoint_cap_get_capCanGrantReply(cap) &
                      seL4_CapRights_get_capAllowGrantReply(cap_rights));

        return new_cap;
    }

    case cap_notification_cap: {
        cap_t new_cap;

        new_cap = cap_notification_cap_set_capNtfnCanSend(
                      cap, cap_notification_cap_get_capNtfnCanSend(cap) &
                      seL4_CapRights_get_capAllowWrite(cap_rights));
        new_cap = cap_notification_cap_set_capNtfnCanReceive(new_cap,
                                                             cap_notification_cap_get_capNtfnCanReceive(cap) &
                                                             seL4_CapRights_get_capAllowRead(cap_rights));

        return new_cap;
    }
    case cap_reply_cap: {
        cap_t new_cap;

        new_cap = cap_reply_cap_set_capReplyCanGrant(
                      cap, cap_reply_cap_get_capReplyCanGrant(cap) &
                      seL4_CapRights_get_capAllowGrant(cap_rights));
        return new_cap;
    }


    default:
        _fail("Invalid cap type", "/home/b/test/sel4test/kernel/src/object/objecttype.c", 507, __func__); /* Sentinel for invalid enums */
    }
}

cap_t createObject(object_t t, void *regionBase, word_t userSize, bool_t deviceMemory)
{
    /* Handle architecture-specific objects. */
    if (t >= (object_t) seL4_NonArchObjectTypeCount) {
        return Arch_createObject(t, regionBase, userSize, deviceMemory);
    }

    /* Create objects. */
    switch ((api_object_t)t) {
    case seL4_TCBObject: {
        tcb_t *tcb;
        tcb = ((tcb_t *)((word_t)regionBase + (1ul << ((12 - 1)))));
        /** AUXUPD: "(True, ptr_retyps 1
          (Ptr ((ptr_val \<acute>tcb) - ctcb_offset) :: (cte_C[5]) ptr)
            o (ptr_retyp \<acute>tcb))" */

        /* Setup non-zero parts of the TCB. */

        Arch_initContext(&tcb->tcbArch.tcbContext);

        tcb->tcbTimeSlice = 5;

        tcb->tcbDomain = ksCurDomain;

        /* Initialize the new TCB to the current core */
        ;


        strlcpy(((debug_tcb_t *)(((cte_t *)((word_t)(tcb)&~((1ul << (12)) - 1ul)))+(tcbCNodeEntries)))->tcbName, "child of: '", ((1ul << (12 -1)) - (tcbCNodeEntries * sizeof(cte_t)) - sizeof(debug_tcb_t)));
        strlcat(((debug_tcb_t *)(((cte_t *)((word_t)(tcb)&~((1ul << (12)) - 1ul)))+(tcbCNodeEntries)))->tcbName, ((debug_tcb_t *)(((cte_t *)((word_t)(ksCurThread)&~((1ul << (12)) - 1ul)))+(tcbCNodeEntries)))->tcbName, ((1ul << (12 -1)) - (tcbCNodeEntries * sizeof(cte_t)) - sizeof(debug_tcb_t)));
        strlcat(((debug_tcb_t *)(((cte_t *)((word_t)(tcb)&~((1ul << (12)) - 1ul)))+(tcbCNodeEntries)))->tcbName, "'", ((1ul << (12 -1)) - (tcbCNodeEntries * sizeof(cte_t)) - sizeof(debug_tcb_t)));
        tcbDebugAppend(tcb);


        return cap_thread_cap_new(((word_t)(tcb)));
    }

    case seL4_EndpointObject:
        /** AUXUPD: "(True, ptr_retyp
          (Ptr (ptr_val \<acute>regionBase) :: endpoint_C ptr))" */
        return cap_endpoint_cap_new(0, true, true, true, true,
                                    ((word_t)(regionBase)));

    case seL4_NotificationObject:
        /** AUXUPD: "(True, ptr_retyp
              (Ptr (ptr_val \<acute>regionBase) :: notification_C ptr))" */
        return cap_notification_cap_new(0, true, true,
                                        ((word_t)(regionBase)));

    case seL4_CapTableObject:
        /** AUXUPD: "(True, ptr_arr_retyps (2 ^ (unat \<acute>userSize))
          (Ptr (ptr_val \<acute>regionBase) :: cte_C ptr))" */
        /** GHOSTUPD: "(True, gs_new_cnodes (unat \<acute>userSize)
                                (ptr_val \<acute>regionBase)
                                (4 + unat \<acute>userSize))" */
        return cap_cnode_cap_new(userSize, 0, 0, ((word_t)(regionBase)));

    case seL4_UntypedObject:
        /*
         * No objects need to be created; instead, just insert caps into
         * the destination slots.
         */
        return cap_untyped_cap_new(0, !!deviceMemory, userSize, ((word_t)(regionBase)));
# 593 "/home/b/test/sel4test/kernel/src/object/objecttype.c"
    default:
        _fail("Invalid object type", "/home/b/test/sel4test/kernel/src/object/objecttype.c", 594, __func__);
    }
}

void createNewObjects(object_t t, cte_t *parent,
                      cte_t *destCNode, word_t destOffset, word_t destLength,
                      void *regionBase, word_t userSize, bool_t deviceMemory)
{
    word_t objectSize;
    void *nextFreeArea;
    word_t i;
    word_t totalObjectSize __attribute__((unused));

    /* ghost check that we're visiting less bytes than the max object size */
    objectSize = getObjectSize(t, userSize);
    totalObjectSize = destLength << objectSize;
    /** GHOSTUPD: "(gs_get_assn cap_get_capSizeBits_'proc \<acute>ghost'state = 0
        \<or> \<acute>totalObjectSize <= gs_get_assn cap_get_capSizeBits_'proc \<acute>ghost'state, id)" */

    /* Create the objects. */
    nextFreeArea = regionBase;
    for (i = 0; i < destLength; i++) {
        /* Create the object. */
        /** AUXUPD: "(True, typ_region_bytes (ptr_val \<acute> nextFreeArea + ((\<acute> i) << unat (\<acute> objectSize))) (unat (\<acute> objectSize)))" */
        cap_t cap = createObject(t, (void *)((word_t)nextFreeArea + (i << objectSize)), userSize, deviceMemory);

        /* Insert the cap into the user's cspace. */
        insertNewCap(parent, &destCNode[destOffset + i], cap);

        /* Move along to the next region of memory. been merged into a formula of i */
    }
}







exception_t decodeInvocation(word_t invLabel, word_t length,
                             cptr_t capIndex, cte_t *slot, cap_t cap,
                             bool_t block, bool_t call,
                             word_t *buffer)

{
    if (isArchCap(cap)) {
        return Arch_decodeInvocation(invLabel, length, capIndex,
                                     slot, cap, call, buffer);
    }

    switch (cap_get_capType(cap)) {
    case cap_null_cap:
        do { printf("\033[0m" "\033[1m" "<<" "\033[0m" "\033[32m" "seL4(CPU %" "lu" ")" "\033[0m" "\033[1m" " [%s/%d T%p \"%s\" @%lx]: " "Attempted to invoke a null cap #%lu." ">>" "\033[0m" "\n", 0lu, __func__, 646, ksCurThread, ((debug_tcb_t *)(((cte_t *)((word_t)(ksCurThread)&~((1ul << (12)) - 1ul)))+(tcbCNodeEntries)))->tcbName, (word_t)getRestartPC(ksCurThread), capIndex); } while (0);
        current_syscall_error.type = seL4_InvalidCapability;
        current_syscall_error.invalidCapNumber = 0;
        return EXCEPTION_SYSCALL_ERROR;

    case cap_zombie_cap:
        do { printf("\033[0m" "\033[1m" "<<" "\033[0m" "\033[32m" "seL4(CPU %" "lu" ")" "\033[0m" "\033[1m" " [%s/%d T%p \"%s\" @%lx]: " "Attempted to invoke a zombie cap #%lu." ">>" "\033[0m" "\n", 0lu, __func__, 652, ksCurThread, ((debug_tcb_t *)(((cte_t *)((word_t)(ksCurThread)&~((1ul << (12)) - 1ul)))+(tcbCNodeEntries)))->tcbName, (word_t)getRestartPC(ksCurThread), capIndex); } while (0);
        current_syscall_error.type = seL4_InvalidCapability;
        current_syscall_error.invalidCapNumber = 0;
        return EXCEPTION_SYSCALL_ERROR;

    case cap_endpoint_cap:
        if (__builtin_expect(!!(!cap_endpoint_cap_get_capCanSend(cap)), 0)) {
            do { printf("\033[0m" "\033[1m" "<<" "\033[0m" "\033[32m" "seL4(CPU %" "lu" ")" "\033[0m" "\033[1m" " [%s/%d T%p \"%s\" @%lx]: " "Attempted to invoke a read-only endpoint cap #%lu." ">>" "\033[0m" "\n", 0lu, __func__, 659, ksCurThread, ((debug_tcb_t *)(((cte_t *)((word_t)(ksCurThread)&~((1ul << (12)) - 1ul)))+(tcbCNodeEntries)))->tcbName, (word_t)getRestartPC(ksCurThread), capIndex); } while (0)
                               ;
            current_syscall_error.type = seL4_InvalidCapability;
            current_syscall_error.invalidCapNumber = 0;
            return EXCEPTION_SYSCALL_ERROR;
        }

        setThreadState(ksCurThread, ThreadState_Restart);







        return performInvocation_Endpoint(
                   ((endpoint_t *)(cap_endpoint_cap_get_capEPPtr(cap))),
                   cap_endpoint_cap_get_capEPBadge(cap),
                   cap_endpoint_cap_get_capCanGrant(cap),
                   cap_endpoint_cap_get_capCanGrantReply(cap), block, call);


    case cap_notification_cap: {
        if (__builtin_expect(!!(!cap_notification_cap_get_capNtfnCanSend(cap)), 0)) {
            do { printf("\033[0m" "\033[1m" "<<" "\033[0m" "\033[32m" "seL4(CPU %" "lu" ")" "\033[0m" "\033[1m" " [%s/%d T%p \"%s\" @%lx]: " "Attempted to invoke a read-only notification cap #%lu." ">>" "\033[0m" "\n", 0lu, __func__, 683, ksCurThread, ((debug_tcb_t *)(((cte_t *)((word_t)(ksCurThread)&~((1ul << (12)) - 1ul)))+(tcbCNodeEntries)))->tcbName, (word_t)getRestartPC(ksCurThread), capIndex); } while (0)
                               ;
            current_syscall_error.type = seL4_InvalidCapability;
            current_syscall_error.invalidCapNumber = 0;
            return EXCEPTION_SYSCALL_ERROR;
        }

        setThreadState(ksCurThread, ThreadState_Restart);
        return performInvocation_Notification(
                   ((notification_t *)(cap_notification_cap_get_capNtfnPtr(cap))),
                   cap_notification_cap_get_capNtfnBadge(cap));
    }
# 704 "/home/b/test/sel4test/kernel/src/object/objecttype.c"
    case cap_reply_cap:
        if (__builtin_expect(!!(cap_reply_cap_get_capReplyMaster(cap)), 0)) {
            do { printf("\033[0m" "\033[1m" "<<" "\033[0m" "\033[32m" "seL4(CPU %" "lu" ")" "\033[0m" "\033[1m" " [%s/%d T%p \"%s\" @%lx]: " "Attempted to invoke an invalid reply cap #%lu." ">>" "\033[0m" "\n", 0lu, __func__, 706, ksCurThread, ((debug_tcb_t *)(((cte_t *)((word_t)(ksCurThread)&~((1ul << (12)) - 1ul)))+(tcbCNodeEntries)))->tcbName, (word_t)getRestartPC(ksCurThread), capIndex); } while (0)
                               ;
            current_syscall_error.type = seL4_InvalidCapability;
            current_syscall_error.invalidCapNumber = 0;
            return EXCEPTION_SYSCALL_ERROR;
        }

        setThreadState(ksCurThread, ThreadState_Restart);
        return performInvocation_Reply(
                   ((tcb_t *)(cap_reply_cap_get_capTCBPtr(cap))), slot,
                   cap_reply_cap_get_capReplyCanGrant(cap));



    case cap_thread_cap:
# 729 "/home/b/test/sel4test/kernel/src/object/objecttype.c"
        return decodeTCBInvocation(invLabel, length, cap, slot, call, buffer);

    case cap_domain_cap:
# 740 "/home/b/test/sel4test/kernel/src/object/objecttype.c"
        return decodeDomainInvocation(invLabel, length, buffer);

    case cap_cnode_cap:
# 751 "/home/b/test/sel4test/kernel/src/object/objecttype.c"
        return decodeCNodeInvocation(invLabel, length, cap, buffer);

    case cap_untyped_cap:
        return decodeUntypedInvocation(invLabel, length, slot, cap, call, buffer);

    case cap_irq_control_cap:
        return decodeIRQControlInvocation(invLabel, length, slot, buffer);

    case cap_irq_handler_cap:
        return decodeIRQHandlerInvocation(invLabel,
                                          (cap_irq_handler_cap_get_capIRQ(cap)));
# 783 "/home/b/test/sel4test/kernel/src/object/objecttype.c"
    default:
        _fail("Invalid cap type", "/home/b/test/sel4test/kernel/src/object/objecttype.c", 784, __func__);
    }
}
# 798 "/home/b/test/sel4test/kernel/src/object/objecttype.c"
exception_t performInvocation_Endpoint(endpoint_t *ep, word_t badge,
                                       bool_t canGrant, bool_t canGrantReply,
                                       bool_t block, bool_t call)
{
    sendIPC(block, call, badge, canGrant, canGrantReply, ksCurThread, ep);

    return EXCEPTION_NONE;
}


exception_t performInvocation_Notification(notification_t *ntfn, word_t badge)
{
    sendSignal(ntfn, badge);

    return EXCEPTION_NONE;
}
# 822 "/home/b/test/sel4test/kernel/src/object/objecttype.c"
exception_t performInvocation_Reply(tcb_t *thread, cte_t *slot, bool_t canGrant)
{
    doReplyTransfer(ksCurThread, thread, slot, canGrant);
    return EXCEPTION_NONE;
}


word_t __attribute__((__const__)) cap_get_capSizeBits(cap_t cap)
{

    cap_tag_t ctag;

    ctag = cap_get_capType(cap);

    switch (ctag) {
    case cap_untyped_cap:
        return cap_untyped_cap_get_capBlockSize(cap);

    case cap_endpoint_cap:
        return 4;

    case cap_notification_cap:
        return 5;

    case cap_cnode_cap:
        return cap_cnode_cap_get_capCNodeRadix(cap) + 5;

    case cap_thread_cap:
        return 12;

    case cap_zombie_cap: {
        word_t type = cap_zombie_cap_get_capZombieType(cap);
        if (type == (1ul << (6))) {
            return 12;
        }
        return ((type) & ((1ul << (6)) - 1ul)) + 5;
    }

    case cap_null_cap:
        return 0;

    case cap_domain_cap:
        return 0;

    case cap_reply_cap:



        return 0;


    case cap_irq_control_cap:



        return 0;

    case cap_irq_handler_cap:
        return 0;






    default:
        return cap_get_archCapSizeBits(cap);
    }

}

/* Returns whether or not this capability has memory associated
 * with it or not. Referring to this as 'being physical' is to
 * match up with the Haskell and abstract specifications */
bool_t __attribute__((__const__)) cap_get_capIsPhysical(cap_t cap)
{
    cap_tag_t ctag;

    ctag = cap_get_capType(cap);

    switch (ctag) {
    case cap_untyped_cap:
        return true;

    case cap_endpoint_cap:
        return true;

    case cap_notification_cap:
        return true;

    case cap_cnode_cap:
        return true;

    case cap_thread_cap:



        return true;

    case cap_zombie_cap:
        return true;

    case cap_domain_cap:
        return false;

    case cap_reply_cap:



        return false;


    case cap_irq_control_cap:



        return false;

    case cap_irq_handler_cap:
        return false;

    default:
        return cap_get_archCapIsPhysical(cap);
    }
}

void *__attribute__((__const__)) cap_get_capPtr(cap_t cap)
{
    cap_tag_t ctag;

    ctag = cap_get_capType(cap);

    switch (ctag) {
    case cap_untyped_cap:
        return ((word_t *)(cap_untyped_cap_get_capPtr(cap)));

    case cap_endpoint_cap:
        return ((endpoint_t *)(cap_endpoint_cap_get_capEPPtr(cap)));

    case cap_notification_cap:
        return ((notification_t *)(cap_notification_cap_get_capNtfnPtr(cap)));

    case cap_cnode_cap:
        return ((cte_t *)(cap_cnode_cap_get_capCNodePtr(cap)));

    case cap_thread_cap:
        return (((cte_t *)((word_t)(cap_thread_cap_get_capTCBPtr(cap))&~((1ul << (12)) - 1ul)))+(0));

    case cap_zombie_cap:
        return ((cte_t *)(cap_zombie_cap_get_capZombiePtr(cap)));

    case cap_domain_cap:
        return ((void *)0);

    case cap_reply_cap:



        return ((void *)0);


    case cap_irq_control_cap:



        return ((void *)0);

    case cap_irq_handler_cap:
        return ((void *)0);






    default:
        return cap_get_archCapPtr(cap);

    }
}

bool_t __attribute__((__const__)) isCapRevocable(cap_t derivedCap, cap_t srcCap)
{
    if (isArchCap(derivedCap)) {
        return Arch_isCapRevocable(derivedCap, srcCap);
    }
    switch (cap_get_capType(derivedCap)) {
    case cap_endpoint_cap:
        return (cap_endpoint_cap_get_capEPBadge(derivedCap) !=
                cap_endpoint_cap_get_capEPBadge(srcCap));

    case cap_notification_cap:
        return (cap_notification_cap_get_capNtfnBadge(derivedCap) !=
                cap_notification_cap_get_capNtfnBadge(srcCap));

    case cap_irq_handler_cap:
        return (cap_get_capType(srcCap) ==
                cap_irq_control_cap);

    case cap_untyped_cap:
        return true;

    default:
        return false;
    }
}
# 1 "/home/b/test/sel4test/kernel/src/object/tcb.c"
/*
 * Copyright 2014, General Dynamics C4 Systems
 *
 * SPDX-License-Identifier: GPL-2.0-only
 */
# 32 "/home/b/test/sel4test/kernel/src/object/tcb.c"
static exception_t checkPrio(prio_t prio, tcb_t *auth)
{
    prio_t mcp;

    mcp = auth->tcbMCP;

    /* system invariant: existing MCPs are bounded */
    do { if (!(mcp <= seL4_MaxPrio)) { _assert_fail("mcp <= seL4_MaxPrio", "/home/b/test/sel4test/kernel/src/object/tcb.c", 39, __func__); } } while(0);

    /* can't assign a priority greater than our own mcp */
    if (prio > mcp) {
        current_syscall_error.type = seL4_RangeError;
        current_syscall_error.rangeErrorMin = seL4_MinPrio;
        current_syscall_error.rangeErrorMax = mcp;
        return EXCEPTION_SYSCALL_ERROR;
    }

    return EXCEPTION_NONE;
}

static inline void addToBitmap(word_t cpu, word_t dom, word_t prio)
{
    word_t l1index;
    word_t l1index_inverted;

    l1index = prio_to_l1index(prio);
    l1index_inverted = invert_l1index(l1index);

    ksReadyQueuesL1Bitmap[dom] |= (1ul << (l1index));
    /* we invert the l1 index when accessed the 2nd level of the bitmap in
       order to increase the likelihood that high prio threads l2 index word will
       be on the same cache line as the l1 index word - this makes sure the
       fastpath is fastest for high prio threads */
    ksReadyQueuesL2Bitmap[dom][l1index_inverted] |= (1ul << (prio & ((1ul << (6)) - 1ul)));
}

static inline void removeFromBitmap(word_t cpu, word_t dom, word_t prio)
{
    word_t l1index;
    word_t l1index_inverted;

    l1index = prio_to_l1index(prio);
    l1index_inverted = invert_l1index(l1index);
    ksReadyQueuesL2Bitmap[dom][l1index_inverted] &= ~(1ul << (prio & ((1ul << (6)) - 1ul)));
    if (__builtin_expect(!!(!ksReadyQueuesL2Bitmap[dom][l1index_inverted]), 0)) {
        ksReadyQueuesL1Bitmap[dom] &= ~(1ul << (l1index));
    }
}

tcb_queue_t tcb_queue_remove(tcb_queue_t queue, tcb_t *tcb)
{
    tcb_t *before;
    tcb_t *after;

    before = tcb->tcbSchedPrev;
    after = tcb->tcbSchedNext;

    if (queue.head == tcb && queue.end == tcb) {
        queue.head = ((void *)0);
        queue.end = ((void *)0);
    } else {
        if (queue.head == tcb) {
            after->tcbSchedPrev = ((void *)0);
            tcb->tcbSchedNext = ((void *)0);
            queue.head = after;
        } else {
            if (queue.end == tcb) {
                before->tcbSchedNext = ((void *)0);
                tcb->tcbSchedPrev = ((void *)0);
                queue.end = before;
            } else {
                before->tcbSchedNext = after;
                after->tcbSchedPrev = before;
                tcb->tcbSchedPrev = ((void *)0);
                tcb->tcbSchedNext = ((void *)0);
            }
        }
    }

    return queue;
}

/* Add TCB to the head of a scheduler queue */
void tcbSchedEnqueue(tcb_t *tcb)
{





    if (!thread_state_get_tcbQueued(tcb->tcbState)) {
        tcb_queue_t queue;
        dom_t dom;
        prio_t prio;
        word_t idx;

        dom = tcb->tcbDomain;
        prio = tcb->tcbPriority;
        idx = ready_queues_index(dom, prio);
        queue = ksReadyQueues[idx];

        if (tcb_queue_empty(queue)) {
            addToBitmap(0, dom, prio);
        }

        ksReadyQueues[idx] = tcb_queue_prepend(queue, tcb);

        thread_state_ptr_set_tcbQueued(&tcb->tcbState, true);
    }
}

/* Add TCB to the end of a scheduler queue */
void tcbSchedAppend(tcb_t *tcb)
{





    if (!thread_state_get_tcbQueued(tcb->tcbState)) {
        tcb_queue_t queue;
        dom_t dom;
        prio_t prio;
        word_t idx;

        dom = tcb->tcbDomain;
        prio = tcb->tcbPriority;
        idx = ready_queues_index(dom, prio);
        queue = ksReadyQueues[idx];

        if (tcb_queue_empty(queue)) {
            addToBitmap(0, dom, prio);
        }

        ksReadyQueues[idx] = tcb_queue_append(queue, tcb);

        thread_state_ptr_set_tcbQueued(&tcb->tcbState, true);
    }
}

/* Remove TCB from a scheduler queue */
void tcbSchedDequeue(tcb_t *tcb)
{
    if (thread_state_get_tcbQueued(tcb->tcbState)) {
        tcb_queue_t queue;
        tcb_queue_t new_queue;
        dom_t dom;
        prio_t prio;
        word_t idx;

        dom = tcb->tcbDomain;
        prio = tcb->tcbPriority;
        idx = ready_queues_index(dom, prio);
        queue = ksReadyQueues[idx];

        new_queue = tcb_queue_remove(queue, tcb);

        ksReadyQueues[idx] = new_queue;

        thread_state_ptr_set_tcbQueued(&tcb->tcbState, false);

        if (__builtin_expect(!!(tcb_queue_empty(new_queue)), 1)) {
            removeFromBitmap(0, dom, prio);
        }
    }
}


void tcbDebugAppend(tcb_t *tcb)
{
    debug_tcb_t *debug_tcb = ((debug_tcb_t *)(((cte_t *)((word_t)(tcb)&~((1ul << (12)) - 1ul)))+(tcbCNodeEntries)));
    /* prepend to the list */
    debug_tcb->tcbDebugPrev = ((void *)0);

    debug_tcb->tcbDebugNext = ksDebugTCBs;

    if (ksDebugTCBs) {
        ((debug_tcb_t *)(((cte_t *)((word_t)(ksDebugTCBs)&~((1ul << (12)) - 1ul)))+(tcbCNodeEntries)))->tcbDebugPrev = tcb;
    }

    ksDebugTCBs = tcb;
}

void tcbDebugRemove(tcb_t *tcb)
{
    debug_tcb_t *debug_tcb = ((debug_tcb_t *)(((cte_t *)((word_t)(tcb)&~((1ul << (12)) - 1ul)))+(tcbCNodeEntries)));

    do { if (!(ksDebugTCBs != ((void *)0))) { _assert_fail("NODE_STATE_ON_CORE(ksDebugTCBs, tcb->tcbAffinity) != NULL", "/home/b/test/sel4test/kernel/src/object/tcb.c", 219, __func__); } } while(0);
    if (tcb == ksDebugTCBs) {
        ksDebugTCBs = ((debug_tcb_t *)(((cte_t *)((word_t)(ksDebugTCBs)&~((1ul << (12)) - 1ul)))+(tcbCNodeEntries)))
                                                                                                                   ->tcbDebugNext;
    } else {
        do { if (!(((debug_tcb_t *)(((cte_t *)((word_t)(tcb)&~((1ul << (12)) - 1ul)))+(tcbCNodeEntries)))->tcbDebugPrev)) { _assert_fail("TCB_PTR_DEBUG_PTR(tcb)->tcbDebugPrev", "/home/b/test/sel4test/kernel/src/object/tcb.c", 224, __func__); } } while(0);
        ((debug_tcb_t *)(((cte_t *)((word_t)(debug_tcb->tcbDebugPrev)&~((1ul << (12)) - 1ul)))+(tcbCNodeEntries)))->tcbDebugNext = debug_tcb->tcbDebugNext;
    }

    if (debug_tcb->tcbDebugNext) {
        ((debug_tcb_t *)(((cte_t *)((word_t)(debug_tcb->tcbDebugNext)&~((1ul << (12)) - 1ul)))+(tcbCNodeEntries)))->tcbDebugPrev = debug_tcb->tcbDebugPrev;
    }

    debug_tcb->tcbDebugPrev = ((void *)0);
    debug_tcb->tcbDebugNext = ((void *)0);
}



/* Add TCB to the end of an endpoint queue */
tcb_queue_t tcbEPAppend(tcb_t *tcb, tcb_queue_t queue)
{
    if (!queue.head) { /* Empty list */
        queue.head = tcb;
    } else {
        queue.end->tcbEPNext = tcb;
    }
    tcb->tcbEPPrev = queue.end;
    tcb->tcbEPNext = ((void *)0);
    queue.end = tcb;

    return queue;
}


/* Remove TCB from an endpoint queue */
tcb_queue_t tcbEPDequeue(tcb_t *tcb, tcb_queue_t queue)
{
    if (tcb->tcbEPPrev) {
        tcb->tcbEPPrev->tcbEPNext = tcb->tcbEPNext;
    } else {
        queue.head = tcb->tcbEPNext;
    }

    if (tcb->tcbEPNext) {
        tcb->tcbEPNext->tcbEPPrev = tcb->tcbEPPrev;
    } else {
        queue.end = tcb->tcbEPPrev;
    }

    return queue;
}
# 338 "/home/b/test/sel4test/kernel/src/object/tcb.c"
cptr_t __attribute__((__pure__)) getExtraCPtr(word_t *bufferPtr, word_t i)
{
    return (cptr_t)bufferPtr[seL4_MsgMaxLength + 2 + i];
}

void setExtraBadge(word_t *bufferPtr, word_t badge,
                   word_t i)
{
    bufferPtr[seL4_MsgMaxLength + 2 + i] = badge;
}


void setupCallerCap(tcb_t *sender, tcb_t *receiver, bool_t canGrant)
{
    cte_t *replySlot, *callerSlot;
    cap_t masterCap __attribute__((unused)), callerCap __attribute__((unused));

    setThreadState(sender, ThreadState_BlockedOnReply);
    replySlot = (((cte_t *)((word_t)(sender)&~((1ul << (12)) - 1ul)))+(tcbReply));
    masterCap = replySlot->cap;
    /* Haskell error: "Sender must have a valid master reply cap" */
    do { if (!(cap_get_capType(masterCap) == cap_reply_cap)) { _assert_fail("cap_get_capType(masterCap) == cap_reply_cap", "/home/b/test/sel4test/kernel/src/object/tcb.c", 359, __func__); } } while(0);
    do { if (!(cap_reply_cap_get_capReplyMaster(masterCap))) { _assert_fail("cap_reply_cap_get_capReplyMaster(masterCap)", "/home/b/test/sel4test/kernel/src/object/tcb.c", 360, __func__); } } while(0);
    do { if (!(cap_reply_cap_get_capReplyCanGrant(masterCap))) { _assert_fail("cap_reply_cap_get_capReplyCanGrant(masterCap)", "/home/b/test/sel4test/kernel/src/object/tcb.c", 361, __func__); } } while(0);
    do { if (!(((tcb_t *)(cap_reply_cap_get_capTCBPtr(masterCap))) == sender)) { _assert_fail("TCB_PTR(cap_reply_cap_get_capTCBPtr(masterCap)) == sender", "/home/b/test/sel4test/kernel/src/object/tcb.c", 362, __func__); } } while(0);
    callerSlot = (((cte_t *)((word_t)(receiver)&~((1ul << (12)) - 1ul)))+(tcbCaller));
    callerCap = callerSlot->cap;
    /* Haskell error: "Caller cap must not already exist" */
    do { if (!(cap_get_capType(callerCap) == cap_null_cap)) { _assert_fail("cap_get_capType(callerCap) == cap_null_cap", "/home/b/test/sel4test/kernel/src/object/tcb.c", 366, __func__); } } while(0);
    cteInsert(cap_reply_cap_new(canGrant, false, ((word_t)(sender))),
              replySlot, callerSlot);
}

void deleteCallerCap(tcb_t *receiver)
{
    cte_t *callerSlot;

    callerSlot = (((cte_t *)((word_t)(receiver)&~((1ul << (12)) - 1ul)))+(tcbCaller));
    /** GHOSTUPD: "(True, gs_set_assn cteDeleteOne_'proc (ucast cap_reply_cap))" */
    cteDeleteOne(callerSlot);
}


extra_caps_t current_extra_caps;

exception_t lookupExtraCaps(tcb_t *thread, word_t *bufferPtr, seL4_MessageInfo_t info)
{
    lookupSlot_raw_ret_t lu_ret;
    cptr_t cptr;
    word_t i, length;

    if (!bufferPtr) {
        current_extra_caps.excaprefs[0] = ((void *)0);
        return EXCEPTION_NONE;
    }

    length = seL4_MessageInfo_get_extraCaps(info);

    for (i = 0; i < length; i++) {
        cptr = getExtraCPtr(bufferPtr, i);

        lu_ret = lookupSlot(thread, cptr);
        if (lu_ret.status != EXCEPTION_NONE) {
            current_fault = seL4_Fault_CapFault_new(cptr, false);
            return lu_ret.status;
        }

        current_extra_caps.excaprefs[i] = lu_ret.slot;
    }
    if (i < ((1ul<<(seL4_MsgExtraCapBits))-1)) {
        current_extra_caps.excaprefs[i] = ((void *)0);
    }

    return EXCEPTION_NONE;
}

/* Copy IPC MRs from one thread to another */
word_t copyMRs(tcb_t *sender, word_t *sendBuf, tcb_t *receiver,
               word_t *recvBuf, word_t n)
{
    word_t i;

    /* Copy inline words */
    for (i = 0; i < n && i < n_msgRegisters; i++) {
        setRegister(receiver, msgRegisters[i],
                    getRegister(sender, msgRegisters[i]));
    }

    if (!recvBuf || !sendBuf) {
        return i;
    }

    /* Copy out-of-line words */
    for (; i < n; i++) {
        recvBuf[i + 1] = sendBuf[i + 1];
    }

    return i;
}
# 521 "/home/b/test/sel4test/kernel/src/object/tcb.c"
static exception_t invokeConfigureSingleStepping(bool_t call, word_t *buffer, tcb_t *t,
                                                 uint16_t bp_num, word_t n_instrs)
{
    bool_t bp_was_consumed;
    tcb_t *thread;
    thread = ksCurThread;
    word_t value;

    bp_was_consumed = configureSingleStepping(t, bp_num, n_instrs, false);
    if (n_instrs == 0) {



        value = false;
    } else {



        value = bp_was_consumed;
    }

    if (call) {
        setRegister(thread, badgeRegister, 0);
        unsigned int length = setMR(thread, buffer, 0, value);
        setRegister(thread, msgInfoRegister, wordFromMessageInfo(
                        seL4_MessageInfo_new(0, 0, 0, length)));
    }
    setThreadState(ksCurThread, ThreadState_Running);
    return EXCEPTION_NONE;
}

static exception_t decodeConfigureSingleStepping(cap_t cap, bool_t call, word_t *buffer)
{
    uint16_t bp_num;
    word_t n_instrs;
    tcb_t *tcb;
    syscall_error_t syserr;

    tcb = ((tcb_t *)(cap_thread_cap_get_capTCBPtr(cap)));

    bp_num = getSyscallArg(0, buffer);
    n_instrs = getSyscallArg(1, buffer);

    syserr = Arch_decodeConfigureSingleStepping(tcb, bp_num, n_instrs, false);
    if (syserr.type != seL4_NoError) {
        current_syscall_error = syserr;
        return EXCEPTION_SYSCALL_ERROR;
    }

    setThreadState(ksCurThread, ThreadState_Restart);
    return invokeConfigureSingleStepping(call, buffer, tcb, bp_num, n_instrs);
}

static exception_t invokeSetBreakpoint(tcb_t *tcb, uint16_t bp_num,
                                       word_t vaddr, word_t type, word_t size, word_t rw)
{
    setBreakpoint(tcb, bp_num, vaddr, type, size, rw);
    /* Signal restore_user_context() to pop the breakpoint context on return. */
    setBreakpointUsedFlag(tcb, bp_num);
    return EXCEPTION_NONE;
}

static exception_t decodeSetBreakpoint(cap_t cap, word_t *buffer)
{
    uint16_t bp_num;
    word_t vaddr, type, size, rw;
    tcb_t *tcb;
    syscall_error_t error;

    tcb = ((tcb_t *)(cap_thread_cap_get_capTCBPtr(cap)));
    bp_num = getSyscallArg(0, buffer);
    vaddr = getSyscallArg(1, buffer);
    type = getSyscallArg(2, buffer);
    size = getSyscallArg(3, buffer);
    rw = getSyscallArg(4, buffer);

    /* We disallow the user to set breakpoint addresses that are in the kernel
     * vaddr range.
     */
    if (vaddr >= (word_t)0x00007fffffffffff) {
        do { printf("\033[0m" "\033[1m" "<<" "\033[0m" "\033[32m" "seL4(CPU %" "lu" ")" "\033[0m" "\033[1m" " [%s/%d T%p \"%s\" @%lx]: " "Debug: Invalid address %lx: bp addresses must be userspace " "addresses." ">>" "\033[0m" "\n", 0lu, __func__, 601, ksCurThread, ((debug_tcb_t *)(((cte_t *)((word_t)(ksCurThread)&~((1ul << (12)) - 1ul)))+(tcbCNodeEntries)))->tcbName, (word_t)getRestartPC(ksCurThread), vaddr); } while (0)

                        ;
        current_syscall_error.type = seL4_InvalidArgument;
        current_syscall_error.invalidArgumentNumber = 1;
        return EXCEPTION_SYSCALL_ERROR;
    }

    if (type != seL4_InstructionBreakpoint && type != seL4_DataBreakpoint) {
        do { printf("\033[0m" "\033[1m" "<<" "\033[0m" "\033[32m" "seL4(CPU %" "lu" ")" "\033[0m" "\033[1m" " [%s/%d T%p \"%s\" @%lx]: " "Debug: Unknown breakpoint type %lx." ">>" "\033[0m" "\n", 0lu, __func__, 610, ksCurThread, ((debug_tcb_t *)(((cte_t *)((word_t)(ksCurThread)&~((1ul << (12)) - 1ul)))+(tcbCNodeEntries)))->tcbName, (word_t)getRestartPC(ksCurThread), type); } while (0);
        current_syscall_error.type = seL4_InvalidArgument;
        current_syscall_error.invalidArgumentNumber = 2;
        return EXCEPTION_SYSCALL_ERROR;
    } else if (type == seL4_InstructionBreakpoint) {
        if (size != 0) {
            do { printf("\033[0m" "\033[1m" "<<" "\033[0m" "\033[32m" "seL4(CPU %" "lu" ")" "\033[0m" "\033[1m" " [%s/%d T%p \"%s\" @%lx]: " "Debug: Instruction bps must have size of 0." ">>" "\033[0m" "\n", 0lu, __func__, 616, ksCurThread, ((debug_tcb_t *)(((cte_t *)((word_t)(ksCurThread)&~((1ul << (12)) - 1ul)))+(tcbCNodeEntries)))->tcbName, (word_t)getRestartPC(ksCurThread)); } while (0);
            current_syscall_error.type = seL4_InvalidArgument;
            current_syscall_error.invalidArgumentNumber = 3;
            return EXCEPTION_SYSCALL_ERROR;
        }
        if (rw != seL4_BreakOnRead) {
            do { printf("\033[0m" "\033[1m" "<<" "\033[0m" "\033[32m" "seL4(CPU %" "lu" ")" "\033[0m" "\033[1m" " [%s/%d T%p \"%s\" @%lx]: " "Debug: Instruction bps must be break-on-read." ">>" "\033[0m" "\n", 0lu, __func__, 622, ksCurThread, ((debug_tcb_t *)(((cte_t *)((word_t)(ksCurThread)&~((1ul << (12)) - 1ul)))+(tcbCNodeEntries)))->tcbName, (word_t)getRestartPC(ksCurThread)); } while (0);
            current_syscall_error.type = seL4_InvalidArgument;
            current_syscall_error.invalidArgumentNumber = 4;
            return EXCEPTION_SYSCALL_ERROR;
        }
        if ((6 == -1 || bp_num >= 6)
            && 0 != 6) {
            do { printf("\033[0m" "\033[1m" "<<" "\033[0m" "\033[32m" "seL4(CPU %" "lu" ")" "\033[0m" "\033[1m" " [%s/%d T%p \"%s\" @%lx]: " "Debug: Can't specify a watchpoint ID with type seL4_InstructionBreakpoint." ">>" "\033[0m" "\n", 0lu, __func__, 629, ksCurThread, ((debug_tcb_t *)(((cte_t *)((word_t)(ksCurThread)&~((1ul << (12)) - 1ul)))+(tcbCNodeEntries)))->tcbName, (word_t)getRestartPC(ksCurThread)); } while (0);
            current_syscall_error.type = seL4_InvalidArgument;
            current_syscall_error.invalidArgumentNumber = 2;
            return EXCEPTION_SYSCALL_ERROR;
        }
    } else if (type == seL4_DataBreakpoint) {
        if (size == 0) {
            do { printf("\033[0m" "\033[1m" "<<" "\033[0m" "\033[32m" "seL4(CPU %" "lu" ")" "\033[0m" "\033[1m" " [%s/%d T%p \"%s\" @%lx]: " "Debug: Data bps cannot have size of 0." ">>" "\033[0m" "\n", 0lu, __func__, 636, ksCurThread, ((debug_tcb_t *)(((cte_t *)((word_t)(ksCurThread)&~((1ul << (12)) - 1ul)))+(tcbCNodeEntries)))->tcbName, (word_t)getRestartPC(ksCurThread)); } while (0);
            current_syscall_error.type = seL4_InvalidArgument;
            current_syscall_error.invalidArgumentNumber = 3;
            return EXCEPTION_SYSCALL_ERROR;
        }
        if (6 != -1 && bp_num < 6) {
            do { printf("\033[0m" "\033[1m" "<<" "\033[0m" "\033[32m" "seL4(CPU %" "lu" ")" "\033[0m" "\033[1m" " [%s/%d T%p \"%s\" @%lx]: " "Debug: Data watchpoints cannot specify non-data watchpoint ID." ">>" "\033[0m" "\n", 0lu, __func__, 642, ksCurThread, ((debug_tcb_t *)(((cte_t *)((word_t)(ksCurThread)&~((1ul << (12)) - 1ul)))+(tcbCNodeEntries)))->tcbName, (word_t)getRestartPC(ksCurThread)); } while (0);
            current_syscall_error.type = seL4_InvalidArgument;
            current_syscall_error.invalidArgumentNumber = 2;
            return EXCEPTION_SYSCALL_ERROR;
        }
    } else if (type == seL4_SoftwareBreakRequest) {
        do { printf("\033[0m" "\033[1m" "<<" "\033[0m" "\033[32m" "seL4(CPU %" "lu" ")" "\033[0m" "\033[1m" " [%s/%d T%p \"%s\" @%lx]: " "Debug: Use a software breakpoint instruction to trigger a " "software breakpoint." ">>" "\033[0m" "\n", 0lu, __func__, 648, ksCurThread, ((debug_tcb_t *)(((cte_t *)((word_t)(ksCurThread)&~((1ul << (12)) - 1ul)))+(tcbCNodeEntries)))->tcbName, (word_t)getRestartPC(ksCurThread)); } while (0)
                                         ;
        current_syscall_error.type = seL4_InvalidArgument;
        current_syscall_error.invalidArgumentNumber = 2;
        return EXCEPTION_SYSCALL_ERROR;
    }

    if (rw != seL4_BreakOnRead && rw != seL4_BreakOnWrite
        && rw != seL4_BreakOnReadWrite) {
        do { printf("\033[0m" "\033[1m" "<<" "\033[0m" "\033[32m" "seL4(CPU %" "lu" ")" "\033[0m" "\033[1m" " [%s/%d T%p \"%s\" @%lx]: " "Debug: Unknown access-type %lu." ">>" "\033[0m" "\n", 0lu, __func__, 657, ksCurThread, ((debug_tcb_t *)(((cte_t *)((word_t)(ksCurThread)&~((1ul << (12)) - 1ul)))+(tcbCNodeEntries)))->tcbName, (word_t)getRestartPC(ksCurThread), rw); } while (0);
        current_syscall_error.type = seL4_InvalidArgument;
        current_syscall_error.invalidArgumentNumber = 3;
        return EXCEPTION_SYSCALL_ERROR;
    }
    if (size != 0 && size != 1 && size != 2 && size != 4 && size != 8) {
        do { printf("\033[0m" "\033[1m" "<<" "\033[0m" "\033[32m" "seL4(CPU %" "lu" ")" "\033[0m" "\033[1m" " [%s/%d T%p \"%s\" @%lx]: " "Debug: Invalid size %lu." ">>" "\033[0m" "\n", 0lu, __func__, 663, ksCurThread, ((debug_tcb_t *)(((cte_t *)((word_t)(ksCurThread)&~((1ul << (12)) - 1ul)))+(tcbCNodeEntries)))->tcbName, (word_t)getRestartPC(ksCurThread), size); } while (0);
        current_syscall_error.type = seL4_InvalidArgument;
        current_syscall_error.invalidArgumentNumber = 3;
        return EXCEPTION_SYSCALL_ERROR;
    }
    if (size > 0 && vaddr & (size - 1)) {
        /* Just Don't allow unaligned watchpoints. They are undefined
         * both ARM and x86.
         *
         * X86: Intel manuals, vol3, 17.2.5:
         *  "Two-byte ranges must be aligned on word boundaries; 4-byte
         *   ranges must be aligned on doubleword boundaries"
         *  "Unaligned data or I/O breakpoint addresses do not yield valid
         *   results"
         *
         * ARM: ARMv7 manual, C11.11.44:
         *  "A DBGWVR is programmed with a word-aligned address."
         */
        do { printf("\033[0m" "\033[1m" "<<" "\033[0m" "\033[32m" "seL4(CPU %" "lu" ")" "\033[0m" "\033[1m" " [%s/%d T%p \"%s\" @%lx]: " "Debug: Unaligned data watchpoint address %lx (size %lx) " "rejected.\n" ">>" "\033[0m" "\n", 0lu, __func__, 681, ksCurThread, ((debug_tcb_t *)(((cte_t *)((word_t)(ksCurThread)&~((1ul << (12)) - 1ul)))+(tcbCNodeEntries)))->tcbName, (word_t)getRestartPC(ksCurThread), vaddr, size); } while (0)

                              ;

        current_syscall_error.type = seL4_AlignmentError;
        return EXCEPTION_SYSCALL_ERROR;
    }

    error = Arch_decodeSetBreakpoint(tcb, bp_num, vaddr, type, size, rw);
    if (error.type != seL4_NoError) {
        current_syscall_error = error;
        return EXCEPTION_SYSCALL_ERROR;
    }

    setThreadState(ksCurThread, ThreadState_Restart);
    return invokeSetBreakpoint(tcb, bp_num,
                               vaddr, type, size, rw);
}

static exception_t invokeGetBreakpoint(bool_t call, word_t *buffer, tcb_t *tcb, uint16_t bp_num)
{
    tcb_t *thread;
    thread = ksCurThread;
    getBreakpoint_t res;
    res = getBreakpoint(tcb, bp_num);
    if (call) {
        setRegister(thread, badgeRegister, 0);
        setMR(ksCurThread, buffer, 0, res.vaddr);
        setMR(ksCurThread, buffer, 1, res.type);
        setMR(ksCurThread, buffer, 2, res.size);
        setMR(ksCurThread, buffer, 3, res.rw);
        setMR(ksCurThread, buffer, 4, res.is_enabled);
        setRegister(thread, msgInfoRegister, wordFromMessageInfo(
                        seL4_MessageInfo_new(0, 0, 0, 5)));
    }
    setThreadState(ksCurThread, ThreadState_Running);
    return EXCEPTION_NONE;
}

static exception_t decodeGetBreakpoint(cap_t cap, bool_t call, word_t *buffer)
{
    tcb_t *tcb;
    uint16_t bp_num;
    syscall_error_t error;

    tcb = ((tcb_t *)(cap_thread_cap_get_capTCBPtr(cap)));
    bp_num = getSyscallArg(0, buffer);

    error = Arch_decodeGetBreakpoint(tcb, bp_num);
    if (error.type != seL4_NoError) {
        current_syscall_error = error;
        return EXCEPTION_SYSCALL_ERROR;
    }

    setThreadState(ksCurThread, ThreadState_Restart);
    return invokeGetBreakpoint(call, buffer, tcb, bp_num);
}

static exception_t invokeUnsetBreakpoint(tcb_t *tcb, uint16_t bp_num)
{
    /* Maintain the bitfield of in-use breakpoints. */
    unsetBreakpoint(tcb, bp_num);
    unsetBreakpointUsedFlag(tcb, bp_num);
    return EXCEPTION_NONE;
}

static exception_t decodeUnsetBreakpoint(cap_t cap, word_t *buffer)
{
    tcb_t *tcb;
    uint16_t bp_num;
    syscall_error_t error;

    tcb = ((tcb_t *)(cap_thread_cap_get_capTCBPtr(cap)));
    bp_num = getSyscallArg(0, buffer);

    error = Arch_decodeUnsetBreakpoint(tcb, bp_num);
    if (error.type != seL4_NoError) {
        current_syscall_error = error;
        return EXCEPTION_SYSCALL_ERROR;
    }

    setThreadState(ksCurThread, ThreadState_Restart);
    return invokeUnsetBreakpoint(tcb, bp_num);
}


static exception_t invokeSetTLSBase(tcb_t *thread, word_t tls_base)
{
    setRegister(thread, TLS_BASE, tls_base);
    if (thread == ksCurThread) {
        /* If this is the current thread force a reschedule to ensure that any changes
         * to the TLS_BASE are realized */
        rescheduleRequired();
    }

    return EXCEPTION_NONE;
}

static exception_t decodeSetTLSBase(cap_t cap, word_t length, word_t *buffer)
{
    word_t tls_base;

    if (length < 1) {
        do { printf("\033[0m" "\033[1m" "<<" "\033[0m" "\033[32m" "seL4(CPU %" "lu" ")" "\033[0m" "\033[1m" " [%s/%d T%p \"%s\" @%lx]: " "TCB SetTLSBase: Truncated message." ">>" "\033[0m" "\n", 0lu, __func__, 784, ksCurThread, ((debug_tcb_t *)(((cte_t *)((word_t)(ksCurThread)&~((1ul << (12)) - 1ul)))+(tcbCNodeEntries)))->tcbName, (word_t)getRestartPC(ksCurThread)); } while (0);
        current_syscall_error.type = seL4_TruncatedMessage;
        return EXCEPTION_SYSCALL_ERROR;
    }

    tls_base = getSyscallArg(0, buffer);

    setThreadState(ksCurThread, ThreadState_Restart);
    return invokeSetTLSBase(((tcb_t *)(cap_thread_cap_get_capTCBPtr(cap))), tls_base);
}

static void invokeSetFlags(tcb_t *thread, word_t clear, word_t set, bool_t call)
{
    tcb_t *cur_thread = ksCurThread;
    word_t flags = thread->tcbFlags;

    flags &= ~clear;
    flags |= set & seL4_TCBFlag_MASK;
    thread->tcbFlags = flags;


    /* Save current FPU state before disabling FPU: */
    if (flags & seL4_TCBFlag_fpuDisabled) {
        fpuRelease(thread);
    }

    if (call) {
        word_t *ipcBuffer = lookupIPCBuffer(true, cur_thread);
        setRegister(cur_thread, badgeRegister, 0);
        unsigned int length = setMR(cur_thread, ipcBuffer, 0, flags);
        setRegister(cur_thread, msgInfoRegister, wordFromMessageInfo(
                        seL4_MessageInfo_new(0, 0, 0, length)));
    }
    setThreadState(cur_thread, ThreadState_Running);
}

static exception_t decodeSetFlags(cap_t cap, word_t length, bool_t call, word_t *buffer)
{
    tcb_t *thread = ((tcb_t *)(cap_thread_cap_get_capTCBPtr(cap)));

    if (length < 2) {
        do { printf("\033[0m" "\033[1m" "<<" "\033[0m" "\033[32m" "seL4(CPU %" "lu" ")" "\033[0m" "\033[1m" " [%s/%d T%p \"%s\" @%lx]: " "TCB SetFlags: Truncated message." ">>" "\033[0m" "\n", 0lu, __func__, 825, ksCurThread, ((debug_tcb_t *)(((cte_t *)((word_t)(ksCurThread)&~((1ul << (12)) - 1ul)))+(tcbCNodeEntries)))->tcbName, (word_t)getRestartPC(ksCurThread)); } while (0);
        current_syscall_error.type = seL4_TruncatedMessage;
        return EXCEPTION_SYSCALL_ERROR;
    }

    word_t clear = getSyscallArg(0, buffer);
    word_t set = getSyscallArg(1, buffer);

    setThreadState(ksCurThread, ThreadState_Restart);
    invokeSetFlags(thread, clear, set, call);
    return EXCEPTION_NONE;
}

/* The following functions sit in the syscall error monad, but include the
 * exception cases for the preemptible bottom end, as they call the invoke
 * functions directly.  This is a significant deviation from the Haskell
 * spec. */
exception_t decodeTCBInvocation(word_t invLabel, word_t length, cap_t cap,
                                cte_t *slot, bool_t call, word_t *buffer)
{
    /* Stall the core if we are operating on a remote TCB that is currently running */
   

    switch (invLabel) {
    case TCBReadRegisters:
        /* Second level of decoding */
        return decodeReadRegisters(cap, length, call, buffer);

    case TCBWriteRegisters:
        return decodeWriteRegisters(cap, length, buffer);

    case TCBCopyRegisters:
        return decodeCopyRegisters(cap, length, buffer);

    case TCBSuspend:
        /* Jump straight to the invoke */
        setThreadState(ksCurThread, ThreadState_Restart);
        return invokeTCB_Suspend(
                   ((tcb_t *)(cap_thread_cap_get_capTCBPtr(cap))));

    case TCBResume:
        setThreadState(ksCurThread, ThreadState_Restart);
        return invokeTCB_Resume(
                   ((tcb_t *)(cap_thread_cap_get_capTCBPtr(cap))));

    case TCBConfigure:
        return decodeTCBConfigure(cap, length, slot, buffer);

    case TCBSetPriority:
        return decodeSetPriority(cap, length, buffer);

    case TCBSetMCPriority:
        return decodeSetMCPriority(cap, length, buffer);

    case TCBSetSchedParams:



        return decodeSetSchedParams(cap, length, buffer);


    case TCBSetIPCBuffer:
        return decodeSetIPCBuffer(cap, length, slot, buffer);

    case TCBSetSpace:
        return decodeSetSpace(cap, length, slot, buffer);

    case TCBBindNotification:
        return decodeBindNotification(cap);

    case TCBUnbindNotification:
        return decodeUnbindNotification(cap);
# 908 "/home/b/test/sel4test/kernel/src/object/tcb.c"
        /* There is no notion of arch specific TCB invocations so this needs to go here */






    case TCBConfigureSingleStepping:
        return decodeConfigureSingleStepping(cap, call, buffer);

    case TCBSetBreakpoint:
        return decodeSetBreakpoint(cap, buffer);

    case TCBGetBreakpoint:
        return decodeGetBreakpoint(cap, call, buffer);

    case TCBUnsetBreakpoint:
        return decodeUnsetBreakpoint(cap, buffer);


    case TCBSetTLSBase:
        return decodeSetTLSBase(cap, length, buffer);

    case TCBSetFlags:
        return decodeSetFlags(cap, length, call, buffer);

    default:
        /* Haskell: "throw IllegalOperation" */
        do { printf("\033[0m" "\033[1m" "<<" "\033[0m" "\033[32m" "seL4(CPU %" "lu" ")" "\033[0m" "\033[1m" " [%s/%d T%p \"%s\" @%lx]: " "TCB: Illegal operation." ">>" "\033[0m" "\n", 0lu, __func__, 936, ksCurThread, ((debug_tcb_t *)(((cte_t *)((word_t)(ksCurThread)&~((1ul << (12)) - 1ul)))+(tcbCNodeEntries)))->tcbName, (word_t)getRestartPC(ksCurThread)); } while (0);
        current_syscall_error.type = seL4_IllegalOperation;
        return EXCEPTION_SYSCALL_ERROR;
    }
}

enum CopyRegistersFlags {
    CopyRegisters_suspendSource = 0,
    CopyRegisters_resumeTarget = 1,
    CopyRegisters_transferFrame = 2,
    CopyRegisters_transferInteger = 3
};

exception_t decodeCopyRegisters(cap_t cap, word_t length, word_t *buffer)
{
    word_t transferArch;
    tcb_t *srcTCB;
    cap_t source_cap;
    word_t flags;

    if (length < 1 || current_extra_caps.excaprefs[0] == ((void *)0)) {
        do { printf("\033[0m" "\033[1m" "<<" "\033[0m" "\033[32m" "seL4(CPU %" "lu" ")" "\033[0m" "\033[1m" " [%s/%d T%p \"%s\" @%lx]: " "TCB CopyRegisters: Truncated message." ">>" "\033[0m" "\n", 0lu, __func__, 957, ksCurThread, ((debug_tcb_t *)(((cte_t *)((word_t)(ksCurThread)&~((1ul << (12)) - 1ul)))+(tcbCNodeEntries)))->tcbName, (word_t)getRestartPC(ksCurThread)); } while (0);
        current_syscall_error.type = seL4_TruncatedMessage;
        return EXCEPTION_SYSCALL_ERROR;
    }

    flags = getSyscallArg(0, buffer);

    transferArch = Arch_decodeTransfer(flags >> 8);

    source_cap = current_extra_caps.excaprefs[0]->cap;

    if (cap_get_capType(source_cap) == cap_thread_cap) {
        srcTCB = ((tcb_t *)(cap_thread_cap_get_capTCBPtr(source_cap)));
    } else {
        do { printf("\033[0m" "\033[1m" "<<" "\033[0m" "\033[32m" "seL4(CPU %" "lu" ")" "\033[0m" "\033[1m" " [%s/%d T%p \"%s\" @%lx]: " "TCB CopyRegisters: Invalid source TCB." ">>" "\033[0m" "\n", 0lu, __func__, 971, ksCurThread, ((debug_tcb_t *)(((cte_t *)((word_t)(ksCurThread)&~((1ul << (12)) - 1ul)))+(tcbCNodeEntries)))->tcbName, (word_t)getRestartPC(ksCurThread)); } while (0);
        current_syscall_error.type = seL4_InvalidCapability;
        current_syscall_error.invalidCapNumber = 1;
        return EXCEPTION_SYSCALL_ERROR;
    }

    setThreadState(ksCurThread, ThreadState_Restart);
    return invokeTCB_CopyRegisters(
               ((tcb_t *)(cap_thread_cap_get_capTCBPtr(cap))), srcTCB,
               flags & (1ul << (CopyRegisters_suspendSource)),
               flags & (1ul << (CopyRegisters_resumeTarget)),
               flags & (1ul << (CopyRegisters_transferFrame)),
               flags & (1ul << (CopyRegisters_transferInteger)),
               transferArch);

}

enum ReadRegistersFlags {
    ReadRegisters_suspend = 0
};

exception_t decodeReadRegisters(cap_t cap, word_t length, bool_t call,
                                word_t *buffer)
{
    word_t transferArch, flags, n;
    tcb_t *thread;

    if (length < 2) {
        do { printf("\033[0m" "\033[1m" "<<" "\033[0m" "\033[32m" "seL4(CPU %" "lu" ")" "\033[0m" "\033[1m" " [%s/%d T%p \"%s\" @%lx]: " "TCB ReadRegisters: Truncated message." ">>" "\033[0m" "\n", 0lu, __func__, 999, ksCurThread, ((debug_tcb_t *)(((cte_t *)((word_t)(ksCurThread)&~((1ul << (12)) - 1ul)))+(tcbCNodeEntries)))->tcbName, (word_t)getRestartPC(ksCurThread)); } while (0);
        current_syscall_error.type = seL4_TruncatedMessage;
        return EXCEPTION_SYSCALL_ERROR;
    }

    flags = getSyscallArg(0, buffer);
    n = getSyscallArg(1, buffer);

    if (n < 1 || n > n_frameRegisters + n_gpRegisters) {
        do { printf("\033[0m" "\033[1m" "<<" "\033[0m" "\033[32m" "seL4(CPU %" "lu" ")" "\033[0m" "\033[1m" " [%s/%d T%p \"%s\" @%lx]: " "TCB ReadRegisters: Attempted to read an invalid number of registers (%d)." ">>" "\033[0m" "\n", 0lu, __func__, 1008, ksCurThread, ((debug_tcb_t *)(((cte_t *)((word_t)(ksCurThread)&~((1ul << (12)) - 1ul)))+(tcbCNodeEntries)))->tcbName, (word_t)getRestartPC(ksCurThread), (int)n); } while (0)
                         ;
        current_syscall_error.type = seL4_RangeError;
        current_syscall_error.rangeErrorMin = 1;
        current_syscall_error.rangeErrorMax = n_frameRegisters +
                                              n_gpRegisters;
        return EXCEPTION_SYSCALL_ERROR;
    }

    transferArch = Arch_decodeTransfer(flags >> 8);

    thread = ((tcb_t *)(cap_thread_cap_get_capTCBPtr(cap)));
    if (thread == ksCurThread) {
        do { printf("\033[0m" "\033[1m" "<<" "\033[0m" "\033[32m" "seL4(CPU %" "lu" ")" "\033[0m" "\033[1m" " [%s/%d T%p \"%s\" @%lx]: " "TCB ReadRegisters: Attempted to read our own registers." ">>" "\033[0m" "\n", 0lu, __func__, 1021, ksCurThread, ((debug_tcb_t *)(((cte_t *)((word_t)(ksCurThread)&~((1ul << (12)) - 1ul)))+(tcbCNodeEntries)))->tcbName, (word_t)getRestartPC(ksCurThread)); } while (0);
        current_syscall_error.type = seL4_IllegalOperation;
        return EXCEPTION_SYSCALL_ERROR;
    }

    setThreadState(ksCurThread, ThreadState_Restart);
    return invokeTCB_ReadRegisters(
               ((tcb_t *)(cap_thread_cap_get_capTCBPtr(cap))),
               flags & (1ul << (ReadRegisters_suspend)),
               n, transferArch, call);
}

enum WriteRegistersFlags {
    WriteRegisters_resume = 0
};

exception_t decodeWriteRegisters(cap_t cap, word_t length, word_t *buffer)
{
    word_t flags, w;
    word_t transferArch;
    tcb_t *thread;

    if (length < 2) {
        do { printf("\033[0m" "\033[1m" "<<" "\033[0m" "\033[32m" "seL4(CPU %" "lu" ")" "\033[0m" "\033[1m" " [%s/%d T%p \"%s\" @%lx]: " "TCB WriteRegisters: Truncated message." ">>" "\033[0m" "\n", 0lu, __func__, 1044, ksCurThread, ((debug_tcb_t *)(((cte_t *)((word_t)(ksCurThread)&~((1ul << (12)) - 1ul)))+(tcbCNodeEntries)))->tcbName, (word_t)getRestartPC(ksCurThread)); } while (0);
        current_syscall_error.type = seL4_TruncatedMessage;
        return EXCEPTION_SYSCALL_ERROR;
    }

    flags = getSyscallArg(0, buffer);
    w = getSyscallArg(1, buffer);

    if (length - 2 < w) {
        do { printf("\033[0m" "\033[1m" "<<" "\033[0m" "\033[32m" "seL4(CPU %" "lu" ")" "\033[0m" "\033[1m" " [%s/%d T%p \"%s\" @%lx]: " "TCB WriteRegisters: Message too short for requested write size (%d/%d)." ">>" "\033[0m" "\n", 0lu, __func__, 1053, ksCurThread, ((debug_tcb_t *)(((cte_t *)((word_t)(ksCurThread)&~((1ul << (12)) - 1ul)))+(tcbCNodeEntries)))->tcbName, (word_t)getRestartPC(ksCurThread), (int)(length - 2), (int)w); } while (0)
                                            ;
        current_syscall_error.type = seL4_TruncatedMessage;
        return EXCEPTION_SYSCALL_ERROR;
    }

    transferArch = Arch_decodeTransfer(flags >> 8);

    thread = ((tcb_t *)(cap_thread_cap_get_capTCBPtr(cap)));
    if (thread == ksCurThread) {
        do { printf("\033[0m" "\033[1m" "<<" "\033[0m" "\033[32m" "seL4(CPU %" "lu" ")" "\033[0m" "\033[1m" " [%s/%d T%p \"%s\" @%lx]: " "TCB WriteRegisters: Attempted to write our own registers." ">>" "\033[0m" "\n", 0lu, __func__, 1063, ksCurThread, ((debug_tcb_t *)(((cte_t *)((word_t)(ksCurThread)&~((1ul << (12)) - 1ul)))+(tcbCNodeEntries)))->tcbName, (word_t)getRestartPC(ksCurThread)); } while (0);
        current_syscall_error.type = seL4_IllegalOperation;
        return EXCEPTION_SYSCALL_ERROR;
    }

    setThreadState(ksCurThread, ThreadState_Restart);
    return invokeTCB_WriteRegisters(thread,
                                    flags & (1ul << (WriteRegisters_resume)),
                                    w, transferArch, buffer);
}
# 1095 "/home/b/test/sel4test/kernel/src/object/tcb.c"
/* TCBConfigure batches SetIPCBuffer and parts of SetSpace. */
exception_t decodeTCBConfigure(cap_t cap, word_t length, cte_t *slot, word_t *buffer)
{
    cte_t *bufferSlot, *cRootSlot, *vRootSlot;
    cap_t bufferCap, cRootCap, vRootCap;
    deriveCap_ret_t dc_ret;
    word_t cRootData, vRootData, bufferAddr;





    if (length < 4 || current_extra_caps.excaprefs[0] == ((void *)0)
        || current_extra_caps.excaprefs[1] == ((void *)0)
        || current_extra_caps.excaprefs[2] == ((void *)0)) {
        do { printf("\033[0m" "\033[1m" "<<" "\033[0m" "\033[32m" "seL4(CPU %" "lu" ")" "\033[0m" "\033[1m" " [%s/%d T%p \"%s\" @%lx]: " "TCB Configure: Truncated message." ">>" "\033[0m" "\n", 0lu, __func__, 1110, ksCurThread, ((debug_tcb_t *)(((cte_t *)((word_t)(ksCurThread)&~((1ul << (12)) - 1ul)))+(tcbCNodeEntries)))->tcbName, (word_t)getRestartPC(ksCurThread)); } while (0);
        current_syscall_error.type = seL4_TruncatedMessage;
        return EXCEPTION_SYSCALL_ERROR;
    }






    cptr_t faultEP = getSyscallArg(0, buffer);
    cRootData = getSyscallArg(1, buffer);
    vRootData = getSyscallArg(2, buffer);
    bufferAddr = getSyscallArg(3, buffer);


    cRootSlot = current_extra_caps.excaprefs[0];
    cRootCap = current_extra_caps.excaprefs[0]->cap;
    vRootSlot = current_extra_caps.excaprefs[1];
    vRootCap = current_extra_caps.excaprefs[1]->cap;
    bufferSlot = current_extra_caps.excaprefs[2];
    bufferCap = current_extra_caps.excaprefs[2]->cap;

    if (bufferAddr == 0) {
        bufferSlot = ((void *)0);
    } else {
        dc_ret = deriveCap(bufferSlot, bufferCap);
        if (dc_ret.status != EXCEPTION_NONE) {
            return dc_ret.status;
        }
        bufferCap = dc_ret.cap;

        exception_t e = checkValidIPCBuffer(bufferAddr, bufferCap);
        if (e != EXCEPTION_NONE) {
            return e;
        }
    }

    if (slotCapLongRunningDelete(
            (((cte_t *)((word_t)(cap_thread_cap_get_capTCBPtr(cap))&~((1ul << (12)) - 1ul)))+(tcbCTable))) ||
        slotCapLongRunningDelete(
            (((cte_t *)((word_t)(cap_thread_cap_get_capTCBPtr(cap))&~((1ul << (12)) - 1ul)))+(tcbVTable)))) {
        do { printf("\033[0m" "\033[1m" "<<" "\033[0m" "\033[32m" "seL4(CPU %" "lu" ")" "\033[0m" "\033[1m" " [%s/%d T%p \"%s\" @%lx]: " "TCB Configure: CSpace or VSpace currently being deleted." ">>" "\033[0m" "\n", 0lu, __func__, 1152, ksCurThread, ((debug_tcb_t *)(((cte_t *)((word_t)(ksCurThread)&~((1ul << (12)) - 1ul)))+(tcbCNodeEntries)))->tcbName, (word_t)getRestartPC(ksCurThread)); } while (0);
        current_syscall_error.type = seL4_IllegalOperation;
        return EXCEPTION_SYSCALL_ERROR;
    }

    if (cRootData != 0) {
        cRootCap = updateCapData(false, cRootData, cRootCap);
    }

    dc_ret = deriveCap(cRootSlot, cRootCap);
    if (dc_ret.status != EXCEPTION_NONE) {
        return dc_ret.status;
    }
    cRootCap = dc_ret.cap;

    if (cap_get_capType(cRootCap) != cap_cnode_cap) {
        do { printf("\033[0m" "\033[1m" "<<" "\033[0m" "\033[32m" "seL4(CPU %" "lu" ")" "\033[0m" "\033[1m" " [%s/%d T%p \"%s\" @%lx]: " "TCB Configure: CSpace cap is invalid." ">>" "\033[0m" "\n", 0lu, __func__, 1168, ksCurThread, ((debug_tcb_t *)(((cte_t *)((word_t)(ksCurThread)&~((1ul << (12)) - 1ul)))+(tcbCNodeEntries)))->tcbName, (word_t)getRestartPC(ksCurThread)); } while (0);
        current_syscall_error.type = seL4_IllegalOperation;
        return EXCEPTION_SYSCALL_ERROR;
    }

    if (vRootData != 0) {
        vRootCap = updateCapData(false, vRootData, vRootCap);
    }

    dc_ret = deriveCap(vRootSlot, vRootCap);
    if (dc_ret.status != EXCEPTION_NONE) {
        return dc_ret.status;
    }
    vRootCap = dc_ret.cap;

    if (!isValidVTableRoot(vRootCap)) {
        do { printf("\033[0m" "\033[1m" "<<" "\033[0m" "\033[32m" "seL4(CPU %" "lu" ")" "\033[0m" "\033[1m" " [%s/%d T%p \"%s\" @%lx]: " "TCB Configure: VSpace cap is invalid." ">>" "\033[0m" "\n", 0lu, __func__, 1184, ksCurThread, ((debug_tcb_t *)(((cte_t *)((word_t)(ksCurThread)&~((1ul << (12)) - 1ul)))+(tcbCNodeEntries)))->tcbName, (word_t)getRestartPC(ksCurThread)); } while (0);
        current_syscall_error.type = seL4_IllegalOperation;
        return EXCEPTION_SYSCALL_ERROR;
    }

    setThreadState(ksCurThread, ThreadState_Restart);
# 1201 "/home/b/test/sel4test/kernel/src/object/tcb.c"
    return invokeTCB_ThreadControl(
               ((tcb_t *)(cap_thread_cap_get_capTCBPtr(cap))), slot,
               faultEP, 0, 0,
               cRootCap, cRootSlot,
               vRootCap, vRootSlot,
               bufferAddr, bufferCap,
               bufferSlot, thread_control_update_space |
               thread_control_update_ipc_buffer);

}

exception_t decodeSetPriority(cap_t cap, word_t length, word_t *buffer)
{
    if (length < 1 || current_extra_caps.excaprefs[0] == ((void *)0)) {
        do { printf("\033[0m" "\033[1m" "<<" "\033[0m" "\033[32m" "seL4(CPU %" "lu" ")" "\033[0m" "\033[1m" " [%s/%d T%p \"%s\" @%lx]: " "TCB SetPriority: Truncated message." ">>" "\033[0m" "\n", 0lu, __func__, 1215, ksCurThread, ((debug_tcb_t *)(((cte_t *)((word_t)(ksCurThread)&~((1ul << (12)) - 1ul)))+(tcbCNodeEntries)))->tcbName, (word_t)getRestartPC(ksCurThread)); } while (0);
        current_syscall_error.type = seL4_TruncatedMessage;
        return EXCEPTION_SYSCALL_ERROR;
    }

    prio_t newPrio = getSyscallArg(0, buffer);
    cap_t authCap = current_extra_caps.excaprefs[0]->cap;

    if (cap_get_capType(authCap) != cap_thread_cap) {
        do { printf("\033[0m" "\033[1m" "<<" "\033[0m" "\033[32m" "seL4(CPU %" "lu" ")" "\033[0m" "\033[1m" " [%s/%d T%p \"%s\" @%lx]: " "Set priority: authority cap not a TCB." ">>" "\033[0m" "\n", 0lu, __func__, 1224, ksCurThread, ((debug_tcb_t *)(((cte_t *)((word_t)(ksCurThread)&~((1ul << (12)) - 1ul)))+(tcbCNodeEntries)))->tcbName, (word_t)getRestartPC(ksCurThread)); } while (0);
        current_syscall_error.type = seL4_InvalidCapability;
        current_syscall_error.invalidCapNumber = 1;
        return EXCEPTION_SYSCALL_ERROR;
    }

    tcb_t *authTCB = ((tcb_t *)(cap_thread_cap_get_capTCBPtr(authCap)));
    exception_t status = checkPrio(newPrio, authTCB);
    if (status != EXCEPTION_NONE) {
        do { printf("\033[0m" "\033[1m" "<<" "\033[0m" "\033[32m" "seL4(CPU %" "lu" ")" "\033[0m" "\033[1m" " [%s/%d T%p \"%s\" @%lx]: " "TCB SetPriority: Requested priority %lu too high (max %lu)." ">>" "\033[0m" "\n", 0lu, __func__, 1233, ksCurThread, ((debug_tcb_t *)(((cte_t *)((word_t)(ksCurThread)&~((1ul << (12)) - 1ul)))+(tcbCNodeEntries)))->tcbName, (word_t)getRestartPC(ksCurThread), (unsigned long) newPrio, (unsigned long) authTCB->tcbMCP); } while (0)
                                                                           ;
        return status;
    }

    setThreadState(ksCurThread, ThreadState_Restart);







    return invokeTCB_ThreadControl(
               ((tcb_t *)(cap_thread_cap_get_capTCBPtr(cap))), ((void *)0),
               0, 0, newPrio,
               cap_null_cap_new(), ((void *)0),
               cap_null_cap_new(), ((void *)0),
               0, cap_null_cap_new(),
               ((void *)0), thread_control_update_priority);

}

exception_t decodeSetMCPriority(cap_t cap, word_t length, word_t *buffer)
{
    if (length < 1 || current_extra_caps.excaprefs[0] == ((void *)0)) {
        do { printf("\033[0m" "\033[1m" "<<" "\033[0m" "\033[32m" "seL4(CPU %" "lu" ")" "\033[0m" "\033[1m" " [%s/%d T%p \"%s\" @%lx]: " "TCB SetMCPriority: Truncated message." ">>" "\033[0m" "\n", 0lu, __func__, 1259, ksCurThread, ((debug_tcb_t *)(((cte_t *)((word_t)(ksCurThread)&~((1ul << (12)) - 1ul)))+(tcbCNodeEntries)))->tcbName, (word_t)getRestartPC(ksCurThread)); } while (0);
        current_syscall_error.type = seL4_TruncatedMessage;
        return EXCEPTION_SYSCALL_ERROR;
    }

    prio_t newMcp = getSyscallArg(0, buffer);
    cap_t authCap = current_extra_caps.excaprefs[0]->cap;

    if (cap_get_capType(authCap) != cap_thread_cap) {
        do { printf("\033[0m" "\033[1m" "<<" "\033[0m" "\033[32m" "seL4(CPU %" "lu" ")" "\033[0m" "\033[1m" " [%s/%d T%p \"%s\" @%lx]: " "TCB SetMCPriority: authority cap not a TCB." ">>" "\033[0m" "\n", 0lu, __func__, 1268, ksCurThread, ((debug_tcb_t *)(((cte_t *)((word_t)(ksCurThread)&~((1ul << (12)) - 1ul)))+(tcbCNodeEntries)))->tcbName, (word_t)getRestartPC(ksCurThread)); } while (0);
        current_syscall_error.type = seL4_InvalidCapability;
        current_syscall_error.invalidCapNumber = 1;
        return EXCEPTION_SYSCALL_ERROR;
    }

    tcb_t *authTCB = ((tcb_t *)(cap_thread_cap_get_capTCBPtr(authCap)));
    exception_t status = checkPrio(newMcp, authTCB);
    if (status != EXCEPTION_NONE) {
        do { printf("\033[0m" "\033[1m" "<<" "\033[0m" "\033[32m" "seL4(CPU %" "lu" ")" "\033[0m" "\033[1m" " [%s/%d T%p \"%s\" @%lx]: " "TCB SetMCPriority: Requested maximum controlled priority %lu too high (max %lu)." ">>" "\033[0m" "\n", 0lu, __func__, 1277, ksCurThread, ((debug_tcb_t *)(((cte_t *)((word_t)(ksCurThread)&~((1ul << (12)) - 1ul)))+(tcbCNodeEntries)))->tcbName, (word_t)getRestartPC(ksCurThread), (unsigned long) newMcp, (unsigned long) authTCB->tcbMCP); } while (0)
                                                                          ;
        return status;
    }

    setThreadState(ksCurThread, ThreadState_Restart);







    return invokeTCB_ThreadControl(
               ((tcb_t *)(cap_thread_cap_get_capTCBPtr(cap))), ((void *)0),
               0, newMcp, 0,
               cap_null_cap_new(), ((void *)0),
               cap_null_cap_new(), ((void *)0),
               0, cap_null_cap_new(),
               ((void *)0), thread_control_update_mcp);

}
# 1335 "/home/b/test/sel4test/kernel/src/object/tcb.c"
exception_t decodeSetSchedParams(cap_t cap, word_t length, word_t *buffer)

{
    if (length < 2 || current_extra_caps.excaprefs[0] == ((void *)0)



       ) {
        do { printf("\033[0m" "\033[1m" "<<" "\033[0m" "\033[32m" "seL4(CPU %" "lu" ")" "\033[0m" "\033[1m" " [%s/%d T%p \"%s\" @%lx]: " "TCB SetSchedParams: Truncated message." ">>" "\033[0m" "\n", 0lu, __func__, 1343, ksCurThread, ((debug_tcb_t *)(((cte_t *)((word_t)(ksCurThread)&~((1ul << (12)) - 1ul)))+(tcbCNodeEntries)))->tcbName, (word_t)getRestartPC(ksCurThread)); } while (0);
        current_syscall_error.type = seL4_TruncatedMessage;
        return EXCEPTION_SYSCALL_ERROR;
    }

    prio_t newMcp = getSyscallArg(0, buffer);
    prio_t newPrio = getSyscallArg(1, buffer);
    cap_t authCap = current_extra_caps.excaprefs[0]->cap;






    if (cap_get_capType(authCap) != cap_thread_cap) {
        do { printf("\033[0m" "\033[1m" "<<" "\033[0m" "\033[32m" "seL4(CPU %" "lu" ")" "\033[0m" "\033[1m" " [%s/%d T%p \"%s\" @%lx]: " "TCB SetSchedParams: authority cap not a TCB." ">>" "\033[0m" "\n", 0lu, __func__, 1358, ksCurThread, ((debug_tcb_t *)(((cte_t *)((word_t)(ksCurThread)&~((1ul << (12)) - 1ul)))+(tcbCNodeEntries)))->tcbName, (word_t)getRestartPC(ksCurThread)); } while (0);
        current_syscall_error.type = seL4_InvalidCapability;
        current_syscall_error.invalidCapNumber = 1;
        return EXCEPTION_SYSCALL_ERROR;
    }

    tcb_t *authTCB = ((tcb_t *)(cap_thread_cap_get_capTCBPtr(authCap)));
    exception_t status = checkPrio(newMcp, authTCB);
    if (status != EXCEPTION_NONE) {
        do { printf("\033[0m" "\033[1m" "<<" "\033[0m" "\033[32m" "seL4(CPU %" "lu" ")" "\033[0m" "\033[1m" " [%s/%d T%p \"%s\" @%lx]: " "TCB SetSchedParams: Requested maximum controlled priority %lu too high (max %lu)." ">>" "\033[0m" "\n", 0lu, __func__, 1367, ksCurThread, ((debug_tcb_t *)(((cte_t *)((word_t)(ksCurThread)&~((1ul << (12)) - 1ul)))+(tcbCNodeEntries)))->tcbName, (word_t)getRestartPC(ksCurThread), (unsigned long) newMcp, (unsigned long) authTCB->tcbMCP); } while (0)
                                                                          ;
        return status;
    }

    status = checkPrio(newPrio, authTCB);
    if (status != EXCEPTION_NONE) {
        do { printf("\033[0m" "\033[1m" "<<" "\033[0m" "\033[32m" "seL4(CPU %" "lu" ")" "\033[0m" "\033[1m" " [%s/%d T%p \"%s\" @%lx]: " "TCB SetSchedParams: Requested priority %lu too high (max %lu)." ">>" "\033[0m" "\n", 0lu, __func__, 1374, ksCurThread, ((debug_tcb_t *)(((cte_t *)((word_t)(ksCurThread)&~((1ul << (12)) - 1ul)))+(tcbCNodeEntries)))->tcbName, (word_t)getRestartPC(ksCurThread), (unsigned long) newPrio, (unsigned long) authTCB->tcbMCP); } while (0)
                                                                           ;
        return status;
    }
# 1432 "/home/b/test/sel4test/kernel/src/object/tcb.c"
    setThreadState(ksCurThread, ThreadState_Restart);







    return invokeTCB_ThreadControl(
               ((tcb_t *)(cap_thread_cap_get_capTCBPtr(cap))), ((void *)0),
               0, newMcp, newPrio,
               cap_null_cap_new(), ((void *)0),
               cap_null_cap_new(), ((void *)0),
               0, cap_null_cap_new(),
               ((void *)0), thread_control_update_mcp |
               thread_control_update_priority);

}


exception_t decodeSetIPCBuffer(cap_t cap, word_t length, cte_t *slot, word_t *buffer)
{
    cptr_t cptr_bufferPtr;
    cap_t bufferCap;
    cte_t *bufferSlot;

    if (length < 1 || current_extra_caps.excaprefs[0] == ((void *)0)) {
        do { printf("\033[0m" "\033[1m" "<<" "\033[0m" "\033[32m" "seL4(CPU %" "lu" ")" "\033[0m" "\033[1m" " [%s/%d T%p \"%s\" @%lx]: " "TCB SetIPCBuffer: Truncated message." ">>" "\033[0m" "\n", 0lu, __func__, 1459, ksCurThread, ((debug_tcb_t *)(((cte_t *)((word_t)(ksCurThread)&~((1ul << (12)) - 1ul)))+(tcbCNodeEntries)))->tcbName, (word_t)getRestartPC(ksCurThread)); } while (0);
        current_syscall_error.type = seL4_TruncatedMessage;
        return EXCEPTION_SYSCALL_ERROR;
    }

    cptr_bufferPtr = getSyscallArg(0, buffer);
    bufferSlot = current_extra_caps.excaprefs[0];
    bufferCap = current_extra_caps.excaprefs[0]->cap;

    if (cptr_bufferPtr == 0) {
        bufferSlot = ((void *)0);
    } else {
        exception_t e;
        deriveCap_ret_t dc_ret;

        dc_ret = deriveCap(bufferSlot, bufferCap);
        if (dc_ret.status != EXCEPTION_NONE) {
            return dc_ret.status;
        }
        bufferCap = dc_ret.cap;
        e = checkValidIPCBuffer(cptr_bufferPtr, bufferCap);
        if (e != EXCEPTION_NONE) {
            return e;
        }
    }

    setThreadState(ksCurThread, ThreadState_Restart);
# 1496 "/home/b/test/sel4test/kernel/src/object/tcb.c"
    return invokeTCB_ThreadControl(
               ((tcb_t *)(cap_thread_cap_get_capTCBPtr(cap))), slot,
               0, 0, 0,
               cap_null_cap_new(), ((void *)0),
               cap_null_cap_new(), ((void *)0),
               cptr_bufferPtr, bufferCap,
               bufferSlot, thread_control_update_ipc_buffer);


}






exception_t decodeSetSpace(cap_t cap, word_t length, cte_t *slot, word_t *buffer)
{
    word_t cRootData, vRootData;
    cte_t *cRootSlot, *vRootSlot;
    cap_t cRootCap, vRootCap;
    deriveCap_ret_t dc_ret;

    if (length < 3 || current_extra_caps.excaprefs[0] == ((void *)0)
        || current_extra_caps.excaprefs[1] == ((void *)0)



       ) {
        do { printf("\033[0m" "\033[1m" "<<" "\033[0m" "\033[32m" "seL4(CPU %" "lu" ")" "\033[0m" "\033[1m" " [%s/%d T%p \"%s\" @%lx]: " "TCB SetSpace: Truncated message." ">>" "\033[0m" "\n", 0lu, __func__, 1525, ksCurThread, ((debug_tcb_t *)(((cte_t *)((word_t)(ksCurThread)&~((1ul << (12)) - 1ul)))+(tcbCNodeEntries)))->tcbName, (word_t)getRestartPC(ksCurThread)); } while (0);
        current_syscall_error.type = seL4_TruncatedMessage;
        return EXCEPTION_SYSCALL_ERROR;
    }
# 1541 "/home/b/test/sel4test/kernel/src/object/tcb.c"
    cptr_t faultEP = getSyscallArg(0, buffer);
    cRootData = getSyscallArg(1, buffer);
    vRootData = getSyscallArg(2, buffer);

    cRootSlot = current_extra_caps.excaprefs[0];
    cRootCap = current_extra_caps.excaprefs[0]->cap;
    vRootSlot = current_extra_caps.excaprefs[1];
    vRootCap = current_extra_caps.excaprefs[1]->cap;


    if (slotCapLongRunningDelete(
            (((cte_t *)((word_t)(cap_thread_cap_get_capTCBPtr(cap))&~((1ul << (12)) - 1ul)))+(tcbCTable))) ||
        slotCapLongRunningDelete(
            (((cte_t *)((word_t)(cap_thread_cap_get_capTCBPtr(cap))&~((1ul << (12)) - 1ul)))+(tcbVTable)))) {
        do { printf("\033[0m" "\033[1m" "<<" "\033[0m" "\033[32m" "seL4(CPU %" "lu" ")" "\033[0m" "\033[1m" " [%s/%d T%p \"%s\" @%lx]: " "TCB SetSpace: CSpace or VSpace currently being deleted." ">>" "\033[0m" "\n", 0lu, __func__, 1555, ksCurThread, ((debug_tcb_t *)(((cte_t *)((word_t)(ksCurThread)&~((1ul << (12)) - 1ul)))+(tcbCNodeEntries)))->tcbName, (word_t)getRestartPC(ksCurThread)); } while (0);
        current_syscall_error.type = seL4_IllegalOperation;
        return EXCEPTION_SYSCALL_ERROR;
    }

    if (cRootData != 0) {
        cRootCap = updateCapData(false, cRootData, cRootCap);
    }

    dc_ret = deriveCap(cRootSlot, cRootCap);
    if (dc_ret.status != EXCEPTION_NONE) {
        return dc_ret.status;
    }
    cRootCap = dc_ret.cap;

    if (cap_get_capType(cRootCap) != cap_cnode_cap) {
        do { printf("\033[0m" "\033[1m" "<<" "\033[0m" "\033[32m" "seL4(CPU %" "lu" ")" "\033[0m" "\033[1m" " [%s/%d T%p \"%s\" @%lx]: " "TCB SetSpace: Invalid CNode cap." ">>" "\033[0m" "\n", 0lu, __func__, 1571, ksCurThread, ((debug_tcb_t *)(((cte_t *)((word_t)(ksCurThread)&~((1ul << (12)) - 1ul)))+(tcbCNodeEntries)))->tcbName, (word_t)getRestartPC(ksCurThread)); } while (0);
        current_syscall_error.type = seL4_IllegalOperation;
        return EXCEPTION_SYSCALL_ERROR;
    }

    if (vRootData != 0) {
        vRootCap = updateCapData(false, vRootData, vRootCap);
    }

    dc_ret = deriveCap(vRootSlot, vRootCap);
    if (dc_ret.status != EXCEPTION_NONE) {
        return dc_ret.status;
    }
    vRootCap = dc_ret.cap;

    if (!isValidVTableRoot(vRootCap)) {
        do { printf("\033[0m" "\033[1m" "<<" "\033[0m" "\033[32m" "seL4(CPU %" "lu" ")" "\033[0m" "\033[1m" " [%s/%d T%p \"%s\" @%lx]: " "TCB SetSpace: Invalid VSpace cap." ">>" "\033[0m" "\n", 0lu, __func__, 1587, ksCurThread, ((debug_tcb_t *)(((cte_t *)((word_t)(ksCurThread)&~((1ul << (12)) - 1ul)))+(tcbCNodeEntries)))->tcbName, (word_t)getRestartPC(ksCurThread)); } while (0);
        current_syscall_error.type = seL4_IllegalOperation;
        return EXCEPTION_SYSCALL_ERROR;
    }
# 1602 "/home/b/test/sel4test/kernel/src/object/tcb.c"
    setThreadState(ksCurThread, ThreadState_Restart);
# 1612 "/home/b/test/sel4test/kernel/src/object/tcb.c"
    return invokeTCB_ThreadControl(
               ((tcb_t *)(cap_thread_cap_get_capTCBPtr(cap))), slot,
               faultEP,
               0, 0,
               cRootCap, cRootSlot,
               vRootCap, vRootSlot,
               0, cap_null_cap_new(), ((void *)0), thread_control_update_space);

}

exception_t decodeDomainInvocation(word_t invLabel, word_t length, word_t *buffer)
{
    dom_t domain;
    cap_t tcap;

    if (__builtin_expect(!!(invLabel != DomainSetSet), 0)) {
        current_syscall_error.type = seL4_IllegalOperation;
        return EXCEPTION_SYSCALL_ERROR;
    }

    if (__builtin_expect(!!(length == 0), 0)) {
        do { printf("\033[0m" "\033[1m" "<<" "\033[0m" "\033[32m" "seL4(CPU %" "lu" ")" "\033[0m" "\033[1m" " [%s/%d T%p \"%s\" @%lx]: " "Domain Configure: Truncated message." ">>" "\033[0m" "\n", 0lu, __func__, 1633, ksCurThread, ((debug_tcb_t *)(((cte_t *)((word_t)(ksCurThread)&~((1ul << (12)) - 1ul)))+(tcbCNodeEntries)))->tcbName, (word_t)getRestartPC(ksCurThread)); } while (0);
        current_syscall_error.type = seL4_TruncatedMessage;
        return EXCEPTION_SYSCALL_ERROR;
    } else {
        domain = getSyscallArg(0, buffer);
        if (domain >= numDomains) {
            do { printf("\033[0m" "\033[1m" "<<" "\033[0m" "\033[32m" "seL4(CPU %" "lu" ")" "\033[0m" "\033[1m" " [%s/%d T%p \"%s\" @%lx]: " "Domain Configure: invalid domain (%lu >= %u)." ">>" "\033[0m" "\n", 0lu, __func__, 1639, ksCurThread, ((debug_tcb_t *)(((cte_t *)((word_t)(ksCurThread)&~((1ul << (12)) - 1ul)))+(tcbCNodeEntries)))->tcbName, (word_t)getRestartPC(ksCurThread), domain, numDomains); } while (0)
                                         ;
            current_syscall_error.type = seL4_InvalidArgument;
            current_syscall_error.invalidArgumentNumber = 0;
            return EXCEPTION_SYSCALL_ERROR;
        }
    }

    if (__builtin_expect(!!(current_extra_caps.excaprefs[0] == ((void *)0)), 0)) {
        do { printf("\033[0m" "\033[1m" "<<" "\033[0m" "\033[32m" "seL4(CPU %" "lu" ")" "\033[0m" "\033[1m" " [%s/%d T%p \"%s\" @%lx]: " "Domain Configure: Truncated message." ">>" "\033[0m" "\n", 0lu, __func__, 1648, ksCurThread, ((debug_tcb_t *)(((cte_t *)((word_t)(ksCurThread)&~((1ul << (12)) - 1ul)))+(tcbCNodeEntries)))->tcbName, (word_t)getRestartPC(ksCurThread)); } while (0);
        current_syscall_error.type = seL4_TruncatedMessage;
        return EXCEPTION_SYSCALL_ERROR;
    }

    tcap = current_extra_caps.excaprefs[0]->cap;
    if (__builtin_expect(!!(cap_get_capType(tcap) != cap_thread_cap), 0)) {
        do { printf("\033[0m" "\033[1m" "<<" "\033[0m" "\033[32m" "seL4(CPU %" "lu" ")" "\033[0m" "\033[1m" " [%s/%d T%p \"%s\" @%lx]: " "Domain Configure: thread cap required." ">>" "\033[0m" "\n", 0lu, __func__, 1655, ksCurThread, ((debug_tcb_t *)(((cte_t *)((word_t)(ksCurThread)&~((1ul << (12)) - 1ul)))+(tcbCNodeEntries)))->tcbName, (word_t)getRestartPC(ksCurThread)); } while (0);
        current_syscall_error.type = seL4_InvalidArgument;
        current_syscall_error.invalidArgumentNumber = 1;
        return EXCEPTION_SYSCALL_ERROR;
    }
    setThreadState(ksCurThread, ThreadState_Restart);
    invokeDomainSetSet(((tcb_t *)(cap_thread_cap_get_capTCBPtr(tcap))), domain);
    return EXCEPTION_NONE;
}

void invokeDomainSetSet(tcb_t *tcb, dom_t domain)
{
    prepareSetDomain(tcb, domain);
    setDomain(tcb, domain);
}

exception_t decodeBindNotification(cap_t cap)
{
    notification_t *ntfnPtr;
    tcb_t *tcb;
    cap_t ntfn_cap;

    if (current_extra_caps.excaprefs[0] == ((void *)0)) {
        do { printf("\033[0m" "\033[1m" "<<" "\033[0m" "\033[32m" "seL4(CPU %" "lu" ")" "\033[0m" "\033[1m" " [%s/%d T%p \"%s\" @%lx]: " "TCB BindNotification: Truncated message." ">>" "\033[0m" "\n", 0lu, __func__, 1678, ksCurThread, ((debug_tcb_t *)(((cte_t *)((word_t)(ksCurThread)&~((1ul << (12)) - 1ul)))+(tcbCNodeEntries)))->tcbName, (word_t)getRestartPC(ksCurThread)); } while (0);
        current_syscall_error.type = seL4_TruncatedMessage;
        return EXCEPTION_SYSCALL_ERROR;
    }

    tcb = ((tcb_t *)(cap_thread_cap_get_capTCBPtr(cap)));

    if (tcb->tcbBoundNotification) {
        do { printf("\033[0m" "\033[1m" "<<" "\033[0m" "\033[32m" "seL4(CPU %" "lu" ")" "\033[0m" "\033[1m" " [%s/%d T%p \"%s\" @%lx]: " "TCB BindNotification: TCB already has a bound notification." ">>" "\033[0m" "\n", 0lu, __func__, 1686, ksCurThread, ((debug_tcb_t *)(((cte_t *)((word_t)(ksCurThread)&~((1ul << (12)) - 1ul)))+(tcbCNodeEntries)))->tcbName, (word_t)getRestartPC(ksCurThread)); } while (0);
        current_syscall_error.type = seL4_IllegalOperation;
        return EXCEPTION_SYSCALL_ERROR;
    }

    ntfn_cap = current_extra_caps.excaprefs[0]->cap;

    if (cap_get_capType(ntfn_cap) == cap_notification_cap) {
        ntfnPtr = ((notification_t *)(cap_notification_cap_get_capNtfnPtr(ntfn_cap)));
    } else {
        do { printf("\033[0m" "\033[1m" "<<" "\033[0m" "\033[32m" "seL4(CPU %" "lu" ")" "\033[0m" "\033[1m" " [%s/%d T%p \"%s\" @%lx]: " "TCB BindNotification: Notification is invalid." ">>" "\033[0m" "\n", 0lu, __func__, 1696, ksCurThread, ((debug_tcb_t *)(((cte_t *)((word_t)(ksCurThread)&~((1ul << (12)) - 1ul)))+(tcbCNodeEntries)))->tcbName, (word_t)getRestartPC(ksCurThread)); } while (0);
        current_syscall_error.type = seL4_IllegalOperation;
        return EXCEPTION_SYSCALL_ERROR;
    }

    if (!cap_notification_cap_get_capNtfnCanReceive(ntfn_cap)) {
        do { printf("\033[0m" "\033[1m" "<<" "\033[0m" "\033[32m" "seL4(CPU %" "lu" ")" "\033[0m" "\033[1m" " [%s/%d T%p \"%s\" @%lx]: " "TCB BindNotification: Insufficient access rights" ">>" "\033[0m" "\n", 0lu, __func__, 1702, ksCurThread, ((debug_tcb_t *)(((cte_t *)((word_t)(ksCurThread)&~((1ul << (12)) - 1ul)))+(tcbCNodeEntries)))->tcbName, (word_t)getRestartPC(ksCurThread)); } while (0);
        current_syscall_error.type = seL4_IllegalOperation;
        return EXCEPTION_SYSCALL_ERROR;
    }

    if ((tcb_t *)notification_ptr_get_ntfnQueue_head(ntfnPtr)
        || (tcb_t *)notification_ptr_get_ntfnBoundTCB(ntfnPtr)) {
        do { printf("\033[0m" "\033[1m" "<<" "\033[0m" "\033[32m" "seL4(CPU %" "lu" ")" "\033[0m" "\033[1m" " [%s/%d T%p \"%s\" @%lx]: " "TCB BindNotification: Notification cannot be bound." ">>" "\033[0m" "\n", 0lu, __func__, 1709, ksCurThread, ((debug_tcb_t *)(((cte_t *)((word_t)(ksCurThread)&~((1ul << (12)) - 1ul)))+(tcbCNodeEntries)))->tcbName, (word_t)getRestartPC(ksCurThread)); } while (0);
        current_syscall_error.type = seL4_IllegalOperation;
        return EXCEPTION_SYSCALL_ERROR;
    }


    setThreadState(ksCurThread, ThreadState_Restart);
    return invokeTCB_NotificationControl(tcb, ntfnPtr);
}

exception_t decodeUnbindNotification(cap_t cap)
{
    tcb_t *tcb;

    tcb = ((tcb_t *)(cap_thread_cap_get_capTCBPtr(cap)));

    if (!tcb->tcbBoundNotification) {
        do { printf("\033[0m" "\033[1m" "<<" "\033[0m" "\033[32m" "seL4(CPU %" "lu" ")" "\033[0m" "\033[1m" " [%s/%d T%p \"%s\" @%lx]: " "TCB UnbindNotification: TCB already has no bound Notification." ">>" "\033[0m" "\n", 0lu, __func__, 1726, ksCurThread, ((debug_tcb_t *)(((cte_t *)((word_t)(ksCurThread)&~((1ul << (12)) - 1ul)))+(tcbCNodeEntries)))->tcbName, (word_t)getRestartPC(ksCurThread)); } while (0);
        current_syscall_error.type = seL4_IllegalOperation;
        return EXCEPTION_SYSCALL_ERROR;
    }

    setThreadState(ksCurThread, ThreadState_Restart);
    return invokeTCB_NotificationControl(tcb, ((void *)0));
}

/* The following functions sit in the preemption monad and implement the
 * preemptible, non-faulting bottom end of a TCB invocation. */
exception_t invokeTCB_Suspend(tcb_t *thread)
{
    suspend(thread);
    return EXCEPTION_NONE;
}

exception_t invokeTCB_Resume(tcb_t *thread)
{
    restart(thread);
    return EXCEPTION_NONE;
}
# 1833 "/home/b/test/sel4test/kernel/src/object/tcb.c"
exception_t invokeTCB_ThreadControl(tcb_t *target, cte_t *slot,
                                    cptr_t faultep, prio_t mcp, prio_t priority,
                                    cap_t cRoot_newCap, cte_t *cRoot_srcSlot,
                                    cap_t vRoot_newCap, cte_t *vRoot_srcSlot,
                                    word_t bufferAddr, cap_t bufferCap,
                                    cte_t *bufferSrcSlot,
                                    thread_control_flag_t updateFlags)
{
    exception_t e;
    cap_t tCap = cap_thread_cap_new((word_t)target);

    if (updateFlags & thread_control_update_space) {
        target->tcbFaultHandler = faultep;
    }

    if (updateFlags & thread_control_update_mcp) {
        setMCPriority(target, mcp);
    }

    if (updateFlags & thread_control_update_space) {
        cte_t *rootSlot;

        rootSlot = (((cte_t *)((word_t)(target)&~((1ul << (12)) - 1ul)))+(tcbCTable));
        e = cteDelete(rootSlot, true);
        if (e != EXCEPTION_NONE) {
            return e;
        }
        if (sameObjectAs(cRoot_newCap, cRoot_srcSlot->cap) &&
            sameObjectAs(tCap, slot->cap)) {
            cteInsert(cRoot_newCap, cRoot_srcSlot, rootSlot);
        }
    }

    if (updateFlags & thread_control_update_space) {
        cte_t *rootSlot;

        rootSlot = (((cte_t *)((word_t)(target)&~((1ul << (12)) - 1ul)))+(tcbVTable));
        e = cteDelete(rootSlot, true);
        if (e != EXCEPTION_NONE) {
            return e;
        }
        if (sameObjectAs(vRoot_newCap, vRoot_srcSlot->cap) &&
            sameObjectAs(tCap, slot->cap)) {
            cteInsert(vRoot_newCap, vRoot_srcSlot, rootSlot);
        }
    }

    if (updateFlags & thread_control_update_ipc_buffer) {
        cte_t *bufferSlot;

        bufferSlot = (((cte_t *)((word_t)(target)&~((1ul << (12)) - 1ul)))+(tcbBuffer));
        e = cteDelete(bufferSlot, true);
        if (e != EXCEPTION_NONE) {
            return e;
        }
        target->tcbIPCBuffer = bufferAddr;

        if (bufferSrcSlot && sameObjectAs(bufferCap, bufferSrcSlot->cap) &&
            sameObjectAs(tCap, slot->cap)) {
            cteInsert(bufferCap, bufferSrcSlot, bufferSlot);
        }

        if (target == ksCurThread) {
            rescheduleRequired();
        }
    }

    if (updateFlags & thread_control_update_priority) {
        setPriority(target, priority);
    }

    return EXCEPTION_NONE;
}
# 1943 "/home/b/test/sel4test/kernel/src/object/tcb.c"
exception_t invokeTCB_CopyRegisters(tcb_t *dest, tcb_t *tcb_src,
                                    bool_t suspendSource, bool_t resumeTarget,
                                    bool_t transferFrame, bool_t transferInteger,
                                    word_t transferArch)
{
    if (suspendSource) {
        suspend(tcb_src);
    }

    if (resumeTarget) {
        restart(dest);
    }

    if (transferFrame) {
        word_t i;
        word_t v;
        word_t pc;

        for (i = 0; i < n_frameRegisters; i++) {
            v = getRegister(tcb_src, frameRegisters[i]);
            setRegister(dest, frameRegisters[i], v);
        }

        pc = getRestartPC(dest);
        setNextPC(dest, pc);
    }

    if (transferInteger) {
        word_t i;
        word_t v;

        for (i = 0; i < n_gpRegisters; i++) {
            v = getRegister(tcb_src, gpRegisters[i]);
            setRegister(dest, gpRegisters[i], v);
        }
    }

    Arch_postModifyRegisters(dest);

    if (dest == ksCurThread) {
        /* If we modified the current thread we may need to reschedule
         * due to changing registers are only reloaded in Arch_switchToThread */
        rescheduleRequired();
    }

    return Arch_performTransfer(transferArch, tcb_src, dest);
}

/* ReadRegisters is a special case: replyFromKernel & setMRs are
 * unfolded here, in order to avoid passing the large reply message up
 * to the top level in a global (and double-copying). We prevent the
 * top-level replyFromKernel_success_empty() from running by setting the
 * thread state. Retype does this too.
 */
exception_t invokeTCB_ReadRegisters(tcb_t *tcb_src, bool_t suspendSource,
                                    word_t n, word_t arch, bool_t call)
{
    word_t i, j;
    exception_t e;
    tcb_t *thread;

    thread = ksCurThread;

    if (suspendSource) {
        suspend(tcb_src);
    }

    e = Arch_performTransfer(arch, tcb_src, ksCurThread);
    if (e != EXCEPTION_NONE) {
        return e;
    }

    if (call) {
        word_t *ipcBuffer;

        ipcBuffer = lookupIPCBuffer(true, thread);

        setRegister(thread, badgeRegister, 0);

        for (i = 0; i < n && i < n_frameRegisters && i < n_msgRegisters; i++) {
            setRegister(thread, msgRegisters[i],
                        getRegister(tcb_src, frameRegisters[i]));
        }

        if (ipcBuffer != ((void *)0) && i < n && i < n_frameRegisters) {
            for (; i < n && i < n_frameRegisters; i++) {
                ipcBuffer[i + 1] = getRegister(tcb_src, frameRegisters[i]);
            }
        }

        j = i;

        for (i = 0; i < n_gpRegisters && i + n_frameRegisters < n
             && i + n_frameRegisters < n_msgRegisters; i++) {
            setRegister(thread, msgRegisters[i + n_frameRegisters],
                        getRegister(tcb_src, gpRegisters[i]));
        }

        if (ipcBuffer != ((void *)0) && i < n_gpRegisters
            && i + n_frameRegisters < n) {
            for (; i < n_gpRegisters && i + n_frameRegisters < n; i++) {
                ipcBuffer[i + n_frameRegisters + 1] =
                    getRegister(tcb_src, gpRegisters[i]);
            }
        }

        setRegister(thread, msgInfoRegister, wordFromMessageInfo(
                        seL4_MessageInfo_new(0, 0, 0, i + j)));
    }
    setThreadState(thread, ThreadState_Running);

    return EXCEPTION_NONE;
}

exception_t invokeTCB_WriteRegisters(tcb_t *dest, bool_t resumeTarget,
                                     word_t n, word_t arch, word_t *buffer)
{
    word_t i;
    word_t pc;
    exception_t e;
    bool_t archInfo;

    e = Arch_performTransfer(arch, ksCurThread, dest);
    if (e != EXCEPTION_NONE) {
        return e;
    }

    if (n > n_frameRegisters + n_gpRegisters) {
        n = n_frameRegisters + n_gpRegisters;
    }

    archInfo = Arch_getSanitiseRegisterInfo(dest);

    for (i = 0; i < n_frameRegisters && i < n; i++) {
        /* Offset of 2 to get past the initial syscall arguments */
        setRegister(dest, frameRegisters[i],
                    sanitiseRegister(frameRegisters[i],
                                     getSyscallArg(i + 2, buffer), archInfo));
    }

    for (i = 0; i < n_gpRegisters && i + n_frameRegisters < n; i++) {
        setRegister(dest, gpRegisters[i],
                    sanitiseRegister(gpRegisters[i],
                                     getSyscallArg(i + n_frameRegisters + 2,
                                                   buffer), archInfo));
    }

    pc = getRestartPC(dest);
    setNextPC(dest, pc);

    Arch_postModifyRegisters(dest);

    if (resumeTarget) {
        restart(dest);
    }

    if (dest == ksCurThread) {
        /* If we modified the current thread we may need to reschedule
         * due to changing registers are only reloaded in Arch_switchToThread */
        rescheduleRequired();
    }

    return EXCEPTION_NONE;
}

exception_t invokeTCB_NotificationControl(tcb_t *tcb, notification_t *ntfnPtr)
{
    if (ntfnPtr) {
        bindNotification(tcb, ntfnPtr);
    } else {
        unbindNotification(tcb);
    }

    return EXCEPTION_NONE;
}


void setThreadName(tcb_t *tcb, const char *name)
{
    strlcpy(((debug_tcb_t *)(((cte_t *)((word_t)(tcb)&~((1ul << (12)) - 1ul)))+(tcbCNodeEntries)))->tcbName, name, ((1ul << (12 -1)) - (tcbCNodeEntries * sizeof(cte_t)) - sizeof(debug_tcb_t)));
}


word_t setMRs_syscall_error(tcb_t *thread, word_t *receiveIPCBuffer)
{
    switch (current_syscall_error.type) {
    case seL4_InvalidArgument:
        return setMR(thread, receiveIPCBuffer, 0,
                     current_syscall_error.invalidArgumentNumber);

    case seL4_InvalidCapability:
        return setMR(thread, receiveIPCBuffer, 0,
                     current_syscall_error.invalidCapNumber);

    case seL4_IllegalOperation:
        return 0;

    case seL4_RangeError:
        setMR(thread, receiveIPCBuffer, 0,
              current_syscall_error.rangeErrorMin);
        return setMR(thread, receiveIPCBuffer, 1,
                     current_syscall_error.rangeErrorMax);

    case seL4_AlignmentError:
        return 0;

    case seL4_FailedLookup:
        setMR(thread, receiveIPCBuffer, 0,
              current_syscall_error.failedLookupWasSource ? 1 : 0);
        return setMRs_lookup_failure(thread, receiveIPCBuffer,
                                     current_lookup_fault, 1);

    case seL4_TruncatedMessage:
    case seL4_DeleteFirst:
    case seL4_RevokeFirst:
        return 0;
    case seL4_NotEnoughMemory:
        return setMR(thread, receiveIPCBuffer, 0,
                     current_syscall_error.memoryLeft);
    default:
        _fail("Invalid syscall error", "/home/b/test/sel4test/kernel/src/object/tcb.c", 2163, __func__);
    }
}
# 1 "/home/b/test/sel4test/kernel/src/object/untyped.c"
/*
 * Copyright 2014, General Dynamics C4 Systems
 *
 * SPDX-License-Identifier: GPL-2.0-only
 */
# 21 "/home/b/test/sel4test/kernel/src/object/untyped.c"
static word_t alignUp(word_t baseValue, word_t alignment)
{
    return (baseValue + ((1ul << (alignment)) - 1)) & ~((1ul << (alignment)) - 1ul);
}

exception_t decodeUntypedInvocation(word_t invLabel, word_t length, cte_t *slot,
                                    cap_t cap, bool_t call, word_t *buffer)
{
    word_t newType, userObjSize, nodeIndex;
    word_t nodeDepth, nodeOffset, nodeWindow;
    cte_t *rootSlot;
    exception_t status;
    cap_t nodeCap;
    lookupSlot_ret_t lu_ret;
    word_t nodeSize;
    word_t i;
    cte_t *destCNode;
    word_t freeRef, alignedFreeRef, objectSize, untypedFreeBytes;
    word_t freeIndex;
    bool_t deviceMemory;
    bool_t reset;

    /* Ensure operation is valid. */
    if (invLabel != UntypedRetype) {
        do { printf("\033[0m" "\033[1m" "<<" "\033[0m" "\033[32m" "seL4(CPU %" "lu" ")" "\033[0m" "\033[1m" " [%s/%d T%p \"%s\" @%lx]: " "Untyped cap: Illegal operation attempted." ">>" "\033[0m" "\n", 0lu, __func__, 45, ksCurThread, ((debug_tcb_t *)(((cte_t *)((word_t)(ksCurThread)&~((1ul << (12)) - 1ul)))+(tcbCNodeEntries)))->tcbName, (word_t)getRestartPC(ksCurThread)); } while (0);
        current_syscall_error.type = seL4_IllegalOperation;
        return EXCEPTION_SYSCALL_ERROR;
    }

    /* Ensure message length valid. */
    if (length < 6 || current_extra_caps.excaprefs[0] == ((void *)0)) {
        do { printf("\033[0m" "\033[1m" "<<" "\033[0m" "\033[32m" "seL4(CPU %" "lu" ")" "\033[0m" "\033[1m" " [%s/%d T%p \"%s\" @%lx]: " "Untyped invocation: Truncated message." ">>" "\033[0m" "\n", 0lu, __func__, 52, ksCurThread, ((debug_tcb_t *)(((cte_t *)((word_t)(ksCurThread)&~((1ul << (12)) - 1ul)))+(tcbCNodeEntries)))->tcbName, (word_t)getRestartPC(ksCurThread)); } while (0);
        current_syscall_error.type = seL4_TruncatedMessage;
        return EXCEPTION_SYSCALL_ERROR;
    }

    /* Fetch arguments. */
    newType = getSyscallArg(0, buffer);
    userObjSize = getSyscallArg(1, buffer);
    nodeIndex = getSyscallArg(2, buffer);
    nodeDepth = getSyscallArg(3, buffer);
    nodeOffset = getSyscallArg(4, buffer);
    nodeWindow = getSyscallArg(5, buffer);
    rootSlot = current_extra_caps.excaprefs[0];

    /* Is the requested object type valid? */
    if (newType >= seL4_ObjectTypeCount) {
        do { printf("\033[0m" "\033[1m" "<<" "\033[0m" "\033[32m" "seL4(CPU %" "lu" ")" "\033[0m" "\033[1m" " [%s/%d T%p \"%s\" @%lx]: " "Untyped Retype: Invalid object type." ">>" "\033[0m" "\n", 0lu, __func__, 68, ksCurThread, ((debug_tcb_t *)(((cte_t *)((word_t)(ksCurThread)&~((1ul << (12)) - 1ul)))+(tcbCNodeEntries)))->tcbName, (word_t)getRestartPC(ksCurThread)); } while (0);
        current_syscall_error.type = seL4_InvalidArgument;
        current_syscall_error.invalidArgumentNumber = 0;
        return EXCEPTION_SYSCALL_ERROR;
    }

    objectSize = getObjectSize(newType, userObjSize);

    /* Exclude impossibly large object sizes. getObjectSize can overflow if userObjSize
       is close to 2^wordBits, which is nonsensical in any case, so we check that this
       did not happen. userObjSize will always need to be less than wordBits. */
    if (userObjSize >= (1 << 6) || objectSize > 47) {
        do { printf("\033[0m" "\033[1m" "<<" "\033[0m" "\033[32m" "seL4(CPU %" "lu" ")" "\033[0m" "\033[1m" " [%s/%d T%p \"%s\" @%lx]: " "Untyped Retype: Invalid object size." ">>" "\033[0m" "\n", 0lu, __func__, 80, ksCurThread, ((debug_tcb_t *)(((cte_t *)((word_t)(ksCurThread)&~((1ul << (12)) - 1ul)))+(tcbCNodeEntries)))->tcbName, (word_t)getRestartPC(ksCurThread)); } while (0);
        current_syscall_error.type = seL4_RangeError;
        current_syscall_error.rangeErrorMin = 0;
        current_syscall_error.rangeErrorMax = 47;
        return EXCEPTION_SYSCALL_ERROR;
    }

    /* If the target object is a CNode, is it at least size 1? */
    if (newType == seL4_CapTableObject && userObjSize == 0) {
        do { printf("\033[0m" "\033[1m" "<<" "\033[0m" "\033[32m" "seL4(CPU %" "lu" ")" "\033[0m" "\033[1m" " [%s/%d T%p \"%s\" @%lx]: " "Untyped Retype: Requested CapTable size too small." ">>" "\033[0m" "\n", 0lu, __func__, 89, ksCurThread, ((debug_tcb_t *)(((cte_t *)((word_t)(ksCurThread)&~((1ul << (12)) - 1ul)))+(tcbCNodeEntries)))->tcbName, (word_t)getRestartPC(ksCurThread)); } while (0);
        current_syscall_error.type = seL4_InvalidArgument;
        current_syscall_error.invalidArgumentNumber = 1;
        return EXCEPTION_SYSCALL_ERROR;
    }

    /* If the target object is a Untyped, is it at least size 4? */
    if (newType == seL4_UntypedObject && userObjSize < 4) {
        do { printf("\033[0m" "\033[1m" "<<" "\033[0m" "\033[32m" "seL4(CPU %" "lu" ")" "\033[0m" "\033[1m" " [%s/%d T%p \"%s\" @%lx]: " "Untyped Retype: Requested UntypedItem size too small." ">>" "\033[0m" "\n", 0lu, __func__, 97, ksCurThread, ((debug_tcb_t *)(((cte_t *)((word_t)(ksCurThread)&~((1ul << (12)) - 1ul)))+(tcbCNodeEntries)))->tcbName, (word_t)getRestartPC(ksCurThread)); } while (0);
        current_syscall_error.type = seL4_InvalidArgument;
        current_syscall_error.invalidArgumentNumber = 1;
        return EXCEPTION_SYSCALL_ERROR;
    }
# 112 "/home/b/test/sel4test/kernel/src/object/untyped.c"
    /* Lookup the destination CNode (where our caps will be placed in). */
    if (nodeDepth == 0) {
        nodeCap = rootSlot->cap;
    } else {
        cap_t rootCap = rootSlot->cap;
        lu_ret = lookupTargetSlot(rootCap, nodeIndex, nodeDepth);
        if (lu_ret.status != EXCEPTION_NONE) {
            do { printf("\033[0m" "\033[1m" "<<" "\033[0m" "\033[32m" "seL4(CPU %" "lu" ")" "\033[0m" "\033[1m" " [%s/%d T%p \"%s\" @%lx]: " "Untyped Retype: Invalid destination address." ">>" "\033[0m" "\n", 0lu, __func__, 119, ksCurThread, ((debug_tcb_t *)(((cte_t *)((word_t)(ksCurThread)&~((1ul << (12)) - 1ul)))+(tcbCNodeEntries)))->tcbName, (word_t)getRestartPC(ksCurThread)); } while (0);
            return lu_ret.status;
        }
        nodeCap = lu_ret.slot->cap;
    }

    /* Is the destination actually a CNode? */
    if (cap_get_capType(nodeCap) != cap_cnode_cap) {
        do { printf("\033[0m" "\033[1m" "<<" "\033[0m" "\033[32m" "seL4(CPU %" "lu" ")" "\033[0m" "\033[1m" " [%s/%d T%p \"%s\" @%lx]: " "Untyped Retype: Destination cap invalid or read-only." ">>" "\033[0m" "\n", 0lu, __func__, 127, ksCurThread, ((debug_tcb_t *)(((cte_t *)((word_t)(ksCurThread)&~((1ul << (12)) - 1ul)))+(tcbCNodeEntries)))->tcbName, (word_t)getRestartPC(ksCurThread)); } while (0);
        current_syscall_error.type = seL4_FailedLookup;
        current_syscall_error.failedLookupWasSource = 0;
        current_lookup_fault = lookup_fault_missing_capability_new(nodeDepth);
        return EXCEPTION_SYSCALL_ERROR;
    }

    /* Is the region where the user wants to put the caps valid? */
    nodeSize = 1ul << cap_cnode_cap_get_capCNodeRadix(nodeCap);
    if (nodeOffset > nodeSize - 1) {
        do { printf("\033[0m" "\033[1m" "<<" "\033[0m" "\033[32m" "seL4(CPU %" "lu" ")" "\033[0m" "\033[1m" " [%s/%d T%p \"%s\" @%lx]: " "Untyped Retype: Destination node offset #%d too large." ">>" "\033[0m" "\n", 0lu, __func__, 137, ksCurThread, ((debug_tcb_t *)(((cte_t *)((word_t)(ksCurThread)&~((1ul << (12)) - 1ul)))+(tcbCNodeEntries)))->tcbName, (word_t)getRestartPC(ksCurThread), (int)nodeOffset); } while (0)
                                  ;
        current_syscall_error.type = seL4_RangeError;
        current_syscall_error.rangeErrorMin = 0;
        current_syscall_error.rangeErrorMax = nodeSize - 1;
        return EXCEPTION_SYSCALL_ERROR;
    }
    if (nodeWindow < 1 || nodeWindow > 256) {
        do { printf("\033[0m" "\033[1m" "<<" "\033[0m" "\033[32m" "seL4(CPU %" "lu" ")" "\033[0m" "\033[1m" " [%s/%d T%p \"%s\" @%lx]: " "Untyped Retype: Number of requested objects (%d) too small or large." ">>" "\033[0m" "\n", 0lu, __func__, 145, ksCurThread, ((debug_tcb_t *)(((cte_t *)((word_t)(ksCurThread)&~((1ul << (12)) - 1ul)))+(tcbCNodeEntries)))->tcbName, (word_t)getRestartPC(ksCurThread), (int)nodeWindow); } while (0)
                                  ;
        current_syscall_error.type = seL4_RangeError;
        current_syscall_error.rangeErrorMin = 1;
        current_syscall_error.rangeErrorMax = 256;
        return EXCEPTION_SYSCALL_ERROR;
    }
    if (nodeWindow > nodeSize - nodeOffset) {
        do { printf("\033[0m" "\033[1m" "<<" "\033[0m" "\033[32m" "seL4(CPU %" "lu" ")" "\033[0m" "\033[1m" " [%s/%d T%p \"%s\" @%lx]: " "Untyped Retype: Requested destination window overruns size of node." ">>" "\033[0m" "\n", 0lu, __func__, 153, ksCurThread, ((debug_tcb_t *)(((cte_t *)((word_t)(ksCurThread)&~((1ul << (12)) - 1ul)))+(tcbCNodeEntries)))->tcbName, (word_t)getRestartPC(ksCurThread)); } while (0);
        current_syscall_error.type = seL4_RangeError;
        current_syscall_error.rangeErrorMin = 1;
        current_syscall_error.rangeErrorMax = nodeSize - nodeOffset;
        return EXCEPTION_SYSCALL_ERROR;
    }

    /* Ensure that the destination slots are all empty. */
    destCNode = ((cte_t *)(cap_cnode_cap_get_capCNodePtr(nodeCap)));
    for (i = nodeOffset; i < nodeOffset + nodeWindow; i++) {
        status = ensureEmptySlot(destCNode + i);
        if (status != EXCEPTION_NONE) {
            do { printf("\033[0m" "\033[1m" "<<" "\033[0m" "\033[32m" "seL4(CPU %" "lu" ")" "\033[0m" "\033[1m" " [%s/%d T%p \"%s\" @%lx]: " "Untyped Retype: Slot #%d in destination window non-empty." ">>" "\033[0m" "\n", 0lu, __func__, 165, ksCurThread, ((debug_tcb_t *)(((cte_t *)((word_t)(ksCurThread)&~((1ul << (12)) - 1ul)))+(tcbCNodeEntries)))->tcbName, (word_t)getRestartPC(ksCurThread), (int)i); } while (0)
                             ;
            return status;
        }
    }

    /*
     * Determine where in the Untyped region we should start allocating new
     * objects.
     *
     * If we have no children, we can start allocating from the beginning of
     * our untyped, regardless of what the "free" value in the cap states.
     * (This may happen if all of the objects beneath us got deleted).
     *
     * If we have children, we just keep allocating from the "free" value
     * recorded in the cap.
     */
    status = ensureNoChildren(slot);
    if (status != EXCEPTION_NONE) {
        freeIndex = cap_untyped_cap_get_capFreeIndex(cap);
        reset = false;
    } else {
        freeIndex = 0;
        reset = true;
    }
    freeRef = ((word_t)(((word_t)(cap_untyped_cap_get_capPtr(cap))) + ((freeIndex)<<4)));

    /*
     * Determine the maximum number of objects we can create, and return an
     * error if we don't have enough space.
     *
     * We don't need to worry about alignment in this case, because if anything
     * fits, it will also fit aligned up (by packing it on the right hand side
     * of the untyped).
     */
    untypedFreeBytes = (1ul << (cap_untyped_cap_get_capBlockSize(cap))) -
                       ((freeIndex)<<4);

    if ((untypedFreeBytes >> objectSize) < nodeWindow) {
        do { printf("\033[0m" "\033[1m" "<<" "\033[0m" "\033[32m" "seL4(CPU %" "lu" ")" "\033[0m" "\033[1m" " [%s/%d T%p \"%s\" @%lx]: " "Untyped Retype: Insufficient memory " "(%lu * %lu bytes needed, %lu bytes available)." ">>" "\033[0m" "\n", 0lu, __func__, 204, ksCurThread, ((debug_tcb_t *)(((cte_t *)((word_t)(ksCurThread)&~((1ul << (12)) - 1ul)))+(tcbCNodeEntries)))->tcbName, (word_t)getRestartPC(ksCurThread), (word_t)nodeWindow, (objectSize >= (1 << 6) ? -1 : (1ul << objectSize)), (word_t)(untypedFreeBytes)); } while (0)



                                             ;
        current_syscall_error.type = seL4_NotEnoughMemory;
        current_syscall_error.memoryLeft = untypedFreeBytes;
        return EXCEPTION_SYSCALL_ERROR;
    }

    deviceMemory = cap_untyped_cap_get_capIsDevice(cap);
    if ((deviceMemory && !Arch_isFrameType(newType))
        && newType != seL4_UntypedObject) {
        do { printf("\033[0m" "\033[1m" "<<" "\033[0m" "\033[32m" "seL4(CPU %" "lu" ")" "\033[0m" "\033[1m" " [%s/%d T%p \"%s\" @%lx]: " "Untyped Retype: Creating kernel objects with device untyped" ">>" "\033[0m" "\n", 0lu, __func__, 217, ksCurThread, ((debug_tcb_t *)(((cte_t *)((word_t)(ksCurThread)&~((1ul << (12)) - 1ul)))+(tcbCNodeEntries)))->tcbName, (word_t)getRestartPC(ksCurThread)); } while (0);
        current_syscall_error.type = seL4_InvalidArgument;
        current_syscall_error.invalidArgumentNumber = 1;
        return EXCEPTION_SYSCALL_ERROR;
    }

    /* Align up the free region so that it is aligned to the target object's
     * size. */
    alignedFreeRef = alignUp(freeRef, objectSize);

    /* Perform the retype. */
    setThreadState(ksCurThread, ThreadState_Restart);
    return invokeUntyped_Retype(slot, reset,
                                (void *)alignedFreeRef, newType, userObjSize,
                                destCNode, nodeOffset, nodeWindow, deviceMemory);
}

static exception_t resetUntypedCap(cte_t *srcSlot)
{
    cap_t prev_cap = srcSlot->cap;
    word_t block_size = cap_untyped_cap_get_capBlockSize(prev_cap);
    void *regionBase = ((word_t *)(cap_untyped_cap_get_capPtr(prev_cap)));
    int chunk = 8;
    word_t offset = ((cap_untyped_cap_get_capFreeIndex(prev_cap))<<4);
    exception_t status;
    bool_t deviceMemory = cap_untyped_cap_get_capIsDevice(prev_cap);

    if (offset == 0) {
        return EXCEPTION_NONE;
    }

    /** AUXUPD: "(True, typ_region_bytes (ptr_val \<acute>regionBase)
        (unat \<acute>block_size))" */
    /** GHOSTUPD: "(True, gs_clear_region (ptr_val \<acute>regionBase)
        (unat \<acute>block_size))" */

    if (deviceMemory || block_size < chunk) {
        if (! deviceMemory) {
            clearMemory(regionBase, block_size);
        }
        srcSlot->cap = cap_untyped_cap_set_capFreeIndex(prev_cap, 0);
    } else {
        for (offset = (((offset - 1) >> (chunk)) << (chunk));
             offset != - (1ul << (chunk)); offset -= (1ul << (chunk))) {
            clearMemory(((void *)(((word_t)(regionBase)) + (offset))), chunk);
            srcSlot->cap = cap_untyped_cap_set_capFreeIndex(prev_cap, ((offset)>>4));
            status = preemptionPoint();
            if (status != EXCEPTION_NONE) {
                return status;
            }
        }
    }
    return EXCEPTION_NONE;
}

exception_t invokeUntyped_Retype(cte_t *srcSlot,
                                 bool_t reset, void *retypeBase,
                                 object_t newType, word_t userSize,
                                 cte_t *destCNode, word_t destOffset, word_t destLength,
                                 bool_t deviceMemory)
{
    word_t freeRef;
    word_t totalObjectSize;
    void *regionBase = ((word_t *)(cap_untyped_cap_get_capPtr(srcSlot->cap)));
    exception_t status;

    if (reset) {
        status = resetUntypedCap(srcSlot);
        if (status != EXCEPTION_NONE) {
            return status;
        }
    }

    /* Update the amount of free space left in this untyped cap.
     *
     * Note that userSize is not necessarily the true size of the object in
     * memory. In the case where newType is seL4_CapTableObject, the size is
     * transformed by getObjectSize. */
    totalObjectSize = destLength << getObjectSize(newType, userSize);
    freeRef = (word_t)retypeBase + totalObjectSize;
    srcSlot->cap = cap_untyped_cap_set_capFreeIndex(srcSlot->cap,
                                                    (((word_t)(freeRef) - (word_t)(regionBase))>>4));

    /* Create new objects and caps. */
    createNewObjects(newType, srcSlot, destCNode, destOffset, destLength,
                     retypeBase, userSize, deviceMemory);

    return EXCEPTION_NONE;
}
# 1 "/home/b/test/sel4test/kernel/src/smp/ipi.c"
/*
 * Copyright 2020, Data61, CSIRO (ABN 41 687 119 230)
 *
 * SPDX-License-Identifier: GPL-2.0-only
 */
# 1 "/home/b/test/sel4test/kernel/src/smp/lock.c"
/*
 * Copyright 2020, Data61, CSIRO (ABN 41 687 119 230)
 *
 * SPDX-License-Identifier: GPL-2.0-only
 */
# 1 "/home/b/test/sel4test/kernel/src/string.c"
/*
 * Copyright 2014, General Dynamics C4 Systems
 *
 * SPDX-License-Identifier: GPL-2.0-only
 */





word_t strnlen(const char *s, word_t maxlen)
{
    word_t len;
    for (len = 0; len < maxlen && s[len]; len++);
    return len;
}

word_t strlcpy(char *dest, const char *src, word_t size)
{
    word_t len;
    for (len = 0; len + 1 < size && src[len]; len++) {
        dest[len] = src[len];
    }
    dest[len] = '\0';
    return len;
}

word_t strlcat(char *dest, const char *src, word_t size)
{
    word_t len;
    /* get to the end of dest */
    for (len = 0; len < size && dest[len]; len++);
    /* check that dest was at least 'size' length to prevent inserting
     * a null byte when we shouldn't */
    if (len < size) {
        for (; len + 1 < size && *src; len++, src++) {
            dest[len] = *src;
        }
        dest[len] = '\0';
    }
    return len;
}
# 1 "/home/b/test/sel4test/kernel/src/util.c"
/*
 * Copyright 2014, General Dynamics C4 Systems
 *
 * SPDX-License-Identifier: GPL-2.0-only
 */





/*
 * memzero needs a custom type that allows us to use a word
 * that has the aliasing properties of a char.
 */
typedef unsigned long __attribute__((__may_alias__)) ulong_alias;

/*
 * Zero 'n' bytes of memory starting from 's'.
 *
 * 'n' and 's' must be word aligned.
 */
void memzero(void *s, unsigned long n)
{
    uint8_t *p = s;

    /* Ensure alignment constraints are met. */
    do { if (!((unsigned long)s % sizeof(unsigned long) == 0)) { _assert_fail("(unsigned long)s % sizeof(unsigned long) == 0", "/home/b/test/sel4test/kernel/src/util.c", 27, __func__); } } while(0);
    do { if (!(n % sizeof(unsigned long) == 0)) { _assert_fail("n % sizeof(unsigned long) == 0", "/home/b/test/sel4test/kernel/src/util.c", 28, __func__); } } while(0);

    /* We will never memzero an area larger than the largest current
       live object */
    /** GHOSTUPD: "(gs_get_assn cap_get_capSizeBits_'proc \<acute>ghost'state = 0
        \<or> \<acute>n <= gs_get_assn cap_get_capSizeBits_'proc \<acute>ghost'state, id)" */

    /* Write out words. */
    while (n != 0) {
        *(ulong_alias *)p = 0;
        p += sizeof(ulong_alias);
        n -= sizeof(ulong_alias);
    }
}

void *__attribute__((externally_visible)) memset(void *s, unsigned long c, unsigned long n)
{
    uint8_t *p;

    /*
     * If we are only writing zeros and we are word aligned, we can
     * use the optimized 'memzero' function.
     */
    if (__builtin_expect(!!(c == 0 && ((unsigned long)s % sizeof(unsigned long)) == 0 && (n % sizeof(unsigned long)) == 0), 1)) {
        memzero(s, n);
    } else {
        /* Otherwise, we use a slower, simple memset. */
        for (p = (uint8_t *)s; n > 0; n--, p++) {
            *p = (uint8_t)c;
        }
    }

    return s;
}

void *__attribute__((externally_visible)) memcpy(void *ptr_dst, const void *ptr_src, unsigned long n)
{
    uint8_t *p;
    const uint8_t *q;

    for (p = (uint8_t *)ptr_dst, q = (const uint8_t *)ptr_src; n; n--, p++, q++) {
        *p = *q;
    }

    return ptr_dst;
}

int __attribute__((__pure__)) strncmp(const char *s1, const char *s2, int n)
{
    word_t i;
    int diff;

    for (i = 0; i < n; i++) {
        diff = ((unsigned char *)s1)[i] - ((unsigned char *)s2)[i];
        if (diff != 0 || s1[i] == '\0') {
            return diff;
        }
    }

    return 0;
}

long __attribute__((__const__)) char_to_long(char c)
{
    if (c >= '0' && c <= '9') {
        return c - '0';
    } else if (c >= 'A' && c <= 'F') {
        return c - 'A' + 10;
    } else if (c >= 'a' && c <= 'f') {
        return c - 'a' + 10;
    }
    return -1;
}

long __attribute__((__pure__)) str_to_long(const char *str)
{
    unsigned int base;
    long res;
    long val = 0;
    char c;

    /* NULL ptr and empty str */
    if (str == ((void *)0) || *str == 0) {
        return -1;
    }

    /* check for "0x", *(str + 1) may be 0, but must be allocated since str is not empty */
    if (*str == '0' && (*(str + 1) == 'x' || *(str + 1) == 'X')) {
        base = 16;
        str += 2;
        /* '0x' on its own is malformed */
        if (*str == 0) {
            return -1;
        }
    } else {
        base = 10;
    }

    c = *str;
    while (c != '\0') {
        res = char_to_long(c);
        if (res == -1 || res >= base) {
            return -1;
        }
        val = val * base + res;
        str++;
        c = *str;
    }

    return val;
}

// The following implementations of CLZ (count leading zeros) and CTZ (count
// trailing zeros) perform a binary search for the first 1 bit from the
// beginning (resp. end) of the input. Initially, the focus is the whole input.
// Then, each iteration determines whether there are any 1 bits set in the
// upper (resp. lower) half of the current focus. If there are (resp. are not),
// then the upper half is shifted into the lower half. Either way, the lower
// half of the current focus becomes the new focus for the next iteration.
// After enough iterations (6 for 64-bit inputs, 5 for 32-bit inputs), the
// focus is reduced to a single bit, and the total number of bits shifted can
// be used to determine the number of zeros before (resp. after) the first 1
// bit.
//
// Although the details vary, the general approach is used in several library
// implementations, including in LLVM and GCC. Wikipedia has some references:
// https://en.wikipedia.org/wiki/Find_first_set
//
// The current implementation avoids branching. The test that determines
// whether the upper (resp. lower) half contains any ones directly produces a
// number which can be used for an unconditional shift. If the upper (resp.
// lower) half is all zeros, the test produces a zero, and the shift is a
// no-op. A branchless implementation has the disadvantage that it requires
// more instructions to execute than one which branches, but the advantage is
// that none will be mispredicted branches. Whether this is a good tradeoff
// depends on the branch predictability and the architecture's pipeline depth.
// The most critical use of clzl in the kernel is in the scheduler priority
// queue. In the absence of a concrete application and hardware implementation
// to evaluate the tradeoff, we somewhat arbitrarily choose a branchless
// implementation. In any case, the compiler might convert this to a branching
// binary.

// Check some assumptions made by the clzl, clzll, ctzl functions:
_Static_assert(sizeof(unsigned long) == 4 || sizeof(unsigned long) == 8, "clz_ulong_32_or_64");;
_Static_assert(sizeof(unsigned long long) == 8, "clz_ullong_64");;
_Static_assert(sizeof(unsigned long) * 8 == 64, "clz_word_size");;

// Count leading zeros.
// This implementation contains no branches. If the architecture provides an
// instruction to set a register to a boolean value on a comparison, then the
// binary might also avoid branching. A branchless implementation might be
// preferable on architectures with deep pipelines, or when the maximum
// priority of runnable threads frequently varies. However, note that the
// compiler may choose to convert this to a branching implementation.
//
// These functions are potentially `UNUSED` because we want to always expose
// them to verification without necessarily linking them into the kernel
// binary.
static __attribute__((unused)) __attribute__((__const__)) inline unsigned clz32(uint32_t x)
{
    // Compiler builtins typically return int, but we use unsigned internally
    // to reduce the number of guards we see in the proofs.
    unsigned count = 32;
    uint32_t mask = (0xFFFFFFFF);

    // Each iteration i (counting backwards) considers the least significant
    // 2^(i+1) bits of x as the current focus. At the first iteration, the
    // focus is the whole input. Each iteration assumes x contains no 1 bits
    // outside its focus. The iteration contains a test which determines
    // whether there are any 1 bits in the upper half (2^i bits) of the focus,
    // setting `bits` to 2^i if there are, or zero if not. Shifting by `bits`
    // then narrows the focus to the lower 2^i bits and satisfies the
    // assumption for the next iteration. After the final iteration, the focus
    // is just the least significant bit, and the most significsnt 1 bit of the
    // original input (if any) has been shifted into this position. The leading
    // zero count can be determined from the total shift.
    //
    // The iterations are given a very regular structure to facilitate proofs,
    // while also generating reasonably efficient binary code.

    // The `if (1)` blocks make it easier to reason by chunks in the proofs.
    if (1) {
        // iteration 4
        mask >>= (1 << 4); // 0x0000ffff
        unsigned bits = ((unsigned)(mask < x)) << 4; // [0, 16]
        x >>= bits; // <= 0x0000ffff
        count -= bits; // [16, 32]
    }
    if (1) {
        // iteration 3
        mask >>= (1 << 3); // 0x000000ff
        unsigned bits = ((unsigned)(mask < x)) << 3; // [0, 8]
        x >>= bits; // <= 0x000000ff
        count -= bits; // [8, 16, 24, 32]
    }
    if (1) {
        // iteration 2
        mask >>= (1 << 2); // 0x0000000f
        unsigned bits = ((unsigned)(mask < x)) << 2; // [0, 4]
        x >>= bits; // <= 0x0000000f
        count -= bits; // [4, 8, 12, ..., 32]
    }
    if (1) {
        // iteration 1
        mask >>= (1 << 1); // 0x00000003
        unsigned bits = ((unsigned)(mask < x)) << 1; // [0, 2]
        x >>= bits; // <= 0x00000003
        count -= bits; // [2, 4, 6, ..., 32]
    }
    if (1) {
        // iteration 0
        mask >>= (1 << 0); // 0x00000001
        unsigned bits = ((unsigned)(mask < x)) << 0; // [0, 1]
        x >>= bits; // <= 0x00000001
        count -= bits; // [1, 2, 3, ..., 32]
    }

    // If the original input was zero, there will have been no shifts, so this
    // gives a result of 32. Otherwise, x is now exactly 1, so subtracting from
    // count gives a result from [0, 1, 2, ..., 31].
    return count - x;
}

static __attribute__((unused)) __attribute__((__const__)) inline unsigned clz64(uint64_t x)
{
    unsigned count = 64;
    uint64_t mask = (0xFFFFFFFFFFFFFFFF);

    // Although we could implement this using clz32, we spell out the
    // iterations in full for slightly better code generation at low
    // optimisation levels, and to allow us to reuse the proof machinery we
    // developed for clz32.
    if (1) {
        // iteration 5
        mask >>= (1 << 5); // 0x00000000ffffffff
        unsigned bits = ((unsigned)(mask < x)) << 5; // [0, 32]
        x >>= bits; // <= 0x00000000ffffffff
        count -= bits; // [32, 64]
    }
    if (1) {
        // iteration 4
        mask >>= (1 << 4); // 0x000000000000ffff
        unsigned bits = ((unsigned)(mask < x)) << 4; // [0, 16]
        x >>= bits; // <= 0x000000000000ffff
        count -= bits; // [16, 32, 48, 64]
    }
    if (1) {
        // iteration 3
        mask >>= (1 << 3); // 0x00000000000000ff
        unsigned bits = ((unsigned)(mask < x)) << 3; // [0, 8]
        x >>= bits; // <= 0x00000000000000ff
        count -= bits; // [8, 16, 24, ..., 64]
    }
    if (1) {
        // iteration 2
        mask >>= (1 << 2); // 0x000000000000000f
        unsigned bits = ((unsigned)(mask < x)) << 2; // [0, 4]
        x >>= bits; // <= 0x000000000000000f
        count -= bits; // [4, 8, 12, ..., 64]
    }
    if (1) {
        // iteration 1
        mask >>= (1 << 1); // 0x0000000000000003
        unsigned bits = ((unsigned)(mask < x)) << 1; // [0, 2]
        x >>= bits; // <= 0x0000000000000003
        count -= bits; // [2, 4, 6, ..., 64]
    }
    if (1) {
        // iteration 0
        mask >>= (1 << 0); // 0x0000000000000001
        unsigned bits = ((unsigned)(mask < x)) << 0; // [0, 1]
        x >>= bits; // <= 0x0000000000000001
        count -= bits; // [1, 2, 3, ..., 64]
    }

    return count - x;
}

// Count trailing zeros.
// See comments on clz32.
static __attribute__((unused)) __attribute__((__const__)) inline unsigned ctz32(uint32_t x)
{
    unsigned count = (x == 0);
    uint32_t mask = (0xFFFFFFFF);

    // Each iteration i (counting backwards) considers the least significant
    // 2^(i+1) bits of x as the current focus. At the first iteration, the
    // focus is the whole input. The iteration contains a test which determines
    // whether there are any 1 bits in the lower half (2^i bits) of the focus,
    // setting `bits` to zero if there are, or 2^i if not. Shifting by `bits`
    // then narrows the focus to the lower 2^i bits for the next iteration.
    // After the final iteration, the focus is just the least significant bit,
    // and the least significsnt 1 bit of the original input (if any) has been
    // shifted into this position. The trailing zero count can be determined
    // from the total shift.
    //
    // If the initial input is zero, every iteration causes a shift, for a
    // total shift count of 31, so in that case, we add one for a total count
    // of 32. In the comments, xi is the initial value of x.
    //
    // The iterations are given a very regular structure to facilitate proofs,
    // while also generating reasonably efficient binary code.

    if (1) {
        // iteration 4
        mask >>= (1 << 4); // 0x0000ffff
        unsigned bits = ((unsigned)((x & mask) == 0)) << 4; // [0, 16]
        x >>= bits; // xi != 0 --> x & 0x0000ffff != 0
        count += bits; // if xi != 0 then [0, 16] else 17
    }
    if (1) {
        // iteration 3
        mask >>= (1 << 3); // 0x000000ff
        unsigned bits = ((unsigned)((x & mask) == 0)) << 3; // [0, 8]
        x >>= bits; // xi != 0 --> x & 0x000000ff != 0
        count += bits; // if xi != 0 then [0, 8, 16, 24] else 25
    }
    if (1) {
        // iteration 2
        mask >>= (1 << 2); // 0x0000000f
        unsigned bits = ((unsigned)((x & mask) == 0)) << 2; // [0, 4]
        x >>= bits; // xi != 0 --> x & 0x0000000f != 0
        count += bits; // if xi != 0 then [0, 4, 8, ..., 28] else 29
    }
    if (1) {
        // iteration 1
        mask >>= (1 << 1); // 0x00000003
        unsigned bits = ((unsigned)((x & mask) == 0)) << 1; // [0, 2]
        x >>= bits; // xi != 0 --> x & 0x00000003 != 0
        count += bits; // if xi != 0 then [0, 2, 4, ..., 30] else 31
    }
    if (1) {
        // iteration 0
        mask >>= (1 << 0); // 0x00000001
        unsigned bits = ((unsigned)((x & mask) == 0)) << 0; // [0, 1]
        x >>= bits; // xi != 0 --> x & 0x00000001 != 0
        count += bits; // if xi != 0 then [0, 1, 2, ..., 31] else 32
    }

    return count;
}

static __attribute__((unused)) __attribute__((__const__)) inline unsigned ctz64(uint64_t x)
{
    unsigned count = (x == 0);
    uint64_t mask = (0xFFFFFFFFFFFFFFFF);

    if (1) {
        // iteration 5
        mask >>= (1 << 5); // 0x00000000ffffffff
        unsigned bits = ((unsigned)((x & mask) == 0)) << 5; // [0, 32]
        x >>= bits; // xi != 0 --> x & 0x00000000ffffffff != 0
        count += bits; // if xi != 0 then [0, 32] else 33
    }
    if (1) {
        // iteration 4
        mask >>= (1 << 4); // 0x000000000000ffff
        unsigned bits = ((unsigned)((x & mask) == 0)) << 4; // [0, 16]
        x >>= bits; // xi != 0 --> x & 0x000000000000ffff != 0
        count += bits; // if xi != 0 then [0, 16, 32, 48] else 49
    }
    if (1) {
        // iteration 3
        mask >>= (1 << 3); // 0x00000000000000ff
        unsigned bits = ((unsigned)((x & mask) == 0)) << 3; // [0, 8]
        x >>= bits; // xi != 0 --> x & 0x00000000000000ff != 0
        count += bits; // if xi != 0 then [0, 8, 16, ..., 56] else 57
    }
    if (1) {
        // iteration 2
        mask >>= (1 << 2); // 0x000000000000000f
        unsigned bits = ((unsigned)((x & mask) == 0)) << 2; // [0, 4]
        x >>= bits; // xi != 0 --> x & 0x000000000000000f != 0
        count += bits; // if xi != 0 then [0, 4, 8, ..., 60] else 61
    }
    if (1) {
        // iteration 1
        mask >>= (1 << 1); // 0x0000000000000003
        unsigned bits = ((unsigned)((x & mask) == 0)) << 1; // [0, 2]
        x >>= bits; // xi != 0 --> x & 0x0000000000000003 != 0
        count += bits; // if xi != 0 then [0, 2, 4, ..., 62] else 63
    }
    if (1) {
        // iteration 0
        mask >>= (1 << 0); // 0x0000000000000001
        unsigned bits = ((unsigned)((x & mask) == 0)) << 0; // [0, 1]
        x >>= bits; // xi != 0 --> x & 0x0000000000000001 != 0
        count += bits; // if xi != 0 then [0, 1, 2, ..., 63] else 64
    }

    return count;
}

// GCC's builtins will emit calls to these functions when the platform does
// not provide suitable inline assembly.
// These are only provided when the relevant config items are set.
// We define these separately from `ctz32` etc. so that we can verify all of
// `ctz32` etc. without necessarily linking them into the kernel binary.
# 1 "/home/b/test/sel4test/projects/sel4test/domain_schedule.c"
/*
 * Copyright 2017, Data61, CSIRO (ABN 41 687 119 230)
 *
 * SPDX-License-Identifier: BSD-2-Clause
 */

/* This is a domain schedule that is suitable for the domains tests in sel4test. All
 * sel4test actually needs is for every domain to be executable for some period of time
 * in order for the tests to make progress.
 *
 * Most tests run only in domain 0, so we give it the longest period to reduce
 * overall idle time. We pick 2 ticks as the shortest period so that tests can
 * make some progress if they exist, and we pick some variety in the first four
 * domains so that not everything is equal.
 */

/* remember that this is compiled as part of the kernel, and so is referencing kernel headers */





/* Default schedule. */
const dschedule_t ksDomSchedule[] = {
    { .domain = 0, .length = 60 },
# 74 "/home/b/test/sel4test/projects/sel4test/domain_schedule.c"
};

const word_t ksDomScheduleLength = sizeof(ksDomSchedule) / sizeof(dschedule_t);
